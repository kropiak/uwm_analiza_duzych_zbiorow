{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dac9bf06-eb11-4900-a17e-78b155a58d65",
   "metadata": {},
   "source": [
    "# Apache Airflow, część 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1276da-b48b-4860-ac4c-4d4015f25993",
   "metadata": {},
   "source": [
    "## 1. Operatory bardziej szczegółowo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87501f4-91fe-43cc-a58e-7b8cda90e34b",
   "metadata": {},
   "source": [
    "W poprzednich zajęciach wspomniano o operatorach oraz ich głównych funkcjach w architekturze Apache Airflow. Baza operatorów już zdefiniowanych, tzw. `core operators` znajduje się poda adresem `URL 1`, a te które są zdefiniowane poza standardową biblioteką pod adresem `URL 2` lub `URL 3`.\n",
    "> URL 1: https://airflow.apache.org/docs/apache-airflow/stable/operators-and-hooks-ref.html  \n",
    "> URL 2: https://airflow.apache.org/docs/apache-airflow-providers/operators-and-hooks-ref/index.html  \n",
    "> URL 3: https://registry.astronomer.io/modules?types=operators\n",
    "\n",
    "Operatory wbudowane dzielą się na kilka rodzajów (patrz `URL 1`) i wyróżniamy:\n",
    "* operatory bazowe -  klasy bazowe, z których następnie dziedziczą kolejne, specyficzne operatory\n",
    "* **operatory wbudowane (core)**:\n",
    "  *  `BashOperator` ([docs](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/bash/index.html#module-airflow.operators.bash)) - operator pozwalający na uruhcamianie poleceń powłoki bash,\n",
    "  *  `PythonOperator` ([docs](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/python/index.html#module-airflow.operators.python)) - operator pozwalający na wykonanie kodu języka Python, zalecane jest wykorzystanie dekoratora `@task` zamiast klasycznego podejścia z wykorzystaniem instancji klasy `PythonOperator`, zobacz opis i przykład [tu](https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/python.html)\n",
    "  *  inne - np. `EmailOperator` , `EmptyOperator`.\n",
    "*  **`Sensory`** - są to operatory, które wprowadzają możliwość pracy z taskami w formie eventów, gdyż te operatory zostały stworzone, aby dostarczać informacji (statusu) zainstnienia różnych zdarzeń, np.:\n",
    "  *  `FileSensor` - pozwala sprawdzać czy w systemie plików pojawiły się określone pliki, co umożliwia wykonanie kolejnych tasków lub nie w zależności od wykrytego stanu,\n",
    "  *  `ExternalTaskSensor` - sensor pozwalający na monitorowanie stanu wykonania w jakimś stopniu zależnych od siebie grafów zadań (ang. DAG), możliwe sprawdzenie stanu wykonania innego grafu,\n",
    "  *  `TimeSensor` - oczekuje na określoną porę dnia,\n",
    "  *  inne: `PythonSensor`, `BashSensor`, `TimeDeltaSensor`, `DayOfWeekSensor` i inne."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313c5af8-fafa-4a5b-95c5-8d969d6f7562",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T09:08:51.607620Z",
     "iopub.status.busy": "2024-12-04T09:08:51.606627Z",
     "iopub.status.idle": "2024-12-04T09:08:51.612893Z",
     "shell.execute_reply": "2024-12-04T09:08:51.611264Z",
     "shell.execute_reply.started": "2024-12-04T09:08:51.607561Z"
    }
   },
   "source": [
    "## 1.1 Przykład własnego grafu zadań wykorzystujących BashOperator oraz PythonOperator z wykorzystanie API TaskFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878fb691-d2ac-4bdc-b818-22c9ab5f8a9e",
   "metadata": {},
   "source": [
    "#### 1.1.1 TaskFlow API\n",
    "\n",
    "> Przewodnik z przykładem wykorzystania API TaskFlow: https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html\n",
    "\n",
    "API TaskFlow jest wprowadzonym w wersji 2.0 zmodyfikowanym podejściem do definiowania grafów za pośrednictwem języka Python i bazuje na wykorzystaniu dekoratorów w miejsce tworzenia instancji klas samego DAG jak i poszczególnych operatorów.\n",
    "\n",
    "Poniżej przykład z dokumentacji z linka powyżej.\n",
    "\n",
    "_**Listing 1**_\n",
    "```python\n",
    "import json\n",
    "import pendulum\n",
    "from airflow.decorators import dag, task\n",
    "\n",
    "\n",
    "@dag(\n",
    "    schedule=None,\n",
    "    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n",
    "    catchup=False,\n",
    "    tags=[\"example\"],\n",
    ")\n",
    "\n",
    "def tutorial_taskflow_api():\n",
    "    \"\"\"\n",
    "    ### TaskFlow API Tutorial Documentation\n",
    "    This is a simple data pipeline example which demonstrates the use of\n",
    "    the TaskFlow API using three simple tasks for Extract, Transform, and Load.\n",
    "    Documentation that goes along with the Airflow TaskFlow API tutorial is\n",
    "    located\n",
    "    [here](https://airflow.apache.org/docs/apache-airflow/stable/tutorial_taskflow_api.html)\n",
    "    \"\"\"\n",
    "    @task()\n",
    "    def extract():\n",
    "        \"\"\"\n",
    "        #### Extract task\n",
    "        A simple Extract task to get data ready for the rest of the data\n",
    "        pipeline. In this case, getting data is simulated by reading from a\n",
    "        hardcoded JSON string.\n",
    "        \"\"\"\n",
    "        data_string = '{\"1001\": 301.27, \"1002\": 433.21, \"1003\": 502.22}'\n",
    "\n",
    "        order_data_dict = json.loads(data_string)\n",
    "        return order_data_dict\n",
    "        \n",
    "    @task(multiple_outputs=True)\n",
    "    def transform(order_data_dict: dict):\n",
    "        \"\"\"\n",
    "        #### Transform task\n",
    "        A simple Transform task which takes in the collection of order data and\n",
    "        computes the total order value.\n",
    "        \"\"\"\n",
    "        total_order_value = 0\n",
    "\n",
    "        for value in order_data_dict.values():\n",
    "            total_order_value += value\n",
    "\n",
    "        return {\"total_order_value\": total_order_value}\n",
    "        \n",
    "    @task()\n",
    "    def load(total_order_value: float):\n",
    "        \"\"\"\n",
    "        #### Load task\n",
    "        A simple Load task which takes in the result of the Transform task and\n",
    "        instead of saving it to end user review, just prints it out.\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"Total order value is: {total_order_value:.2f}\")\n",
    "\n",
    "    # nie jest konieczne jawne deklarowanie zależności między taskami, jeżeli powinny zostać wywołane sekwencyjnie\n",
    "    # kolejność ich wywołania będzie to determinowała w tym przypadku\n",
    "    order_data = extract()\n",
    "    order_summary = transform(order_data)\n",
    "    load(order_summary[\"total_order_value\"])\n",
    "\n",
    "# i ostatecznie uruchomienie całego grafu \n",
    "tutorial_taskflow_api()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5efca6-aec3-4583-b3d2-ceace4b01ebf",
   "metadata": {},
   "source": [
    "#### 1.1.1 Edycja konfiguracji i przebudowanie bazy Airflow.\n",
    "\n",
    "Aby wskazać inny niż domyślny folder z grafami zadań należy zmienić wpis w konfiguracji Apache Airflow w pliku `airflow.cfg`. Domyślnie jest to ścieżka `/home/spark/airflow/airflow.cfg`. Jeżeli mapowanie tej ścieżki w oprogramowaniu docker zostało wykonane poprawnie (lab 8) to ten plik powinien być widoczny również z poziomu komputera hosta jeżeli w takiej konfiguracji zostało to uruchomione.\n",
    "\n",
    "**Nie jest złym pomysłem wcześniejsze wykonanie kopii tego pliku**\n",
    "\n",
    "Wpis w konfiguracji, który wskazuje położenie grafów to `core.dags_folder`, która znajduje się na samym początku pliku:\n",
    "```bash\n",
    "[core]\n",
    "# The folder where your airflow pipelines live, most likely a\n",
    "# subfolder in a code repository. This path must be absolute.\n",
    "#\n",
    "# Variable: AIRFLOW__CORE__DAGS_FOLDER\n",
    "#\n",
    "dags_folder = /home/spark/airflow/dags\n",
    "```\n",
    "Tutaj będziemy umieszczać zdefiniowane przez nas grafy.\n",
    "Domyślnie Airflow wczytuje również wszystkie przykładowe grafy, które widoczne są na liście i na podstawie których przedstawiony był opis przykładowego grafu w lba 8. Aby wyłączyć wczytywanie przykładoweych grafów należy zmienić linię w konfiguracji z:\n",
    "```bash\n",
    "load_examples = True\n",
    "```\n",
    "na\n",
    "```bash\n",
    "load_examples = False\n",
    "```\n",
    "\n",
    "Lista grafów, które znajdują się na liście jest przeładowywana co pewien czas, ale informacje te znajdują się również w bazie, która jest konfigurowana przy pierwszym uruchomieniu Airflow.\n",
    "\n",
    "Zmiana folderu grafów to dość daleko idąca zmiana, więc zazwyczaj wykonana w celu konfiguracji zupełnie nowego środowiska. Pozbędziemy się więc z bazy informacji, które już tam zostały zapisane.\n",
    "\n",
    "Polecenie `airflow db reset` zresetuje bazę do ustawień początkowych. Po jej wykonaniu należy przeładować Apache Airflow i zalogować się nowym hasłem, które zostanie utworzone.\n",
    "Lista dostępnych grafów powinna być teraz pusta.\n",
    "\n",
    "> Więcej poleceń Airflow CLI dla bazy danych: https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.html#db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5844fc-b454-4726-9477-8813016a0631",
   "metadata": {},
   "source": [
    "#### 1.1.2 Stworzenie własnego grafu zadań z wykorzystaniem TaskFlow API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c867b46-f5b8-4a55-bd84-e64a44c280b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sprawdzamy czy pakiet BeautifulSoup jest już zainstalowany\n",
    "!pip list | grep beautifulsoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32662cf-9344-4824-99d8-7ce7b71a0a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jeżeli nie to instalujemy\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baee5226-d3a6-4dd5-a7e8-24b65f7497b6",
   "metadata": {},
   "source": [
    "**Listing 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3e3e26-e3ec-48e1-a59d-9a9a4dbf4b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plik o nazwie bgg_top_games.py\n",
    "import pendulum\n",
    "from datetime import datetime, timedelta\n",
    "from airflow.decorators import dag, task\n",
    "\n",
    "\n",
    "@dag(\n",
    "    schedule=timedelta(days=1),\n",
    "    start_date=pendulum.datetime(2024, 12, 4, tz=\"UTC\"),\n",
    "    catchup=False,\n",
    "    tags=[\"bgg\"],\n",
    ")\n",
    "def bgg_top_games_list():\n",
    "    \"\"\"\n",
    "    ### Zadania polegające na pobraniu aktualnego zestawienia najlepiej ocenianych gier planszowych\n",
    "    z serwisu BoardGameGeek.com w postaci dokumentu HTML, parsowanie i zapisanie w konkretnym formacie\n",
    "    danych.\n",
    "    Adres zestawienia: https://boardgamegeek.com/browse/boardgame\n",
    "    \"\"\"\n",
    "\n",
    "    # @task.bash(cwd='../data/bgg/raw/')\n",
    "    # powyższa linia nie zadziała w naszym przypadku, gdyż narzędzie cwd nie jest zainstalowane w naszym obrazie dockerowym\n",
    "    # będzie więc używana pełna ścieżka\n",
    "    @task.bash\n",
    "    def extract():\n",
    "        \"\"\"\n",
    "        #### Zadanie ekstrakcji danych. Tu można podejść do tego na kilka sposobów. Np. pobrać\n",
    "        dane bezpośrednio z poziomu Pythona, ale dla, żeby pokazać szersze spektrum zadań,\n",
    "        użyte zostanie inne podejście. Dane zostaną pobrane z pomocą BashOperator i polecenia curl.\n",
    "        \"\"\"\n",
    "        base_path = '/home/spark/airflow/data/bgg/raw/'\n",
    "        filepath = f'{base_path}bgg_{datetime.strftime(datetime.now(), \"%Y-%m-%d\")}.html'\n",
    "        command = f'curl -s https://boardgamegeek.com/browse/boardgame > {filepath} && echo {filepath}'\n",
    "\n",
    "        return command\n",
    "    \n",
    "    @task()\n",
    "    def transform(bgg_page_file: str):\n",
    "        \"\"\"\n",
    "        #### Zadanie transformacji danych.\n",
    "        \"\"\"\n",
    "        from bs4 import BeautifulSoup\n",
    "        import csv\n",
    "\n",
    "        csv_path = '/home/spark/airflow/data/bgg/csv/'\n",
    "\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"Processing file: {bgg_page_file}\")\n",
    "\n",
    "        try:\n",
    "            with open(bgg_page_file, 'r') as file:\n",
    "                parsed_html = BeautifulSoup(file, 'html.parser')\n",
    "        except OSError as err:\n",
    "            raise OSError()\n",
    "\n",
    "        # parsowanie tabeli i zapisanie danych jako json\n",
    "        table_html = parsed_html.body.find('table', attrs={'class':'collection_table'})\n",
    "\n",
    "        rows = table_html.find_all('tr')\n",
    "        data = []\n",
    "        col_names = []\n",
    "        for row_id, row in enumerate(rows):\n",
    "            if row_id == 0:\n",
    "                col_names = [ele.text.strip() for ele in row.find_all('th')]\n",
    "                continue\n",
    "            cols = [ele.text.strip() for ele in row.find_all('td')]\n",
    "            data.append([ele for ele in cols if ele])\n",
    "\n",
    "        # zapisanie danych w formacie csv\n",
    "        csv_filename = bgg_page_file.split('/')[-1].split('.')[0] + '.csv'\n",
    "        try:\n",
    "            with open(csv_path + csv_filename, 'w') as csvfile:\n",
    "                bggwriter = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "                bggwriter.writerow(col_names)\n",
    "                bggwriter.writerows(data)\n",
    "        except OSError as err:\n",
    "            raise OSError()\n",
    "        \n",
    "        return csv_path + csv_filename\n",
    "\n",
    "    @task\n",
    "    def load(bgg_csv_file: str):\n",
    "        import pandas as pd\n",
    "\n",
    "        df = pd.read_csv(bgg_csv_file, header=0)\n",
    "        print(df.info())\n",
    "        print(df.head())\n",
    "        \n",
    "\n",
    "    bgg_page_of_the_day = extract()\n",
    "    bgg_csv = transform(bgg_page_of_the_day)\n",
    "    bgg_pandas_data = load(bgg_csv)\n",
    "\n",
    "\n",
    "bgg_top_games_list()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6657e7b9-5d8f-4b58-9085-749f58b58022",
   "metadata": {},
   "source": [
    "> Definiowanie konfiguracji połączenia Spark w Airflow\n",
    "https://airflow.apache.org/docs/apache-airflow-providers-apache-spark/stable/connections/spark-submit.html\n",
    "\n",
    "> Aby możliwe było wywoływanie zadań Apache Spark z poziomu Apache Airflow konieczne będzie zainstalowanie poniższej paczki:\n",
    "```console\n",
    "pip install apache-airflow[apache-spark]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4f7bb4-26aa-433c-891b-934e79372f54",
   "metadata": {},
   "source": [
    "Kolejny przykład grafu zadań pokazuje wykorzystanie operatora `FileSensor`, który nasłuchuje czy w folderze `/home/spark/airflow/data/bgg/csv/` znajduje się jakiś plik, a jeżeli tak to kolejne zadanie z wykorzystaniem Sparka przetwarza ten plik.\n",
    "\n",
    "Apache Airflow pozwala na zdefiniowanie połączeń do różnych usług i zapisanie ich do późniejszego wykorzystania w specjalnie do tego przeznaczonym miejscu. Dostajemy się do tych ustawień poprzez menu `Admin -> Connections`. W poniższym grafie wykorzystane są dwa takie połączenia.\n",
    "\n",
    "Połączenie do systemu plików w zadaniu `wait_for_file` przekazywane przez parametr `fs_conn_id='fs_bgg'` zostało zdefiniowane w sposób przedstawiony na poniższym zrzucie ekranu.\n",
    "\n",
    "![airflow_config_fs](airflow_config_fs.png)\n",
    "\n",
    "\n",
    "Kolejne połączenie dotyczy zadania `read_csv_with_spark`, które wymaga połączenia do klastra Sparka. Ponownie w `connections` modyfikujemy lub dodajemy połączenie podając parametry jak na zrzucie ekranu poniżej.\n",
    "\n",
    "![airflow_config_spark](airflow_config_spark.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8718fa-824b-4ac4-ac29-06805f3742c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plik bgg_spark_warehouse.py\n",
    "# przykład operatora Sensora\n",
    "import pendulum\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from airflow.decorators import dag, task\n",
    "from airflow.models.baseoperator import chain\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "from airflow.contrib.operators.spark_submit_operator import SparkSubmitOperator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "\n",
    "@dag(\n",
    "    schedule=timedelta(hours=4),\n",
    "    start_date=pendulum.datetime(2024, 12, 11, tz=\"UTC\"),\n",
    "    catchup=False,\n",
    "    tags=[\"bgg\"],\n",
    ")\n",
    "def bgg_save_to_warehouse():\n",
    "\n",
    "    # base_filepath = '/home/spark/airflow/data/bgg/csv/'\n",
    "    # today = datetime.strftime(datetime.now(), \"%Y-%m-%d\")\n",
    "\n",
    "    # jeżeli ścieżka wskazuje na folder to nasłuchiwane będzie pojawienie się\n",
    "    # jakiegokolwiek pliku w tym folderze\n",
    "    wait_for_file = FileSensor(\n",
    "        task_id='wait_for_file',\n",
    "        fs_conn_id='fs_bgg',\n",
    "        filepath='csv',\n",
    "        poke_interval=10,\n",
    "        timeout=300\n",
    "    )\n",
    "\n",
    "    # @task.pyspark(conn_id=\"spark_default\")\n",
    "    # def read_csv_with_spark(spark: SparkSession, sc: SparkContext):\n",
    "    #     print(\"Spark task!\")\n",
    "    #     for file in os.listdir(base_filepath):\n",
    "    #         if file.endswith(\".csv\"):\n",
    "    #             print(\"Plik odnaleziony!\")\n",
    "    #             print(f\"{base_filepath + file}\")\n",
    "    #             print(spark.sparkContext)\n",
    "    #             df = spark.read.csv(base_filepath + file, header=True, inferSchema=True)\n",
    "    #             # print(df.show(5))\n",
    "\n",
    "    spark_submit_task = SparkSubmitOperator(\n",
    "        task_id='spark_read_csv',\n",
    "        application='/opt/spark/work-dir/spark_scripts/read_bgg_csv.py',\n",
    "        conn_id='spark_default',\n",
    "        executor_cores=2,\n",
    "        executor_memory='2g',\n",
    "        num_executors=2,\n",
    "        name='airflow-spark',\n",
    "    )\n",
    "    \n",
    "    \n",
    "    wait_for_file >> spark_submit_task\n",
    "    \n",
    "\n",
    "bgg_save_to_warehouse()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a137a54-0b81-4297-a186-a0ee237e0ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plik read_bgg_csv.py\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "warehouse_location = '/opt/spark/work-dir/lab_07/metastore_db'\n",
    "base_filepath = '/home/spark/airflow/data/bgg/csv/'\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[2]\")\\\n",
    "        .appName(\"Apache SQL and Hive\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse_location)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "for file in os.listdir(base_filepath):\n",
    "    if file.endswith(\".csv\"):\n",
    "        df = spark.read.csv(base_filepath + file, header=True, inferSchema=True)\n",
    "        print(df.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584c2938-fc3c-41e5-a929-da4bf9c60da3",
   "metadata": {},
   "source": [
    "## Zadania"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee5f901-ee00-4864-962e-415a0d2da713",
   "metadata": {},
   "source": [
    "**Zadanie 1**  \n",
    "Dodaj do kolekcji swoich grafów zadań w Apache Airflow graf z listingu 2. Uruchom graf i sprawdź czy każde zadanie wykonuje się poprawnie.\n",
    "\n",
    "**Zadanie 2**  \n",
    "Zmodyfikuj efekt działania grafu tak, aby:\n",
    "* w kolumnie `Thumbnail image` znajdowała się ścieżka do pliku z grafiką (adres URL),\n",
    "* w pliku znajdowała się jeszcze jedna kolumna o nazwie `description`, która zawierać będzie to, co w kolumnie `Title` jest opisem (tekst po dacie premiery gry w nawiasach () ),\n",
    "* pozbądź się kolumny `shop`.\n",
    "\n",
    "**Zadanie 3**  \n",
    "Zmodyfikuj graf z listingu 2 tak, aby przetworzony plik csv po wykonaniu zadania `read_csv_with_spark` był przenoszony w inne miejsce, np. do podfolderu '/home/spark/airflow/data/bgg/processed/'. Dodaj nowy task w tym grafie i wykorzystaj operator bash do wykonania przeniesienia tego pliku.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
