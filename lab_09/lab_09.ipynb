{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dac9bf06-eb11-4900-a17e-78b155a58d65",
   "metadata": {},
   "source": [
    "# Apache Airflow, część 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9755eb44-5add-4fb8-9cec-f6c6671a4c47",
   "metadata": {},
   "source": [
    "W tym labie\n",
    "* plik konfiguracyjny, omówienie najważniejszych rzeczy, zmiana domyślnego folderu z dag\n",
    "* napisanie jednego daga przykładowego z 2-3 różnymi operatorami\n",
    "* sensory\n",
    "* TaskFlow i inny sposób deklarowania tasków\n",
    "* Jinja templating ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1276da-b48b-4860-ac4c-4d4015f25993",
   "metadata": {},
   "source": [
    "## 1. Operatory bardziej szczegółowo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87501f4-91fe-43cc-a58e-7b8cda90e34b",
   "metadata": {},
   "source": [
    "W poprzednich zajęciach wspomniano o operatorach oraz ich głównych funkcjach w architekturze Apache Airflow. Baza operatorów już zdefiniowanych, tzw. `core operators` znajduje się poda adresem `URL 1`, a te które są zdefiniowane poza standardową biblioteką pod adresem `URL 2` lub `URL 3`.\n",
    "> URL 1: https://airflow.apache.org/docs/apache-airflow/stable/operators-and-hooks-ref.html  \n",
    "> URL 2: https://airflow.apache.org/docs/apache-airflow-providers/operators-and-hooks-ref/index.html\n",
    "> URL 3: https://registry.astronomer.io/modules?types=operators\n",
    "\n",
    "Operatory wbudowane dzielą się na kilka rodzajów (patrz `URL 1`) i wyróżniamy:\n",
    "* operatory bazowe -  klasy bazowe, z których następnie dziedziczą kolejne, specyficzne operatory\n",
    "* **operatory wbudowane (core)**:\n",
    "  *  `BashOperator` ([docs](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/bash/index.html#module-airflow.operators.bash)) - operator pozwalający na uruhcamianie poleceń powłoki bash,\n",
    "  *  `PythonOperator` ([docs](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/python/index.html#module-airflow.operators.python)) - operator pozwalający na wykonanie kodu języka Python, zalecane jest wykorzystanie dekoratora `@task` zamiast klasycznego podejścia z wykorzystaniem instancji klasy `PythonOperator`, zobacz opis i przykład [tu](https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/python.html)\n",
    "  *  inne - np. `EmailOperator` , `EmptyOperator`.\n",
    "*  **`Sensory`** - są to operatory, które wprowadzają możliwość pracy z taskami w formie eventów, gdyż te operatory zostały stworzone, aby dostarczać informacji (statusu) zainstnienia różnych zdarzeń, np.:\n",
    "  *  `FileSensor` - pozwala sprawdzać czy w systemie plików pojawiły się określone pliki, co umożliwia wykonanie kolejnych tasków lub nie w zależności od wykrytego stanu,\n",
    "  *  `ExternalTaskSensor` - sensor pozwalający na monitorowanie stanu wykonania w jakimś stopniu zależnych od siebie grafów zadań (ang. DAG), możliwe sprawdzenie stanu wykonania innego grafu,\n",
    "  *  `TimeSensor` - oczekuje na określoną porę dnia,\n",
    "  *  inne: `PythonSensor`, `BashSensor`, `TimeDeltaSensor`, `DayOfWeekSensor` i inne."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313c5af8-fafa-4a5b-95c5-8d969d6f7562",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T09:08:51.607620Z",
     "iopub.status.busy": "2024-12-04T09:08:51.606627Z",
     "iopub.status.idle": "2024-12-04T09:08:51.612893Z",
     "shell.execute_reply": "2024-12-04T09:08:51.611264Z",
     "shell.execute_reply.started": "2024-12-04T09:08:51.607561Z"
    }
   },
   "source": [
    "## 1.1 Przykład własnego grafu zadań wykorzystujących BashOperator oraz PythonOperator z wykorzystanie API TaskFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878fb691-d2ac-4bdc-b818-22c9ab5f8a9e",
   "metadata": {},
   "source": [
    "#### 1.1.1 TaskFlow API\n",
    "\n",
    "> Przewodnik z przykładem wykorzystania API TaskFlow: https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html\n",
    "\n",
    "API TaskFlow jest wprowadzonym w wersji 2.0 zmodyfikowanym podejściem do definiowania grafów za pośrednictwem języka Python i bazuje na wykorzystaniu dekoratorów w miejsce tworzenia instancji klas samego DAG jak i poszczególnych operatorów.\n",
    "\n",
    "Poniżej przykład z dokumentacji z linka powyżej.\n",
    "\n",
    "_**Listing 1**_\n",
    "```python\n",
    "import json\n",
    "import pendulum\n",
    "from airflow.decorators import dag, task\n",
    "\n",
    "\n",
    "@dag(\n",
    "    schedule=None,\n",
    "    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n",
    "    catchup=False,\n",
    "    tags=[\"example\"],\n",
    ")\n",
    "\n",
    "def tutorial_taskflow_api():\n",
    "    \"\"\"\n",
    "    ### TaskFlow API Tutorial Documentation\n",
    "    This is a simple data pipeline example which demonstrates the use of\n",
    "    the TaskFlow API using three simple tasks for Extract, Transform, and Load.\n",
    "    Documentation that goes along with the Airflow TaskFlow API tutorial is\n",
    "    located\n",
    "    [here](https://airflow.apache.org/docs/apache-airflow/stable/tutorial_taskflow_api.html)\n",
    "    \"\"\"\n",
    "    @task()\n",
    "    def extract():\n",
    "        \"\"\"\n",
    "        #### Extract task\n",
    "        A simple Extract task to get data ready for the rest of the data\n",
    "        pipeline. In this case, getting data is simulated by reading from a\n",
    "        hardcoded JSON string.\n",
    "        \"\"\"\n",
    "        data_string = '{\"1001\": 301.27, \"1002\": 433.21, \"1003\": 502.22}'\n",
    "\n",
    "        order_data_dict = json.loads(data_string)\n",
    "        return order_data_dict\n",
    "        \n",
    "    @task(multiple_outputs=True)\n",
    "    def transform(order_data_dict: dict):\n",
    "        \"\"\"\n",
    "        #### Transform task\n",
    "        A simple Transform task which takes in the collection of order data and\n",
    "        computes the total order value.\n",
    "        \"\"\"\n",
    "        total_order_value = 0\n",
    "\n",
    "        for value in order_data_dict.values():\n",
    "            total_order_value += value\n",
    "\n",
    "        return {\"total_order_value\": total_order_value}\n",
    "        \n",
    "    @task()\n",
    "    def load(total_order_value: float):\n",
    "        \"\"\"\n",
    "        #### Load task\n",
    "        A simple Load task which takes in the result of the Transform task and\n",
    "        instead of saving it to end user review, just prints it out.\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"Total order value is: {total_order_value:.2f}\")\n",
    "\n",
    "    # nie jest konieczne jawne deklarowanie zależności między taskami, jeżeli powinny zostać wywołane sekwencyjnie\n",
    "    # kolejność ich wywołania będzie to determinowała w tym przypadku\n",
    "    order_data = extract()\n",
    "    order_summary = transform(order_data)\n",
    "    load(order_summary[\"total_order_value\"])\n",
    "\n",
    "# i ostatecznie uruchomienie całego grafu \n",
    "tutorial_taskflow_api()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5efca6-aec3-4583-b3d2-ceace4b01ebf",
   "metadata": {},
   "source": [
    "#### 1.1.1 Edycja konfiguracji i przebudowanie bazy Airflow.\n",
    "\n",
    "Aby wskazać inny niż domyślny folder z grafami zadań należy zmienić wpis w konfiguracji Apache Airflow w pliku `airflow.cfg`. Domyślnie jest to ścieżka `/home/spark/airflow/airflow.cfg`. Jeżeli mapowanie tej ścieżki w oprogramowaniu docker zostało wykonane poprawnie (lab 8) to ten plik powinien być widoczny również z poziomu komputera hosta jeżeli w takiej konfiguracji zostało to uruchomione.\n",
    "\n",
    "**Nie jest złym pomysłem wcześniejsze wykonanie kopii tego pliku**\n",
    "\n",
    "Wpis w konfiguracji, który wskazuje położenie grafów to `core.dags_folder`, która znajduje się na samym początku pliku:\n",
    "```bash\n",
    "[core]\n",
    "# The folder where your airflow pipelines live, most likely a\n",
    "# subfolder in a code repository. This path must be absolute.\n",
    "#\n",
    "# Variable: AIRFLOW__CORE__DAGS_FOLDER\n",
    "#\n",
    "dags_folder = /home/spark/airflow/dags\n",
    "```\n",
    "Tutaj będziemy umieszczać zdefiniowane przez nas grafy.\n",
    "Domyślnie Airflow wczytuje również wszystkie przykładowe grafy, które widoczne są na liście i na podstawie których przedstawiony był opis przykładowego grafu w lba 8. Aby wyłączyć wczytywanie przykładoweych grafów należy zmienić linię w konfiguracji z:\n",
    "```bash\n",
    "load_examples = True\n",
    "```\n",
    "na\n",
    "```bash\n",
    "load_examples = False\n",
    "```\n",
    "\n",
    "Lista grafów, które znajdują się na liście jest przeładowywana co pewien czas, ale informacje te znajdują się również w bazie, która jest konfigurowana przy pierwszym uruchomieniu Airflow.\n",
    "\n",
    "Zmiana folderu grafów to dość daleko idąca zmiana, więc zazwyczaj wykonana w celu konfiguracji zupełnie nowego środowiska. Pozbędziemy się więc z bazy informacji, które już tam zostały zapisane.\n",
    "\n",
    "Polecenie `airflow db reset` zresetuje bazę do ustawień początkowych. Po jej wykonaniu należy przeładować Apache Airflow i zalogować się nowym hasłem, które zostanie utworzone.\n",
    "Lista dostępnych grafów powinna być teraz pusta.\n",
    "\n",
    "> Więcej poleceń Airflow CLI dla bazy danych: https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.html#db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5844fc-b454-4726-9477-8813016a0631",
   "metadata": {},
   "source": [
    "#### 1.1.2 Stworzenie własnego grafu zadań z wykorzystaniem TaskFlow API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c867b46-f5b8-4a55-bd84-e64a44c280b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T11:37:22.009263Z",
     "iopub.status.busy": "2024-12-04T11:37:22.008995Z",
     "iopub.status.idle": "2024-12-04T11:37:26.733636Z",
     "shell.execute_reply": "2024-12-04T11:37:26.732590Z",
     "shell.execute_reply.started": "2024-12-04T11:37:22.009243Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beautifulsoup4                           4.12.3\n"
     ]
    }
   ],
   "source": [
    "# sprawdzamy czy pakiet BeautifulSoup jest już zainstalowany\n",
    "!pip list | grep beautifulsoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32662cf-9344-4824-99d8-7ce7b71a0a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jeżeli nie to instalujemy\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3e3e26-e3ec-48e1-a59d-9a9a4dbf4b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aktualna forma przykładowego DAG-a w stylu ETL (Extract Transform Load)\n",
    "\n",
    "import pendulum\n",
    "from datetime import datetime, timedelta\n",
    "from airflow.decorators import dag, task\n",
    "\n",
    "\n",
    "@dag(\n",
    "    schedule=timedelta(days=1),\n",
    "    start_date=pendulum.datetime(2024, 12, 4, tz=\"UTC\"),\n",
    "    catchup=False,\n",
    "    tags=[\"bgg\"],\n",
    ")\n",
    "def bgg_top_games_list():\n",
    "    \"\"\"\n",
    "    ### Zadania polegające na pobraniu aktualnego zestawienia najlepiej ocenianych gier planszowych\n",
    "    z serwisu BoardGameGeek.com w postaci dokumentu HTML, parsowanie i zapisanie w konkretnym formacie\n",
    "    danych.\n",
    "    Adres zestawienia: https://boardgamegeek.com/browse/boardgame\n",
    "    \"\"\"\n",
    "\n",
    "    # @task.bash(cwd='../data/bgg/raw/')\n",
    "    # powyższa linia nie zadziała w naszym przypadku, gdyż narzędzie cwd nie jest zainstalowane w naszym obrazie dockerowym\n",
    "    # będzie więc używana pełna ścieżka\n",
    "    @task.bash(multiple_outputs=True)\n",
    "    def extract():\n",
    "        \"\"\"\n",
    "        #### Zadanie ekstrakcji danych. Tu można podejść do tego na kilka sposobów. Np. pobrać\n",
    "        dane bezpośrednio z poziomu Pythona, ale dla, żeby pokazać szersze spektrum zadań,\n",
    "        użyte zostanie inne podejście. Dane zostaną pobrane z pomocą BashOperator i polecenia curl.\n",
    "        \"\"\"\n",
    "        base_path = '/home/spark/airflow/data/bgg/raw/'\n",
    "        filepath = f'{base_path}bgg_{datetime.strftime(datetime.now(), \"%Y-%m-%d\")}.html'\n",
    "        command = f'curl -s https://boardgamegeek.com/browse/boardgame > {filepath} && cat {filepath}'\n",
    "\n",
    "        return command\n",
    "    \n",
    "    @task()\n",
    "    def transform(bgg_page: str):\n",
    "        \"\"\"\n",
    "        #### Zadanie transformacji danych.\n",
    "\n",
    "        \"\"\"\n",
    "        from bs4 import BeautifulSoup\n",
    "\n",
    "        print(\"*\" * 50)\n",
    "        print(bgg_page)\n",
    "        html = bgg_page\n",
    "        parsed_html = BeautifulSoup(html)\n",
    "        print(parsed_html.body.find('table', attrs={'class':'collection_table'}).text)\n",
    "\n",
    "\n",
    "    bgg_page_of_the_day = extract()\n",
    "    bgg_table = transform(bgg_page_of_the_day)\n",
    "    # ostatnie zadanie w stylu load (np. do pandas)\n",
    "\n",
    "bgg_top_games_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed7b461-464c-4d17-9317-46c0c0867b25",
   "metadata": {},
   "source": [
    "Wyświetlenie listy widocznych DAG-ów przez Apache Airflow:\n",
    "\n",
    "```bash\n",
    "airflow dags list\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
