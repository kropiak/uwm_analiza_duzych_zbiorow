{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb979dc4-e992-4682-88f7-e53cfaa9476e",
   "metadata": {},
   "source": [
    "# lab_03 - Biblioteka Dask. Część 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e06c199-d2e8-49f4-9e5e-cb912369e469",
   "metadata": {},
   "source": [
    "## 1. Dask Bags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e028e713-c4e8-42c8-aa76-7ccf58e075e0",
   "metadata": {},
   "source": [
    "Struktura danych Dask Bag (z angielskiego torba, worek) została stworzona po to, aby na wysokim poziomie abstrakcji oraz, właściwie przede wszystkim, możliwe było wykonanie operacji na wielu obiektach Pythona w sposób rozproszony. Dask Bag implementuje między innymi takie metody jak `map, filter, groupby` i jest podobny w działaniu do zrównoleglonej wersji modułu itertools oraz sposobu w jaki działa PySpark RDD (Spark'owa wersja rozproszonoych ramek danych).\n",
    "\n",
    "> **Dokumentacja API Dask Bag:** https://docs.dask.org/en/latest/bag-api.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0a6ddc-b343-47f1-9283-b54465c44eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, progress\n",
    "\n",
    "client = Client(n_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc5d428-d2f6-4dc4-98cb-6174fbeab89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fbb88f-31b8-4fb0-9182-6f2f2fc87935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import json\n",
    "import os\n",
    "\n",
    "# zmienna ze ścieżką do folderu, w którym będą składowane dane\n",
    "DATAPATH = '../data'\n",
    "\n",
    "# tworzymy FOLDERY jeżeli wymagane jest stworzenie całej ścieżki folderów, jeżeli istnieje nie nadpisze\n",
    "os.makedirs(DATAPATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7feca6c-1c13-4534-bbc0-fedcd6a88499",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# wykorzystujemy wbudowaną metodę generującą dane dla określonej liczby partycji oraz ilości rekordów per partycja (plik)\n",
    "# szczegóły implementacji tej metody znajdziemy pod adresem https://docs.dask.org/en/stable/_modules/dask/datasets.html\n",
    "# wymagane będzie zainstalowanie modułu mimesis\n",
    "# tworzymy Dask Bag danymi, zmień parametry na coś, co będzie Ci odpowiadało bardziej (np. bardzo duża liczba małych plików lub mniejsza liczba plików, ale więcej wierszy w każdym)\n",
    "\n",
    "b = dask.datasets.make_people(npartitions=100, records_per_partition=10000)\n",
    "# po wykonaniu operacji 10000 rekordów zajmuje około 2.4 MB, co razem daje około 240 MB\n",
    "# można więc z powodzeniem dla zadania Dask'owi większej ilości pracy, zwiększyć te parametry kilkukrotnie\n",
    "\n",
    "# wykorzystujemy metodę map do wykonania json.dumps na każdym elemencie zawartym w Bag (a jak prześledzimy źródła to jest to słownik krotek)\n",
    "# następnie wykonujemy metode to_textfiles() z API Dask Bag i zapisujemyy każdą partition do oddzielnego pliku JSON\n",
    "# operacje te wykonają się w sposób zrównoleglony (na ile to możliwe, gdyż dane ostatcznie zapisujemy w jednym miejscu na dysku)\n",
    "\n",
    "b.map(json.dumps).to_textfiles(os.path.join(DATAPATH, '*.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7d506a-8053-48a1-ae81-7b76e662f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# korzystamy z polecenia head, które wyświetla n pierwszych linii z zadanego pliku, tu n=2\n",
    "!head -n 2 ../data/00.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abdca1e-4f90-4a2a-abad-c589e0d66c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "import dask.dataframe as dd\n",
    "import json\n",
    "\n",
    "# ponownie wykorzystujemy metodę map (tym razem mapujemy json.loads) na każdym pliku w folderze\n",
    "b = db.read_text(os.path.join(DATAPATH, '*.json')).map(json.loads)\n",
    "# graf obliczeniowy został przygotowany, ale obliczenia jeszcze się nie wykonały\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6729321-0f3a-4431-a592-66b93bf13912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# policzmy teraz ile jest osób pełnoletnich\n",
    "count_adults = b.filter(lambda record: record['age'] >= 18).count().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e765829-e290-4278-93c6-c74a3ab23fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_adults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bb659d-a401-4f95-8e9f-250e819a42c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# poprzez metodę map możemy również dobrać tylko wybrane wartości rekordu (nasz rekord to słownik)\n",
    "# dalej zliczamy częstość wartości dla cechy occupation. UWAGA, wynik może składać się z wielu wartości.\n",
    "occupations_counts = b.map(lambda record: record['occupation']).frequencies(sort=True).compute()\n",
    "occupations_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79677972-e000-40e5-afa6-2de579d36cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "adults = b.filter(lambda record: record['age'] >= 18)\n",
    "adults"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f716321-146e-4f48-a611-b89647aeb39f",
   "metadata": {},
   "source": [
    "## 2. Dask delayed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c929a581-190b-420c-bf44-3c06863bd691",
   "metadata": {},
   "source": [
    "Czasami wykorzystanie samych struktur danych, które dostarcza dask może nie wystarczyć, aby współbieżnie wykonać jakieś zadania. Dzięki dask delayed możliwe jest zrównoleglanie wykonania własnego kodu, co pozwala na optymalizowanie bardziej złożonych zadań w mniejsze fragmenty, które można przekazać do wykonania do klastra dask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb11875e-a71d-4e8d-b255-b67073d2065d",
   "metadata": {},
   "source": [
    "Poniższy przykład wykorzystania dask delayed pochodzi z dokumentacji dask dostępnej pod adresem: https://docs.dask.org/en/latest/delayed.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584e569b-cad0-4867-9b82-5b1e89195161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc(x):\n",
    "    return x + 1\n",
    "\n",
    "def double(x):\n",
    "    return x * 2\n",
    "\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "data = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2c0089-2b0a-4d19-87f8-7faf61758521",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for x in data:\n",
    "    a = dask.delayed(inc)(x)\n",
    "    b = dask.delayed(double)(x)\n",
    "    c = dask.delayed(add)(a, b)\n",
    "    output.append(c)\n",
    "\n",
    "total = dask.delayed(sum)(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0194738f-2f5d-4b82-aa80-edd98a8d184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb332aea-9276-4b36-8af7-d539b1d353f5",
   "metadata": {},
   "source": [
    "Widać tutaj, że dwie pierwsze operacje mogą być wywołane niezależnie gdyż polegają na przetworzeniu listy `x` dwiema niezależnymi funkcjami (`inc` oraz `double`), więc można je wykonać równolegle. Późniejsza operacja `add` musi już być wykonana po wykonaniu wcześniejszych obliczeń, ale dzieląc wcześniej listę `x` na fragmenty również można rozbić to zadanie na wiele mniejszych i zrównoleglić. Finalnie potrzebujemy obliczyć sumę z wyników pośrednich z funkcji `add`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5bf221-d255-48e8-976c-712e406f460e",
   "metadata": {},
   "source": [
    "**Zadanie 1**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6222ef-cc3a-4675-b0e3-83d8cfddc1cd",
   "metadata": {},
   "source": [
    "Zbiór (niemały) logów z różnych systemów do wykorzystania w badaniach. Zwróć uwagę na licencję i cytowanie w razie wykorzystania zbioru.\n",
    "\n",
    "Link: https://github.com/logpai/loghub\n",
    "\n",
    "W przykładzie poniżej zostanie wykorzystany log serwera Apache ze strony wymienionej wyżej.\n",
    "Link do pliku sample: https://github.com/logpai/loghub/blob/master/Apache/Apache_2k.log\n",
    "\n",
    "Kilka przykładowych linii z loga:\n",
    "\n",
    "```console\n",
    "[Sun Dec 04 07:18:00 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties\n",
    "[Sun Dec 04 07:18:00 2005] [error] mod_jk child workerEnv in error state 6\n",
    "[Sun Dec 04 07:18:00 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties\n",
    "[Sun Dec 04 07:18:00 2005] [error] mod_jk child workerEnv in error state 7\n",
    "[Sun Dec 04 07:45:45 2005] [error] [client 63.13.186.196] Directory index forbidden by rule: /var/www/html/\n",
    "[Sun Dec 04 08:54:17 2005] [error] [client 147.31.138.75] Directory index forbidden by rule: /var/www/html/\n",
    "[Sun Dec 04 09:35:12 2005] [error] [client 207.203.80.15] Directory index forbidden by rule: /var/www/html/\n",
    "[Sun Dec 04 10:53:30 2005] [error] [client 218.76.139.20] Directory index forbidden by rule: /var/www/html/\n",
    "```\n",
    "\n",
    "Poniżej zaprezentowany kod jest podejściem sekwencyjnym do wykonania zadania przetworzenia logów (parsowanie, konwersja daty) i w takiej formie nie można zrównoleglić go tak jak zostało to zaprezentowane na przykładzie powyżej. Przekształcenie łańcucha daty na obiekt datetime wymaga najpierw wykonania parsowania pliku. **Zastanów się i spróbuj przerobić to rozwiązanie tak, aby możliwe było użycie wywołań dask delayed w celu zrównoleglenia części funkcji, np. parsowanie danych w celu pobrania wartości kolumn niezależnie (tylko jednej na raz). Dane końcowe możesz zapisać do dask DataFrame, a następnie do plików parquet.**\n",
    "\n",
    "Aby zyskać równie jakieś porównanie między wersją sekwencyjną a zrównolegloną, dodaj odpowiednio dużo danych ze zbiorów podlinkowanych powyżej oraz zmierz czas obu rozwiązań.\n",
    "\n",
    "W procesie poszukiwania dobrego rozwiązania z użyciem dask delayed pomocny może być poradnik z dobrymi praktykami dla tej części biblioteki dask: https://docs.dask.org/en/latest/delayed-best-practices.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3743a343-b92a-4e5f-8036-ca6ecec945d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kwestia wczytania plików zostanie tu pominięta\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "lines = [\n",
    "\"[Sun Dec 04 07:18:00 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties\",\n",
    "\"[Sun Dec 04 07:18:00 2005] [error] mod_jk child workerEnv in error state 6\",\n",
    "\"[Sun Dec 04 07:18:00 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties\",\n",
    "\"[Sun Dec 04 07:18:00 2005] [error] mod_jk child workerEnv in error state 7\",\n",
    "\"[Sun Dec 04 07:45:45 2005] [error] [client 63.13.186.196] Directory index forbidden by rule: /var/www/html/\",\n",
    "\"[Sun Dec 04 08:54:17 2005] [error] [client 147.31.138.75] Directory index forbidden by rule: /var/www/html/\",\n",
    "\"[Sun Dec 04 09:35:12 2005] [error] [client 207.203.80.15] Directory index forbidden by rule: /var/www/html/\",\n",
    "\"[Sun Dec 04 10:53:30 2005] [error] [client 218.76.139.20] Directory index forbidden by rule: /var/www/html/\"\n",
    "]\n",
    "\n",
    "def parse(inp: str):\n",
    "    record = {}\n",
    "    \n",
    "    date_start = inp.find('[') + 1\n",
    "    date_end = inp.find(']')\n",
    "    date_s = slice(date_start, date_end)\n",
    "\n",
    "    level_start = inp.find('[', date_end) + 1\n",
    "    level_end = inp.find(']', level_start)\n",
    "    level_s = slice(level_start, level_end)\n",
    "\n",
    "    client_start = inp.find('[', level_end)\n",
    "    client_end = inp.find(']', client_start)\n",
    "\n",
    "    record[\"date\"] = inp[date_s]    \n",
    "    record[\"level\"] = inp[level_s]\n",
    "    record[\"client\"] = \"\" if client_start == -1 else inp[client_start + 8: client_end]\n",
    "    record[\"message\"] = inp[client_end + 2:] if record[\"client\"] else inp[level_end + 2:]\n",
    "    \n",
    "    return record\n",
    "\n",
    "def convert_date(rec):\n",
    "    rec[\"date\"] = datetime.strptime(rec[\"date\"], \"%a %b %d %H:%M:%S %Y\")\n",
    "\n",
    "    return rec\n",
    "    \n",
    "\n",
    "# przetworzenie loga\n",
    "\n",
    "output = []\n",
    "\n",
    "for line in lines:\n",
    "    record = parse(line)\n",
    "    record = convert_date(record)\n",
    "    output.append(list(record.values()))\n",
    "    \n",
    "df = pd.DataFrame(output, columns=[\"date\", \"level\", \"client\", \"message\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb19dd6a-a5fe-497c-9ee7-f7d01f9287d5",
   "metadata": {},
   "source": [
    "## 3. Dask futures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46c13f3-cf9a-431a-a97f-97c4bb79636a",
   "metadata": {},
   "source": [
    "Dask futures pozwala na wysyłanie zadań do wykonania w trybie natychmiastowym, bez blokowania schedulera jeżeli nie oczekujemy natychmiastowego wyniku do dalszego przetworzenia.\n",
    "\n",
    "> Dokumentacja: https://docs.dask.org/en/latest/futures.html\n",
    "\n",
    "Przykłady zaprezentowane poniżej wprowadzają tylko do podstawowych zastosowań dask futures i nie wyczerpują tematu. Ten mechanizm posiada również bardziej zaawansowane opcje wywołań oraz i kolejkowania i zagnieżdżania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b10bcb1-6328-4e0e-8cf5-3131b5698d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(runs: int):\n",
    "    sum = 0\n",
    "    for run in range(runs):\n",
    "        time.sleep(2)\n",
    "        sum += run\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ffb975-5fb8-456c-aaa4-8193d38bc349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# przekazujemy do clienta zadania do wykonania i możemy kontynuować wykonanie innych operacji w notebooku\n",
    "fut = client.submit(func, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227c9282-0db5-4cfb-b5bd-6627a4504b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zmienna fut przechowuje obiekt typu future, który może znajdować się w kilku stanach: zadanie w trakcie wykonania, zakończone lub błąd\n",
    "fut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7984cecf-a6bc-4c91-b623-ddb5e4306c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kiedy potrzebny jest nam wyniki zadania typu future możemy wywołać metodę result, ale teraz nastąpi blokowanie wykonania kolejnych zadań, aż do\n",
    "# momentu aż to wywołanie future zakończy swoje działanie i zwróci wynik lub zakończy się błędem\n",
    "fut.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3665654-cb33-401b-af07-bb3324875dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "tasks = 100\n",
    "futures = []\n",
    "\n",
    "# tworzymy 100 zadań i wysyłamy je do wykonania dla skonfigurowanego klienta\n",
    "for task in range(tasks):\n",
    "    futures.append(client.submit(func, random.randint(1, 15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3a3ec6-ff7e-4a27-9f1a-0e0d2c94498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jeżeli zadań jest dużo i potrzebujemy zebrać wszystkie wyniki przed kontynuowaniem pracy, możemy to zrobić\n",
    "# poprzez wywołanie metody gather dla obiektu client\n",
    "\n",
    "results = client.gather(futures)\n",
    "sum(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e953d02-1577-4f70-8baf-f5b5660bae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_list = [random.randint(1, 30) for _ in range(100)]\n",
    "futures2 = client.map(func, task_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d114b795-51ec-45b0-be30-b38347f0f421",
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = client.gather(futures2)\n",
    "sum(results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4545641b-ec20-4d5e-a68f-556a53bd319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jeżeli istnieje możliwość przetwarzania już zwróconych wyników to możemy wykorzystać funkcję as_completed i pobierać ukończone zadania\n",
    "# w paczkach w kolejności, w jakiej zostały ukończone i przetwarzać je dalej, aż do momentu ukończenia wszystkich futures\n",
    "# aby widzieć jak przybywają kolejne paczki wykonanych zadań, należy najpierw ponownie uruchomić je na klastrze, ale najlepiej\n",
    "\n",
    "from dask.distributed import as_completed\n",
    "\n",
    "# display(len(set(task_list)))  # kontrolnie\n",
    "\n",
    "for num, batch in enumerate(as_completed(futures2, with_results=True).batches(), start=1):\n",
    "   display(f\"Batch {num}\")\n",
    "   for future, result in batch:\n",
    "       # przetwarzaj kolejną paczkę ukończonych futures\n",
    "       display(f\"Future {future} result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64677e3c-cac9-460b-8527-0e89a291ef41",
   "metadata": {},
   "source": [
    "Zwróć uwagę na wyniki oraz widok w dashboardzie dla kilkukrotnego wywołania tych samych futures. Czy zauważyłeś coś ciekawego?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132379c6-af46-4285-9442-3af8460ad204",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.who_has(futures2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30226185-fb7e-43d8-8c23-815bc79dcaa3",
   "metadata": {},
   "source": [
    "## 4. Lokalny klaster w pracowni komputerowej."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc20ee36-3718-4a2b-8f6a-5e239702d085",
   "metadata": {},
   "source": [
    "Scenariusz skonfigurowania i przetestowania klastra w pracowni komputerowej zakłada:\n",
    "* zdefiniowanie jednego lub kilku maszyn jako schedulerów klastra Dask (można część zasobów skonfigurować jednocześnie jako worker),\n",
    "* podłączenie do schedulera workerów poprzez sieć lokalną, które są uruchomione na maszynach w pracowni komputerowej,\n",
    "* wykonanie kilku zadań o zróżnicowanym stopniu skomplikowania i wykorzystania zasobów na skonfigurowanym klastrze."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207256e0-5406-4a36-af34-b010897ae812",
   "metadata": {},
   "source": [
    "### 4.1 Wstępne wymagania elementów klastra.\n",
    "\n",
    "Dask (oraz większość systemów tego typu) zaleca, aby poszczególne elementy klastra zostały skonfigurowane na tej samej wersji środowiska obliczeniowego. W związku z tym zarówno na maszynie będącej schedulerem jak i na workerach należy przygotować środowisko wirtualne Pythona z tą samą wersją interpretera.\n",
    "\n",
    "> Dokumentacja dla CLI dask poniższych poleceń: https://docs.dask.org/en/latest/deploying-cli.html\n",
    "\n",
    "**Krok 1.**  \n",
    "Przygotowanie środowiska wirtualnego na wszystkich maszynach, które mają być częścią klastra:\n",
    "* instalujemy interpreter Pythona w tej samej wersji,\n",
    "* instalujemy również moduł `dask distributed`,\n",
    "\n",
    "**Krok 2**  \n",
    "Uruchamiamy scheduler. Na wybranej maszynie (maszynach) uruchamiamy scheduler za pomocą polecenia `dask scheduler` w konsoli Pythona z aktywowanym środowiskiem wirtualnym. Można równiez uruchomić scheduler w sposób przedstawiany do tej porty w załączonych notebookach. Notujemy adres IP oraz port, na którym działa scheduler klastra dask.\n",
    "\n",
    "**Krok 3**  \n",
    "Dołączamy kolejne workery do klastra. W konsoli Python pozostałych maszyn, z aktywowanym środowiskiem wirtualnym Python, uruchamiamy workery poleceniem `dask worker 192.168.1.1:8786 --nworkers 1 --nthreads 1 --memory-limit 2GB`, ustawiając odpowiedni adres IP oraz ilość procesów, wątków dla każdego workera.\n",
    "\n",
    "**Krok 4**  \n",
    "Z poziomu schedulera uruchamiamy zadania i śledzimy ich wykonanie w dashboardzie.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9043ba-1b68-4363-a912-f9e6b18e16c4",
   "metadata": {},
   "source": [
    "### 4.2 Scenariusze wykorzystania klastra.\n",
    "\n",
    "**Scenariusz 1**  \n",
    "Uruchomienie pierwszego testowego zadania polegającego na przetworzeniu stosunkowo dużej tablicy dask na dostępnych workerach. Pamiętaj, że tworząc obiekt klienta możesz wskazać ile workerów chcesz wykorzystać na tym kliencie i to nie musi być całość dostępnych zasobów. Ważne jest również prześledzenie utylizacji workerów oraz struktury zadań (np. transfer danych między workerami) w dashboardzie. Dopasuj wielkość tablicy tak, aby wykorzystać dostępną pamięć RAM klastra w dość znacznym stopniu.\n",
    "\n",
    "**Scenariusz 2**  \n",
    "W tym zadaniu należy przygotować wybrany zbiór danych na komputerze, na którym uruchomiony został scheduler i wykonać proces wczytania danych do wybranej kolekcji dask w sposób rozproszony. Ponownie monitoruj sposób pracy klastra w dashboardzie. Pierwsze uruchomienie powinno odbyć się na stosunkowo niewielkiej liczbie danych, z czasem należy ją zwiększyć tak, aby klaster był faktycznie zajęty czymś przez pewien czas (np. kilka minut). \n",
    "Tu można przywołać zadania z poprzedniego labu z danymi ze zbioru `private_instagram` lub zbiorami przedstawionymi w tym labie.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982ba547-be47-412d-98f0-79ca7362bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zamykamy połączenie z klientem\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05639093-2626-4f99-bd28-a547ac3f5119",
   "metadata": {},
   "source": [
    "## **Zadania c.d.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69354b7-806d-40e5-8091-5aef1f43f708",
   "metadata": {},
   "source": [
    "**Zadanie 2**  \n",
    "Wykorzystując przykłady zaprezentowane w labie wykonaj na danych `people` (możesz zmniejszyć lub zwiększyć ich wolumen w zależności od potrzeb) operację z użyciem Dask bag, która polegać będzie na przetworzeniu wszystkich plików i zapisaniu do plików o nazwie `expired_{partition}.json` rekordów, których ważność karty kredytowej wygasła (jest to wartość w formacie miesiąc/rok). Zapisując ustaw finalną liczbę plików na 10 jeżeli była inna. Możesz to zrobić poprzez zmianę ilość partycji dask bag (patrz link do API na początku laba).\n",
    "\n",
    "**Zadanie 3**  \n",
    "Wybierając z danych `people` dane tylko osób dorosłych (zaprezentowane w przykładach w tym labie) przechowaj je w obiekcie typu `bag`, a następnie zapisz je do dask dataframe za pomocą metody `to_dataframe` (pamiętaj o tym jaka jest aktualna struktura pojedynczego rekordu), a następnie zapisz do jednego pliku w formacie parquet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
