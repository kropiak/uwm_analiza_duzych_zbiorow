{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27e4442b-6e13-4a4b-9a4f-0d3676bd78ad",
   "metadata": {},
   "source": [
    "# lab 4 - Wykorzystanie biblioteki Dask w zadaniach Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2742d07-71d0-4335-839e-eb3faa714303",
   "metadata": {},
   "source": [
    "## 1. Eksperymetny bez wykorzystania biblioteki Dask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f36a3a-1993-48d2-92fb-4a6af4768b77",
   "metadata": {},
   "source": [
    "**Przykład 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0428bd3b-c4a7-48ba-8cbb-952ea4549e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_rcv1, fetch_covtype\n",
    "import numpy as np\n",
    "\n",
    "# pobranie zbioru RCV1 i zapisanie w postaci binarnej za pomocą ZARR\n",
    "# dokumentacja sklearn z opcjami pobrania zbioru: https://scikit-learn.org/1.5/modules/generated/sklearn.datasets.fetch_rcv1.html#sklearn.datasets.fetch_rcv1\n",
    "# rcv1 = fetch_rcv1()\n",
    "# dane w oryginale są zwracane jako tablice rzadkie (sparse) i trzeba je zamienić\n",
    "# na tablicę gęstą\n",
    "\n",
    "# UWAGA: ta operacja spowoduje potrzebę zaalokowania około 283 GB pamięci RAM, co zapewne się nie uda\n",
    "# więc tutaj trzeba będzie użyć klastra do realizacji tego zadania\n",
    "# X, y = rcv1.data.toarray(), rcv1.target.toarray()\n",
    "\n",
    "# mniejszy zbiór do przeprowadzenia przykładu\n",
    "cov = fetch_covtype()\n",
    "X, y = cov.data, cov.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915bafe2-7e70-47a6-8f15-351329310629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "data = [X, da.atleast_2d(y).T]\n",
    "ddf = da.concatenate(data, axis=1).to_dask_dataframe(columns=cov.feature_names + cov.target_names, index=None).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab1f5ce-3d06-446f-9138-e48ca8bf1785",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = ddf.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966dff42-6531-4b0f-9ffc-cc48d4394bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c9b614-ec52-46ac-b029-ba5db429aeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uwaga na faktyczny typ danych tej ramki!\n",
    "type(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44948bce-6559-488a-b69f-3ae13faefada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fastparquet\n",
    "\n",
    "DATADIR = './data/'\n",
    "# os.makedirs(DATADIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b709cfb2-9d65-4b86-9b59-6e051cf4a696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to pandas DataFrame (brak podziału na partycje)\n",
    "ddf.to_parquet(os.path.join(DATADIR, 'data.parquet'))\n",
    "\n",
    "# ddf.to_parquet(DATADIR, name_function=lambda x: f\"data-{x}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39221e3-9c05-4747-94f5-6c4c9b3e16b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jako, że ta wersja biblioteki XGBoost wymaga, aby wartości dla target(y) były indeksowane od 0\n",
    "# należy wykonać enkodowanie dla tego datasetu\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1949a5e2-30c1-449b-8f23-58fdbb3e7a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# X, y = load_breast_cancer(return_X_y=True)\n",
    "n_splits = 5\n",
    "\n",
    "def fit_and_score(estimator, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Fit the estimator on the train set and score it on both sets\"\"\"\n",
    "    estimator.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n",
    "\n",
    "    train_score = estimator.score(X_train, y_train)\n",
    "    test_score = estimator.score(X_test, y_test)\n",
    "\n",
    "    return estimator, train_score, test_score\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=94)\n",
    "\n",
    "\n",
    "# ta implementacja algorytmu XGBoost wykorzystuje joblib i domyślnie jego praca jest zrównoleglana\n",
    "# wydajność użycia klasycznego podejścia vs. Dask na klastrze lokalnym prawdopodobnie przyniesie gorsze\n",
    "# rezultaty dla tego drugiego rozwiązania, ale wymaga to sprawdzenia\n",
    "\n",
    "# jednak jeżeli weźmiemy pod uwagę ograniczenia pamięci, które możemy napotkać pracując bez wykorzystania\n",
    "# daska, może okazać się, że podejście klasyczne nie będzie możliwe do uruchomienia jeżeli nie\n",
    "# dysponujemy wystarczającą ilością zasobów sprzętowych\n",
    "\n",
    "clf = xgb.XGBClassifier(tree_method=\"hist\", early_stopping_rounds=3)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for train, test in tqdm(cv.split(X, y), desc=\"Training...\", total=n_splits):\n",
    "    X_train = X[train]\n",
    "    X_test = X[test]\n",
    "    y_train = y[train]\n",
    "    y_test = y[test]\n",
    "    est, train_score, test_score = fit_and_score(\n",
    "        clone(clf), X_train, X_test, y_train, y_test\n",
    "    )\n",
    "    results[est] = (train_score, test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c879ba6c-80ee-4eba-926a-eac362ca37c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold_n, fold in enumerate(results.items(), start=1):\n",
    "    print(f\"Fold {fold_n}: train_score: {fold[1][0]}, test_score: {fold[1][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b1dbce-c1bd-4767-a623-7b3a9c74162c",
   "metadata": {},
   "source": [
    "## 2. Eksperyment z wykorzystaniem Dask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1df3eb6-ccb2-4b81-b149-49e5b84fef06",
   "metadata": {},
   "source": [
    "### 2.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dde4c2-75d0-4ead-babc-2de134d517d0",
   "metadata": {},
   "source": [
    "**Przykład 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0426ac65-9c39-4b07-9e35-45fa5e7b91d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "from dask_ml.model_selection import KFold\n",
    "import xgboost as xgb\n",
    "from dask.distributed import LocalCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4b34df-d1bd-4f1a-937d-3787b3a275b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# konwersja numpy array na Dask array\n",
    "X = da.from_array(X)\n",
    "y = da.from_array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd5dd4f-4162-454a-b462-50b206412b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# szczegóły definiowania parametrów: https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n",
    "# tu klasyfikacja wieloklasowa\n",
    "\n",
    "params = {'objective': 'multi:softmax',\n",
    "          'max_depth': 4, 'eta': 0.01, 'subsample': 0.5,\n",
    "          'min_child_weight': 0.5,\n",
    "         'num_class': 7}\n",
    "\n",
    "\n",
    "n_splits = 5\n",
    "cv = KFold(n_splits=n_splits, shuffle=True, random_state=94)\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "with LocalCluster(n_workers=4) as cluster:\n",
    "    display(cluster)\n",
    "    with cluster.get_client() as client:\n",
    "\n",
    "        for i, (train, test) in enumerate(cv.split(X, y)):\n",
    "            \n",
    "            X_train = X[train, :-1]\n",
    "            X_test = X[test, :-1]\n",
    "            y_train = y[train]\n",
    "            y_test = y[test]\n",
    "            \n",
    "            d_train = xgb.dask.DaskDMatrix(client, X_train, y_train, enable_categorical=True)\n",
    "            model = xgb.dask.train(client, params=params, dtrain=d_train)\n",
    "            predictions[f'fold_{i}'] = xgb.dask.predict(client, model, X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895dd87a-eab7-4176-ab5c-ada0fc2c14e9",
   "metadata": {},
   "source": [
    "### 2.2 Rozbicie całego procesu na niezależne zadania"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64aff09-b212-4310-adbc-fca024b67722",
   "metadata": {},
   "source": [
    "W pierwszej fazie zazwyczaj zajęlibyśmy się procesem ekstrakcji, oczyszczania i wstępnego przetwarzania danych, jednak póki co ten etap zostanie tutaj pominięty.\n",
    "\n",
    "W dokumentacji biblioteki Dask znajdziemy cały dział poruszający temat Machine Learning z wykorzystaniem tej biblioteki na różnych etapach tego procesu.\n",
    "\n",
    "> Dokumentacja Dask ML: https://ml.dask.org/\n",
    "\n",
    "Są tam również przykłady (chociaż dość ubogie i czasem nie działające z najnowszymi wersjami bibliotek zależnych), które obrazują w jakich przypadkach można skorzystać z możliwości Dask w kontekście ML. Należy dość dokładnie przeczytać uwagi i wskazówki, które się tam znajdują gdyż nie wszystkie elementy (np. znane biblioteki do tworzenia modeli ML) współpracują z Dask w tzw. trybie \"out of the box\" i trzeba wykorzystywać wrappery lub specjalnie przygotowane integracje, np. bibliotekę Skorch, która pozwala korzystać z Pytorch w sposób bardziej kompatybilny ze scikit-learn a co za tym idzie również z Dask, gdyż tutaj położono największy nacisk na integrację."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c627145-d240-4499-927e-77659d424698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ładowanie danych\n",
    "# w zależności od potrzeb dane mogą być przekazane do etapu treningowego w różnym formacie: dask dataframe, dask array lub inny.\n",
    "# dodatkowo dzięki wielu formatom przechowywania danych, szczególnie w kontekście big data, gdzie właściwy dobór bibliotek zależy od\n",
    "# dostępnej infrastruktury.\n",
    "# Dane mogą również być wczytywane i następnie przetwarzane w paczkach (ang. batch), co dodatkowo może narzucić pewne ograniczenia co\n",
    "# do miejsca uruchomienia procesu ładowania danych (może tylko scheduler, a może zdalnie na klastrze) lub konieczność rozproszenia\n",
    "# danych po całym klastrze.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f259a1-da1b-436d-a347-7ef50c607924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Podział danych na potrzeby etapu trenowania modelu.\n",
    "# Jeżeli docelowo dysponujemy dużym zbiorem danych, nie oznacza to wcale, że tak jak klasycznie zazwyczaj się to odbywa,\n",
    "# dzielimy cały zbiór na część treningową oraz testową (np. w proporcjach 80/20, 70/30 czy innych) i uruchamiamy na nich\n",
    "# trening wybranego modelu. Lepszym pomysłem jest dobranie odpowiedniej, reprezentatywnej próbki danych, na których wykonamy\n",
    "# wstępny trening. W zależności od różnorodności zbioru może się okazać, że wyniki będzie wystarczająco dobry. Pamiętajmy również, że\n",
    "# finalna wielkość modelu i ilość zasobów potrzebnych, żeby go przechowywać w stanie dostępnym dla etapu inferencji może być kosztowne\n",
    "# i również często wymaga pewnej optymalizacji. A skoro w danym przypadku mniejszy model jest porównywalnie dobry z modelem większym,\n",
    "# mniejszy wygrywa. Również w kontekście szybkości inferencji w fazie produkcyjnej.\n",
    "\n",
    "\n",
    "# zobacz przykład podziału danych w punkcie 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46244031-188b-498e-b4b6-b9549ab567bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Trening modelu.\n",
    "# Ta faza zazwyczaj zajmuje dużo czasu, w kontekście samego treningu, ale również w kontekście iteracyjnej natury tego etapu.\n",
    "# Szukamy optymalnych parametrów modelu (tu mogą pomóc dodatkowe narzędzia i techniki takie jak Optune, ML Flow, grid search),\n",
    "# eksperymentujemy z doborem danych (tu często poprzedzamy to fazą feature engineering).\n",
    "\n",
    "# trening modelu na dużej ilości danych można wykonać na kilka sposobów.\n",
    "# 1. Użycie modelu ensemble, który trenuje większą ilość mniejszych modeli na fragmentach danych.\n",
    "# Ważne jest, aby podział danych odbył się zgodnie z rozkładem w zbiorze oryginalnym, w przeciwnym wypadku część\n",
    "# modeli może dość mocno wpływać na ogólny wynik całego modelu.\n",
    "# zobacz: https://ml.dask.org/modules/generated/dask_ml.ensemble.BlockwiseVotingClassifier.html#dask_ml.ensemble.BlockwiseVotingClassifier\n",
    "\n",
    "# 2. Wykorzystanie jednej z klas biblioteki Dask, która pozwala wykorzystać modele z biblioteki scikit-learn, które wspierają\n",
    "# operację partial_fit, która pozwala na trenowanie modelu na zbiorze danych uczących, który jest podzielony na części (nie \n",
    "# mylić z paczką, ang. batch, która jest dzielona ze zbioru treningowego) i dzięki temu można tu przekazać np. Dask Array jako dane wejściowe.\n",
    "# zobacz: https://ml.dask.org/modules/generated/dask_ml.wrappers.Incremental.html#dask_ml.wrappers.Incremental\n",
    "# oraz https://scikit-learn.org/0.15/modules/scaling_strategies.html\n",
    "\n",
    "# przykłady\n",
    "\n",
    "# przykład z ofocjalnej dokumentacji: https://examples.dask.org/machine-learning/incremental.html\n",
    "# inne przykłady\n",
    "# https://skorch.readthedocs.io/en/stable/user/parallelism.html\n",
    "# https://github.com/skorch-dev/skorch/blob/master/notebooks/MNIST.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de047e81-b825-4e8e-bbb5-c1a230677b06",
   "metadata": {},
   "source": [
    "**Przykład 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366171fe-e28f-40ea-80c9-824b49a9b2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "# pamiętaj o zamykaniu klientów lub używaniu już wcześniej stworzonego\n",
    "client = Client(n_workers=4, threads_per_worker=2, memory_limit=\"4GB\")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b456ed3-3444-450b-acfc-ce4b17da7974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# przykład z użyciem incremental learning z dokumentacji dask\n",
    "# z lekką modyfikacją\n",
    "\n",
    "import dask\n",
    "import dask.array as da\n",
    "from dask_ml.datasets import make_classification\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from dask_ml.wrappers import Incremental\n",
    "\n",
    "\n",
    "# dostosuj wielkość zbioru oraz ilość/wielkość chunka\n",
    "# wielkość chunka niedobrana prawidłowo do pamięci na workerze może\n",
    "# skutecznie zakończyć przeliczanie całego grafu\n",
    "# przy parametrach poniżej potrzeba około 38GB pamięci RAM, ale przy dobrze\n",
    "# dobranych chunkach obliczymy to na dużo mniejszej ilości zasobów\n",
    "# wykonanie poniższego kodu na mojej maszynie z zadanymi parametrami klastra lokalnego\n",
    "# zajęło kilkanaście minut\n",
    "n, d = 10000000, 500\n",
    "X, y = make_classification(n_samples=n, n_features=d,\n",
    "                           chunks=n // 64, flip_y=0.2)\n",
    "display(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "display(X_train)\n",
    "\n",
    "# jeżeli dysponujemy wystarczająco dużą ilością pamięci RAM rozproszoną po workerach\n",
    "# to możemy przechować dane właśnie tam w celu przyspieszenia części obliczeń\n",
    "# X_train, X_test, y_train, y_test = dask.persist(X_train, X_test, y_train, y_test)\n",
    "\n",
    "\n",
    "classes = da.unique(y_train).compute()\n",
    "# classes\n",
    "\n",
    "est = SGDClassifier(loss='log_loss', penalty='l2', tol=1e-3)\n",
    "inc = Incremental(est, scoring='accuracy')\n",
    "\n",
    "inc.fit(X_train, y_train, classes=classes)\n",
    "inc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b220312-b804-481d-8f1e-71b9204f7d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Serializacja modelu i jego wczytywanie.\n",
    "# Serializacja modelu jest konieczna ze względu na chęć przechowania go w formie bardziej trwałej niż w pamięci operacyjnej, ale\n",
    "# również ze względu na niski koszt jego wczytania wględem konieczności ponownego jego trenowania, to oczywiste.\n",
    "# Często również w cyklu życia modeli ML następuje ich aktualizacja oraz archiwizacja modeli aktualnie nie używanych.\n",
    "\n",
    "\n",
    "def save_model(model):\n",
    "    pass\n",
    "\n",
    "def load_model(path):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458c52a2-963b-4c8d-be0c-7502cf4f2593",
   "metadata": {},
   "source": [
    "**Przykład 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8304e3ae-a5af-45bc-abac-be8ccf06110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Inferencja\n",
    "# W tej fazie podajemy do modelu dane, w kontekście których cały ten proces był wykonany. Chcemy się dowiedzieć kto, z jaką\n",
    "# szansą porzuci w niedalekiej przyszłości naszą usługę, co jest na zdjęciu lub czy na zdjęciu jest coś co nas szczególnie interesuje,\n",
    "# a może spełniamy prośbę użytkownika o wygenerowanie zabawnego tekstu życzeń urodzinowych dla najlepszego kolegi.\n",
    "\n",
    "# przykładowy przepływ z dokumentacji dask - batch prediction\n",
    "# przykład nie jest kompletny\n",
    "\n",
    "\n",
    "from dask.distributed import LocalCluster\n",
    "\n",
    "cluster = LocalCluster(processes=False)\n",
    "client = cluster.get_client()\n",
    "\n",
    "# tu możemy wykorzystać poznany już Dask Bag\n",
    "filenames = [...]\n",
    "\n",
    "def predict(filename, model):\n",
    "    data = load(filename)\n",
    "    result = model.predict(data)\n",
    "    return result\n",
    "\n",
    "model = client.submit(load_model, path_to_model)\n",
    "predictions = client.map(predict, filenames, model=model)\n",
    "# czekamy na wszystkie wyniki\n",
    "results = client.gather(predictions)\n",
    "\n",
    "# lub wykorzystując przykład z lab_3 z użyciem dask.distributed.as_completed możemy odbierać wyniki paczkami i przetwarzać je dalej"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd7053b-4dab-43b1-a072-0e035ec42c50",
   "metadata": {},
   "source": [
    "**Przykład 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e783491-accc-4895-a1b7-f8dd08b090f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skrypt pokazujący jak można wykorzystać Dask do rozproszonego\n",
    "# poszukiwania najbardziej optymalnych hiperparametrów danego klasyfikatora z wybranymi danymi\n",
    "# Dzięki temu możemy na niewielkiej próbce danych (ale reprezentatywnej) dobrać\n",
    "# hiperparametry modelu i przejść do szkolenia modelu docelowego na większej ilości danych\n",
    "\n",
    "from dask_ml.model_selection import IncrementalSearchCV\n",
    "import numpy as np\n",
    "from dask_ml.datasets import make_classification\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "X, y = make_classification(n_samples=5000000, n_features=20,\n",
    "                           chunks=100000, random_state=56)\n",
    "\n",
    "\n",
    "model = SGDClassifier(tol=1e-3, penalty='elasticnet', random_state=0)\n",
    "\n",
    "params = {'alpha': np.logspace(-2, 1, num=1000),\n",
    "          'l1_ratio': np.linspace(0, 1, num=1000),\n",
    "          'average': [True, False]}\n",
    "\n",
    "# search = IncrementalSearchCV(model, params, random_state=0)\n",
    "search.fit(X, y, classes=[0, 1])\n",
    "\n",
    "search = IncrementalSearchCV(model, params, random_state=0,\n",
    "                             n_initial_parameters=1000,\n",
    "                             patience=20, max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1246fe-bb73-459e-8778-94ad200e2c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# najlepszy model oraz najlepsze parametry\n",
    "# więcej o tym przykładzie: \n",
    "# https://ml.dask.org/modules/generated/dask_ml.model_selection.IncrementalSearchCV.html#dask_ml.model_selection.IncrementalSearchCV\n",
    "\n",
    "search.best_score_, search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f95920-c18b-4a20-8e35-9f64a4404e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e5b1dd-95a5-4954-bafc-c70e9a251bcc",
   "metadata": {},
   "source": [
    "### Zadania"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3816e3c9-d6c3-449a-9446-933b78304ebb",
   "metadata": {},
   "source": [
    "**Zadanie 1**  \n",
    "Uruchom przykład Incremental learning z punktu 2.1 dobierając parametry tak, aby ilość danych do przeliczenia była większa niż sumaryczna ilość pamięci RAM workerów. Obserwuj daszboard i w razie niepowodzenia dostosuj wielkość i ilość chunków tak, aby obliczenia się wykonały na tych samych parametrach workerów. Zobacz jak wygląda struktura pamięci na workerach, czy nie dochodzi do zrzucania pamięci na dysk (zapewne będzie on wąskim gardłem, więc w menedżerze będzie widać jego mocne obciążenie). Zastanów się czy można to jakoś zoptymalizować przy dostępnych workerach i wykonaj kilka eksperymentów szukając większej wydajności i krótszego czasu wykonania całego zadania.\n",
    "\n",
    "**Zadanie 2**  \n",
    "Dokonaj serializacji modelu z zadania 1 na dysk i następnie go wczytaj ponownie tak, aby można było uruchomić na nim predykcję dla tablic X_test oraz y_test (dla użycia miar klasyfikacji) i wyświetl macierz klasyfikacji (confusion matrix).\n",
    "\n",
    "**Zadanie 3**  \n",
    "Korzystając z danych stworzonych w zadaniu 1 uruchom poszukiwanie optymalnych parametrów modelu tak jak zostało to zaprezentowane w przykładzie 5. Ta metoda powinna sama wybierać modele obiecujące i trenować je na większej liczbie danych porzucając jednocześnie modele, które nie rokują.\n",
    "Sprawdź jak wyglądają najlepsze wyliczone parametry vs. te użyte w zadaniu 1 i ewentualnie dopasuj próbkę danych jeżeli jej inicjalna wielkość nie pozwala na wykonanie zadania (zwróć uwagę na ilość i wielkość chunków w przykładzie 3 oraz 5, w tym drugim jest ich znacznie więcej, co przyspiesza poszukiwanie optymalnych parametrów)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
