{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b16751d-a424-4d5e-84ea-3384f5674aee",
   "metadata": {},
   "source": [
    "# Lab 7 - PySpark i SQL, wiaderkowanie i partycjonowanie plików oraz zapi w hurtowni danych Hive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1443a061-cd89-407d-b328-14a501c6e6d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T12:07:50.375157Z",
     "iopub.status.busy": "2024-11-19T12:07:50.373219Z",
     "iopub.status.idle": "2024-11-19T12:07:50.403261Z",
     "shell.execute_reply": "2024-11-19T12:07:50.398472Z",
     "shell.execute_reply.started": "2024-11-19T12:07:50.375013Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dc59125-9750-4d7b-93e5-5d1eab5a83a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T12:07:50.405740Z",
     "iopub.status.busy": "2024-11-19T12:07:50.405140Z",
     "iopub.status.idle": "2024-11-19T12:07:55.360517Z",
     "shell.execute_reply": "2024-11-19T12:07:55.359172Z",
     "shell.execute_reply.started": "2024-11-19T12:07:50.405648Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/19 12:07:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://c1a39d506425:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Create-DataFrame</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=Create-DataFrame>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# ścieżka do bazy danych hurtowni danych oraz plików\n",
    "# należy dostosować do ścieżki względnej, w której umieszczony został bieżący notebook\n",
    "warehouse_location = '/opt/spark/work-dir/lab_07/metastore_db'\n",
    "\n",
    "# utworzenie sesji Spark, ze wskazaniem włączenia obsługi Hive oraz\n",
    "# lokalizacją przechowywania hurtowni danych\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[2]\")\\\n",
    "        .appName(\"Apache SQL and Hive\")\\\n",
    "        .config(\"spark.memory.offHeap.enabled\",\"true\")\\\n",
    "        .config(\"spark.memory.offHeap.size\",\"4g\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse_location)\\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ff70d5-c0e3-42b3-a63d-e1035c976b16",
   "metadata": {},
   "source": [
    "## 1. Spark i SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cfd5f5-af66-4d72-bfa8-b196ccaf786c",
   "metadata": {},
   "source": [
    "Spark umożliwia zarejestrowanie obiektu DataFrame jako widoku, co umożliwia korzystanie z niego w sposób bardzo zbliżony do pracy z językiem SQL. Poniżej przykład."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e9b4be41-a7fd-4be0-8752-da78d9543757",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T11:39:23.010078Z",
     "iopub.status.busy": "2024-11-20T11:39:23.009573Z",
     "iopub.status.idle": "2024-11-20T11:39:24.660221Z",
     "shell.execute_reply": "2024-11-20T11:39:24.658450Z",
     "shell.execute_reply.started": "2024-11-20T11:39:23.010055Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# dostosuj ścieżkę do pliku do swoich danych, tutaj został utworzony mniejszy zbiór niż w poprzednim labie\n",
    "df = spark.read.csv('../lab_06/employee_1m.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "3f76ff22-0cfd-4865-a03c-7eb45059c515",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T11:39:25.370252Z",
     "iopub.status.busy": "2024-11-20T11:39:25.369023Z",
     "iopub.status.idle": "2024-11-20T11:39:25.384611Z",
     "shell.execute_reply": "2024-11-20T11:39:25.383213Z",
     "shell.execute_reply.started": "2024-11-20T11:39:25.370182Z"
    }
   },
   "outputs": [],
   "source": [
    "# tworzymy widok tymczasowy w pamięci węzła\n",
    "df.createOrReplaceTempView(\"EMPLOYEE_DATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "275b1480-7790-4e50-8fff-008b235234e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T20:48:05.322255Z",
     "iopub.status.busy": "2024-11-19T20:48:05.321258Z",
     "iopub.status.idle": "2024-11-19T20:48:05.535017Z",
     "shell.execute_reply": "2024-11-19T20:48:05.533842Z",
     "shell.execute_reply.started": "2024-11-19T20:48:05.322174Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='employee_bucketed', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='employees_partitioned_lastname', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='EMPLOYEE_DATA', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wypisanie tabeli, zwróć uwagę na to, czy stworzona tabela jest tymczasowa czy trwała\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "1594570a-d565-40a4-bc0c-2c748a085bbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T11:39:29.951040Z",
     "iopub.status.busy": "2024-11-20T11:39:29.950010Z",
     "iopub.status.idle": "2024-11-20T11:39:30.150860Z",
     "shell.execute_reply": "2024-11-20T11:39:30.149288Z",
     "shell.execute_reply.started": "2024-11-20T11:39:29.950999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------------+---+-------+\n",
      "| id|firstname|    lastname|age| salary|\n",
      "+---+---------+------------+---+-------+\n",
      "|  1|  Wisława|Mieczykowski| 24|6409.28|\n",
      "|  2|    Agata|      Szczaw| 46|8980.03|\n",
      "|  3|     Adam|Mieczykowski| 19|8817.88|\n",
      "|  4|Krzysztof|Mieczykowski| 18|7531.72|\n",
      "+---+---------+------------+---+-------+\n",
      "\n",
      "+----------+\n",
      "| firstname|\n",
      "+----------+\n",
      "|   Wisława|\n",
      "|     Agata|\n",
      "|      Adam|\n",
      "| Krzysztof|\n",
      "| Krzysztof|\n",
      "|  Zbigniew|\n",
      "| Krzysztof|\n",
      "|      Adam|\n",
      "|      Adam|\n",
      "|Mieczysław|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pobranie danych jak z tabeli SQL\n",
    "spark.sql(\"Select * from EMPLOYEE_DATA limit 4\").show()\n",
    "spark.sql(\"select firstname from EMPLOYEE_DATA\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57bd2477-3289-4512-a0f7-d5b2100b4fd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T19:46:13.107477Z",
     "iopub.status.busy": "2024-11-19T19:46:13.106723Z",
     "iopub.status.idle": "2024-11-19T19:46:14.966626Z",
     "shell.execute_reply": "2024-11-19T19:46:14.964656Z",
     "shell.execute_reply.started": "2024-11-19T19:46:13.107403Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+------------------+\n",
      "| firstname|count(firstname)|       avg(salary)|\n",
      "+----------+----------------+------------------+\n",
      "|   Wisława|           99640| 7848.952735246865|\n",
      "|Mieczysław|          100565| 7851.463656441106|\n",
      "|     Agata|          100127| 7853.484773637447|\n",
      "| Krzysztof|           99704|7849.4506867327345|\n",
      "|     Marek|           99741| 7846.229979948024|\n",
      "|      Adam|          100170| 7847.951781371652|\n",
      "| Katarzyna|          100650| 7853.827649180384|\n",
      "|  Wojciech|           99553| 7856.233001315921|\n",
      "|  Zbigniew|           99699|  7850.48046128846|\n",
      "|Aleksandra|          100151| 7856.509769248448|\n",
      "+----------+----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select firstname, count(firstname), avg(salary) from EMPLOYEE_DATA group by firstname\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4013cf1d-d6e9-4939-badf-e1512bac553d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T19:53:24.138228Z",
     "iopub.status.busy": "2024-11-19T19:53:24.137195Z",
     "iopub.status.idle": "2024-11-19T19:53:24.272964Z",
     "shell.execute_reply": "2024-11-19T19:53:24.271125Z",
     "shell.execute_reply.started": "2024-11-19T19:53:24.138199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-------+------------+\n",
      "|firstname|    lastname| salary|after_rising|\n",
      "+---------+------------+-------+------------+\n",
      "|  Wisława|Mieczykowski|6409.28|     7050.21|\n",
      "|    Agata|      Szczaw|8980.03|     9878.03|\n",
      "|     Adam|Mieczykowski|8817.88|     9699.67|\n",
      "|Krzysztof|Mieczykowski|7531.72|     8284.89|\n",
      "|Krzysztof|  Wróblewski|8371.61|     9208.77|\n",
      "+---------+------------+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rising = 0.1 # 10% podwyżki\n",
    "spark.sql(f\"select firstname, lastname, salary, round(salary + salary * {rising},2) as after_rising from EMPLOYEE_DATA\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eba1d3-40b7-48bb-a4c6-1aacbe852b80",
   "metadata": {},
   "source": [
    "## 2. Apache Hive\n",
    "\n",
    "https://hive.apache.org/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e306da2c-c57f-4c05-bde9-7e6d30a203a9",
   "metadata": {},
   "source": [
    "Apache Hive, który pierwotnie został stworzony w 2007 przez Facebooka, a następnie w 2008 przekazany pod skrzydła Apache Foundation, jest nazywany hurtownią danych. Dane przechowywane są głównie w systemie **HDFS** (**Hadoop Distributed File System**), ale Hive integruje się również z innymi silnikami baz danych.\n",
    "\n",
    "Dostęp do danych jest realizowany przez **Hive QL**, który bardzo przypomina język SQL i taki sposób obsługi różnorodnych danych był jedną z głównych motywacji powstania Hive.\n",
    "\n",
    "Za pomocą zapytań Hive QL (HQL) można wykonać takie zapytania jak:\n",
    "* tworzenie i zmiana struktur tabel,\n",
    "* import i export danych,\n",
    "* agregacja danych, filtrowanie i złączenia danych.\n",
    "\n",
    "Apache Hive jest wykorzystywany w dużych ekosystemach i mimo wymienionych wyżej zalet posiada również kilka ograniczeń:\n",
    "* opóźnienie w czasie przetwarzania ze zwględu na wsadową naturę przetwarzania,\n",
    "* brak możliwości przetwarzania real-time,\n",
    "* język HQL nie daje możliwości wykonania takich operacji jak modyfikacja danych na poziomie wiersza,\n",
    "* brak możliwości przeprowadzenia zaawansowanych analiz jak współczesne nowoczesne bazy SQL.\n",
    "\n",
    "Alternatywne technologie:\n",
    "\n",
    "* Presto\n",
    "* Snowflake\n",
    "* Apache Impala\n",
    "* IBM Db2\n",
    "* Google BigQuery\n",
    "* Amazon Redshift\n",
    "* ClickHouse\n",
    "* Apache Hadoop\n",
    "* Apache HBase\n",
    "* Oracle Exadata\n",
    "* Teradata Vantage\n",
    "* Cloudera Impala"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84cdad0-562d-461a-acf0-e7aac125bc6d",
   "metadata": {},
   "source": [
    "### 2.1 Hive QL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187a784b-be51-4458-a45d-86f17e84c318",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T20:06:44.191527Z",
     "iopub.status.busy": "2024-11-19T20:06:44.190880Z",
     "iopub.status.idle": "2024-11-19T20:06:44.204303Z",
     "shell.execute_reply": "2024-11-19T20:06:44.201620Z",
     "shell.execute_reply.started": "2024-11-19T20:06:44.191477Z"
    }
   },
   "source": [
    "> Dokumentacja Apache Hive QL (dość archaiczna) jest dostępna pod adresem: https://cwiki.apache.org/confluence/display/Hive/LanguageManual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "56dfdd76-9691-44ca-8d7c-a44bccd443fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T11:39:39.589978Z",
     "iopub.status.busy": "2024-11-20T11:39:39.589355Z",
     "iopub.status.idle": "2024-11-20T11:39:39.602121Z",
     "shell.execute_reply": "2024-11-20T11:39:39.599861Z",
     "shell.execute_reply.started": "2024-11-20T11:39:39.589931Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spark_catalog'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentCatalog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "0782ea53-1963-46be-b135-524096d06151",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T11:39:42.782141Z",
     "iopub.status.busy": "2024-11-20T11:39:42.781523Z",
     "iopub.status.idle": "2024-11-20T11:39:42.796603Z",
     "shell.execute_reply": "2024-11-20T11:39:42.793708Z",
     "shell.execute_reply.started": "2024-11-20T11:39:42.782095Z"
    }
   },
   "outputs": [],
   "source": [
    "# dla zrealizowania kolejnych przykładów dokonamy kilku modyfikacji pliku employee\n",
    "# 1. dodanie kolumny ID - indeksu\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "df = df.withColumn(\"ID\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "7e38ec69-5916-4619-9210-fd543a9a1fbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T11:39:45.486943Z",
     "iopub.status.busy": "2024-11-20T11:39:45.486391Z",
     "iopub.status.idle": "2024-11-20T11:39:45.609390Z",
     "shell.execute_reply": "2024-11-20T11:39:45.607290Z",
     "shell.execute_reply.started": "2024-11-20T11:39:45.486901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+---+--------+\n",
      "| ID| firstname|           lastname|age|  salary|\n",
      "+---+----------+-------------------+---+--------+\n",
      "|  0|   Wisława|       Mieczykowski| 24| 6409.28|\n",
      "|  1|     Agata|             Szczaw| 46| 8980.03|\n",
      "|  2|      Adam|       Mieczykowski| 19| 8817.88|\n",
      "|  3| Krzysztof|       Mieczykowski| 18| 7531.72|\n",
      "|  4| Krzysztof|         Wróblewski| 31| 8371.61|\n",
      "|  5|  Zbigniew|Brzęczyszczykiewicz| 30| 7884.59|\n",
      "|  6| Krzysztof|       Mieczykowski| 21| 7110.25|\n",
      "|  7|      Adam|       Mieczykowski| 50| 6661.27|\n",
      "|  8|      Adam|             Szczaw| 41| 8383.29|\n",
      "|  9|Mieczysław|         Malinowski| 37|10625.15|\n",
      "+---+----------+-------------------+---+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "25b71a3d-6bc7-43a6-a5c6-802cdd4f67fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T11:39:52.137316Z",
     "iopub.status.busy": "2024-11-20T11:39:52.136695Z",
     "iopub.status.idle": "2024-11-20T11:39:52.153811Z",
     "shell.execute_reply": "2024-11-20T11:39:52.151529Z",
     "shell.execute_reply.started": "2024-11-20T11:39:52.137250Z"
    }
   },
   "outputs": [],
   "source": [
    "# dokonamy podziału danych i zapisania w różnych formatach\n",
    "splits = df.randomSplit(weights=[0.3, 0.7], seed=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "341b5b7d-78bb-425a-bc27-92678c7d455b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T11:39:54.129016Z",
     "iopub.status.busy": "2024-11-20T11:39:54.128470Z",
     "iopub.status.idle": "2024-11-20T11:39:57.644454Z",
     "shell.execute_reply": "2024-11-20T11:39:57.642371Z",
     "shell.execute_reply.started": "2024-11-20T11:39:54.128967Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(298932, 701068)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[0].count(), splits[1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe26e1b-3b69-463e-97b9-4177b68d6eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to dość dziwne zjawisko niezbyt równego podziału danych jest opisane w artykułach:\n",
    "# https://medium.com/udemy-engineering/pyspark-under-the-hood-randomsplit-and-sample-inconsistencies-examined-7c6ec62644bc\n",
    "# oraz\n",
    "# https://www.geeksforgeeks.org/pyspark-randomsplit-and-sample-methods/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "24e74751-9e52-4404-ac50-059f9a361a9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T11:40:03.905675Z",
     "iopub.status.busy": "2024-11-20T11:40:03.904595Z",
     "iopub.status.idle": "2024-11-20T11:40:03.920418Z",
     "shell.execute_reply": "2024-11-20T11:40:03.918111Z",
     "shell.execute_reply.started": "2024-11-20T11:40:03.905637Z"
    }
   },
   "outputs": [],
   "source": [
    "# większa część trafi do nowej tymczasowej tabeli\n",
    "splits[1].createOrReplaceTempView(\"EMPLOYEE_DATA_SPLIT_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "8425355d-865a-4bda-8d48-d6ef2351a5b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T11:40:07.075254Z",
     "iopub.status.busy": "2024-11-20T11:40:07.073919Z",
     "iopub.status.idle": "2024-11-20T11:40:10.516395Z",
     "shell.execute_reply": "2024-11-20T11:40:10.514895Z",
     "shell.execute_reply.started": "2024-11-20T11:40:07.075189Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# a mniejsza do plików JSON\n",
    "splits[0].write.json('employee_data.json', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2b1af32c-a2ce-4d10-9ef9-7b8471a6d98a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T11:40:11.135632Z",
     "iopub.status.busy": "2024-11-20T11:40:11.133516Z",
     "iopub.status.idle": "2024-11-20T11:40:11.287363Z",
     "shell.execute_reply": "2024-11-20T11:40:11.285838Z",
     "shell.execute_reply.started": "2024-11-20T11:40:11.135595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./employee_data.json/part-00000-c876c302-7856-404f-a1b9-e2a0a6f918b6-c000.json\n",
      "./employee_data.json/part-00001-c876c302-7856-404f-a1b9-e2a0a6f918b6-c000.json\n"
     ]
    }
   ],
   "source": [
    "!ls ./employee_data.json/*.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "29b41920-f283-4e0b-9b0c-050bcde0fc1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T11:40:32.221632Z",
     "iopub.status.busy": "2024-11-20T11:40:32.221013Z",
     "iopub.status.idle": "2024-11-20T11:40:32.257231Z",
     "shell.execute_reply": "2024-11-20T11:40:32.255257Z",
     "shell.execute_reply.started": "2024-11-20T11:40:32.221580Z"
    }
   },
   "outputs": [],
   "source": [
    "# aby móc wykorzystać dane w przykładach ze złączaniem, zapiszemy jeszcze próbkę danych z głównej ramki\n",
    "# z identyfikatorami oraz dodatkową kolumną z podwyżką\n",
    "from pyspark.sql.functions import col, lit, round\n",
    "\n",
    "lucky_guys = spark.sql(\"select * from EMPLOYEE_DATA\").sample(0.01)\\\n",
    ".withColumn('rising', lit('10%')).withColumn('salary after rising', round(col('salary') * 1.1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "3139791d-3262-4eaa-839d-c018fca4d5d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T11:43:24.447575Z",
     "iopub.status.busy": "2024-11-20T11:43:24.446787Z",
     "iopub.status.idle": "2024-11-20T11:43:27.000012Z",
     "shell.execute_reply": "2024-11-20T11:43:26.998806Z",
     "shell.execute_reply.started": "2024-11-20T11:43:24.447542Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# zapisujemy szczęściarzy do oddzielnej tabeli w hurtowni\n",
    "lucky_guys.write.mode('overwrite').saveAsTable(\"lucky_employees\", format='parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "4bd56d3d-815d-434f-a4fc-882601e421bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T11:40:50.186139Z",
     "iopub.status.busy": "2024-11-20T11:40:50.185740Z",
     "iopub.status.idle": "2024-11-20T11:40:50.376036Z",
     "shell.execute_reply": "2024-11-20T11:40:50.374420Z",
     "shell.execute_reply.started": "2024-11-20T11:40:50.186095Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='employee_firstname_bucketed', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='employee_id_bucketed', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='employee_salary_bucketed', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='employees_partitioned_lastname', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='lucky_eployees', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='EMPLOYEE_DATA', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='EMPLOYEE_DATA_SPLIT_1', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62071a63-692e-4102-895a-8105349a2f4c",
   "metadata": {},
   "source": [
    "#### Złączenie danych z różnych źródeł danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "6facb924-c66d-47b9-8ab7-de13b33a404c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T11:43:37.871496Z",
     "iopub.status.busy": "2024-11-20T11:43:37.870184Z",
     "iopub.status.idle": "2024-11-20T11:43:38.016980Z",
     "shell.execute_reply": "2024-11-20T11:43:38.015747Z",
     "shell.execute_reply.started": "2024-11-20T11:43:37.871461Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./metastore_db/lucky_employees/part-00000-c81c8888-314f-4e44-abd8-8329050c21c6-c000.snappy.parquet\n",
      "./metastore_db/lucky_employees/part-00001-c81c8888-314f-4e44-abd8-8329050c21c6-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls ./metastore_db/lucky_employees/*.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "4082ccce-86f4-4bc9-b37f-71c37f09ec32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T11:47:14.803748Z",
     "iopub.status.busy": "2024-11-20T11:47:14.802373Z",
     "iopub.status.idle": "2024-11-20T11:47:16.981845Z",
     "shell.execute_reply": "2024-11-20T11:47:16.979830Z",
     "shell.execute_reply.started": "2024-11-20T11:47:14.803680Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/20 11:47:14 WARN ObjectStore: Failed to get database json, returning NoSuchObjectException\n",
      "24/11/20 11:47:14 WARN ObjectStore: Failed to get database parquet, returning NoSuchObjectException\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------------+--------+------+-------------------+\n",
      "|  ID| firstname|    lastname|  salary|rising|salary after rising|\n",
      "+----+----------+------------+--------+------+-------------------+\n",
      "|  85|  Wojciech|      Wlotka| 8035.24|   10%|            8838.76|\n",
      "| 989|Aleksandra|       Pysla| 8177.19|   10%|            8994.91|\n",
      "|1727|      Adam|    Kowalski|  7317.8|   10%|            8049.58|\n",
      "|1773|  Zbigniew|  Malinowski| 8484.68|   10%|            9333.15|\n",
      "|1814| Katarzyna|  Malinowski| 6898.55|   10%|            7588.41|\n",
      "|1894| Katarzyna|    Kowalski| 6235.94|   10%|            6859.53|\n",
      "|2505|     Marek|      Wlotka| 6551.83|   10%|            7207.01|\n",
      "|3723|     Marek|Mieczykowski| 8034.91|   10%|             8838.4|\n",
      "|4502|Mieczysław|        Glut|10172.99|   10%|           11190.29|\n",
      "|5146|     Marek|        Glut| 6995.49|   10%|            7695.04|\n",
      "+----+----------+------------+--------+------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# przykład złączania danych na różnych źródłach danych\n",
    "# zapytanie SQL bezpośrednio na plikach - tutaj zapisanych wcześniej JSON-ach oraz parquet\n",
    "query = \"\"\"\n",
    "SELECT ed.ID, ed.firstname, ed.lastname, ed.salary, lucky.rising, lucky.`salary after rising`\n",
    "FROM json.`./employee_data.json/` as jtable \n",
    "join EMPLOYEE_DATA ed on jtable.ID=ed.ID \n",
    "join parquet.`./metastore_db/lucky_employees/` as lucky on ed.ID=lucky.ID\n",
    "\"\"\"\n",
    "df_from_json = spark.sql(query).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19de17c-1ff5-4f4f-8cff-f961cfa1346f",
   "metadata": {},
   "source": [
    "#### Dzielenie danych na wiaderka (ang. buckets) i partycje"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba3ad9f-9c7b-460e-832c-6960e431f896",
   "metadata": {},
   "source": [
    "Dzielenie danych na wiaderka jest rozwiązaniem, które stosowane jest do podziału danych na mniejsze części w sposób, który może przyspieszyć obliczenia poprzez zredukowanie liczby operacji przetasowania danych (ang. shuffle, a w kontekście Sparka mówimy o operacji exchange), które są bardzo kosztowne, gdyż wykonywane są często między węzłami (workerami)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ab4b1370-d4dc-4a73-8a7a-61c7c0794f7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T09:18:26.448597Z",
     "iopub.status.busy": "2024-11-20T09:18:26.447685Z",
     "iopub.status.idle": "2024-11-20T09:18:36.061905Z",
     "shell.execute_reply": "2024-11-20T09:18:36.060844Z",
     "shell.execute_reply.started": "2024-11-20T09:18:26.448559Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ten przykład pokazuje podział na 16 wiaderek danych bazując na podziale po kolumnie ID (tu używane jest hashowanie)\n",
    "# dane posortowane są w każdym buckecie po kolumnie salary\n",
    "# dane zapisywane są do hurtowni Hive, a informacje o zapisanych tam tabelach przechowywane są w\n",
    "# Hive metastore (domyślnie jest do baza danych Derby)\n",
    "df.write.bucketBy(16, 'ID').mode('overwrite').sortBy('salary').saveAsTable('employee_id_bucketed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6aa48c-3abf-415d-b87e-fb0ff6258fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls metastore_db/employee_salary_bucketed/*.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d119e520-0a46-4769-9c4c-a0cae74d3001",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T09:18:36.065034Z",
     "iopub.status.busy": "2024-11-20T09:18:36.064152Z",
     "iopub.status.idle": "2024-11-20T09:18:37.018881Z",
     "shell.execute_reply": "2024-11-20T09:18:37.017076Z",
     "shell.execute_reply.started": "2024-11-20T09:18:36.065002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------------+---+-------+\n",
      "|    ID| firstname|    lastname|age| salary|\n",
      "+------+----------+------------+---+-------+\n",
      "|503945|   Wisława|    Barański| 54|3390.11|\n",
      "|383781|   Wisława|    Barański| 39|4106.95|\n",
      "|371700|     Marek|      Wlotka| 65|4155.71|\n",
      "|110060|Aleksandra|Mieczykowski| 64|4213.95|\n",
      "|260561|     Marek|        Glut| 47|4304.21|\n",
      "|171761|   Wisława|Mieczykowski| 28| 4307.0|\n",
      "| 97494|      Adam|        Glut| 30|4310.82|\n",
      "|301272|Aleksandra|    Barański| 54|4343.63|\n",
      "|248768|     Agata|        Glut| 42| 4352.4|\n",
      "|   875|  Zbigniew|        Glut| 49|4385.59|\n",
      "+------+----------+------------+---+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table('employee_id_bucketed').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "42459d91-1ac8-45fe-a703-36868e5bb78d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T21:16:49.269358Z",
     "iopub.status.busy": "2024-11-19T21:16:49.268510Z",
     "iopub.status.idle": "2024-11-19T21:16:49.482023Z",
     "shell.execute_reply": "2024-11-19T21:16:49.479994Z",
     "shell.execute_reply.started": "2024-11-19T21:16:49.269331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='employee_bucketed', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='employees_partitioned_lastname', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='EMPLOYEE_DATA', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wypisanie tabeli\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3b776873-29ba-4003-9e5a-40102584a028",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T21:39:32.642211Z",
     "iopub.status.busy": "2024-11-19T21:39:32.641807Z",
     "iopub.status.idle": "2024-11-19T21:39:33.000434Z",
     "shell.execute_reply": "2024-11-19T21:39:32.998535Z",
     "shell.execute_reply.started": "2024-11-19T21:39:32.642182Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# usunięcie tabeli\n",
    "spark.sql('DROP TABLE employee_bucketed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "72a3ec83-e800-4e74-9c9b-18bf06374cd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T06:59:50.080532Z",
     "iopub.status.busy": "2024-11-20T06:59:50.079628Z",
     "iopub.status.idle": "2024-11-20T07:00:07.211803Z",
     "shell.execute_reply": "2024-11-20T07:00:07.209913Z",
     "shell.execute_reply.started": "2024-11-20T06:59:50.080493Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# jeżeli dane, z którymi pracujemy zawierają stosunkowo niewiele różnorodnych wartości w danych kolumnach\n",
    "# lub filtrowanie i obliczenia często odbywają się na podgrupach danych to lepsze efekty uzyskamy\n",
    "# poprzez wykorzystanie możliwości partycjonowania tych danych, które to partycjonowanie\n",
    "# będzie również odzwierciedlone w fizycznej strukturze plików na dysku twardym w hurtowni danych\n",
    "\n",
    "# zobaczmy przykład poniżej\n",
    "\n",
    "df.write.partitionBy(\"lastname\").mode('overwrite').saveAsTable(\"employees_partitioned_lastname\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "01e6899a-aae3-42cd-9daf-f372150bc551",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T21:14:21.660365Z",
     "iopub.status.busy": "2024-11-19T21:14:21.659648Z",
     "iopub.status.idle": "2024-11-19T21:14:23.469626Z",
     "shell.execute_reply": "2024-11-19T21:14:23.467509Z",
     "shell.execute_reply.started": "2024-11-19T21:14:21.660319Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dobrym pomysłem jest też określenie ilości bucketów wynikających z danych w konkretnej kolumnie\n",
    "# i wykorzystanie do podziału\n",
    "# https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.bucketBy.html\n",
    "buckets = spark.sql(\"select distinct firstname from EMPLOYEE_DATA\").count()\n",
    "buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "35217863-794b-4ae9-9fda-5ec6341c519d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T07:00:42.791919Z",
     "iopub.status.busy": "2024-11-20T07:00:42.791440Z",
     "iopub.status.idle": "2024-11-20T07:00:42.923947Z",
     "shell.execute_reply": "2024-11-20T07:00:42.921206Z",
     "shell.execute_reply.started": "2024-11-20T07:00:42.791867Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'lastname=Barański'\t\t'lastname=Malinowski'\t 'lastname=Wlotka'\n",
      "'lastname=Brzęczyszczykiewicz'\t'lastname=Mieczykowski'  'lastname=Wróblewski'\n",
      "'lastname=Glut'\t\t\t'lastname=Pysla'\t  _SUCCESS\n",
      "'lastname=Kowalski'\t\t'lastname=Szczaw'\n"
     ]
    }
   ],
   "source": [
    "# widok danych podzielonych na partycję z punktu widzenia systemu plików\n",
    "!ls metastore_db/employees_partitioned_lastname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a9eb0165-7e9f-42a4-a161-11ddb6c06925",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T07:12:12.692265Z",
     "iopub.status.busy": "2024-11-20T07:12:12.691803Z",
     "iopub.status.idle": "2024-11-20T07:12:12.737642Z",
     "shell.execute_reply": "2024-11-20T07:12:12.736103Z",
     "shell.execute_reply.started": "2024-11-20T07:12:12.692226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[lastname#169], functions=[avg(salary#171)])\n",
      "   +- Exchange hashpartitioning(lastname#169, 200), ENSURE_REQUIREMENTS, [plan_id=3316]\n",
      "      +- HashAggregate(keys=[lastname#169], functions=[partial_avg(salary#171)])\n",
      "         +- Filter (isnotnull(lastname#169) AND (lastname#169 = Pysla))\n",
      "            +- FileScan csv [lastname#169,salary#171] Batched: false, DataFilters: [isnotnull(lastname#169), (lastname#169 = Pysla)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/lab_06/employee_1m.csv], PartitionFilters: [], PushedFilters: [IsNotNull(lastname), EqualTo(lastname,Pysla)], ReadSchema: struct<lastname:string,salary:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.lastname == 'Pysla').groupby('lastname').agg({'salary': 'avg'}).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "721dc597-bc44-4573-bd78-ef6cddf1f403",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T07:12:24.653593Z",
     "iopub.status.busy": "2024-11-20T07:12:24.653124Z",
     "iopub.status.idle": "2024-11-20T07:12:27.351765Z",
     "shell.execute_reply": "2024-11-20T07:12:27.344118Z",
     "shell.execute_reply.started": "2024-11-20T07:12:24.653556Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 178:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+\n",
      "|lastname|      avg(salary)|\n",
      "+--------+-----------------+\n",
      "|   Pysla|7849.311716746816|\n",
      "+--------+-----------------+\n",
      "\n",
      "CPU times: user 10.7 ms, sys: 7.67 ms, total: 18.3 ms\n",
      "Wall time: 2.68 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.filter(df.lastname == 'Pysla').groupby('lastname').agg({'salary': 'avg'}).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d4881a6b-8b57-4f42-b596-645ba485a5b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T07:12:35.494422Z",
     "iopub.status.busy": "2024-11-20T07:12:35.493968Z",
     "iopub.status.idle": "2024-11-20T07:12:35.581533Z",
     "shell.execute_reply": "2024-11-20T07:12:35.579649Z",
     "shell.execute_reply.started": "2024-11-20T07:12:35.494386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[lastname#2348], functions=[avg(salary#2347)])\n",
      "   +- Exchange hashpartitioning(lastname#2348, 200), ENSURE_REQUIREMENTS, [plan_id=3382]\n",
      "      +- HashAggregate(keys=[lastname#2348], functions=[partial_avg(salary#2347)])\n",
      "         +- FileScan parquet spark_catalog.default.employees_partitioned_lastname[salary#2347,lastname#2348] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/lab_07/metastore_db/employees_partitioned_las..., PartitionFilters: [isnotnull(lastname#2348), (lastname#2348 = Pysla)], PushedFilters: [], ReadSchema: struct<salary:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select lastname, avg(salary) from employees_partitioned_lastname where lastname='Pysla' group by lastname\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "faed71dd-9206-4cf1-b4d9-3a1771e7a2d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T07:12:41.691478Z",
     "iopub.status.busy": "2024-11-20T07:12:41.691064Z",
     "iopub.status.idle": "2024-11-20T07:12:42.221483Z",
     "shell.execute_reply": "2024-11-20T07:12:42.216416Z",
     "shell.execute_reply.started": "2024-11-20T07:12:41.691442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+\n",
      "|lastname|      avg(salary)|\n",
      "+--------+-----------------+\n",
      "|   Pysla|7849.311716746816|\n",
      "+--------+-----------------+\n",
      "\n",
      "CPU times: user 5.77 ms, sys: 0 ns, total: 5.77 ms\n",
      "Wall time: 516 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark.sql(\"select lastname, avg(salary) from employees_partitioned_lastname where lastname='Pysla' group by lastname\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4091f3b3-4ea6-4211-bfc5-9509b3a2fef5",
   "metadata": {},
   "source": [
    "Jak widać, operacja wykonała się szybciej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c27b5bda-2b0a-4709-82ba-05dafc721131",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T12:06:52.272940Z",
     "iopub.status.busy": "2024-11-19T12:06:52.271626Z",
     "iopub.status.idle": "2024-11-19T12:06:52.371156Z",
     "shell.execute_reply": "2024-11-19T12:06:52.370100Z",
     "shell.execute_reply.started": "2024-11-19T12:06:52.272886Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.sparkContext.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bb2215-9073-4480-8bf2-5618dd6a954a",
   "metadata": {},
   "source": [
    "### Zadania"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3a43c2-4594-4ffe-a0aa-0f3fc028fba8",
   "metadata": {},
   "source": [
    "**Zadanie 1**  \n",
    "Pamiętacie plik zamówienia.txt ?\n",
    "Plik został umieszczony w folderze z labem w repozytorium.\n",
    "\n",
    "Wczytaj ten plik za pomocą Sparka do dowolnego typu danych (RDD, Spark DataFrame) i dokonaj transformacji tak aby:\n",
    "* naprawić problemy z kodowaniem znaków (replace?) w kolumnie Sprzedawca\n",
    "* poprawić format danych w kolumnie Utarg\n",
    "* dodać odpowiednie typy danych\n",
    "* kolumna idZamowienia powinna być traktowana jako klucz (indeks)\n",
    "\n",
    "**Zadanie 2**  \n",
    "Po wykonaniu zadania 1, wykorzystaj przykłady z laboratorium i:\n",
    "* 2.1 wykonaj wiaderkowanie danych i wykonaj dowolne zapytanie agregujące na tych danych vs. dane nie podzielone na wiaderka - porównaj czas\n",
    "* 2.2 wykonaj partycjonowanie danych i zapisz je w formcie csv (wypróbuj partycjonowanie wg. kraju, nazwiska\n",
    "* 2.3 wykonaj zapytanie agregujące z filtrowanie po kolumnie, której użyłeś/-aś do partycjonowania na danych oryginalnych oraz partycjonowanych i porównaj czas wykonania\n",
    "\n",
    "**Zadanie 3**  \n",
    "Z danych wygeneruj 4 różne podzbiory próbek (wiersze wybrane losowo) i dodaj nową kolumnę w każdym z nich, np. w jednym stwórz kolumnę month wyciągając tylko miesiąc z daty, w drugim wartość netto zamówienia (przyjmując, że vat to 23%), w kolejnym zamień nazwisko na wielkie litery, w kolejnym dodaj kolumnę waluta z wartością PLN.\n",
    "\n",
    "Następnie zapisz każdy z tych zbiorów tak, że:\n",
    "* zbiór pierwszy to będzie tymczasowa tabela in-memory Sparka\n",
    "* zbiór drugi to plik(i) parquet\n",
    "* zbiór trzeci to plik(i) csv\n",
    "* zbiór czwarty to plik(i) json\n",
    "\n",
    "Wykonaj zapytanie złączające jak w przykładzie pobierając dane bezpośrednio z plików i wyświetl idZamowienia, Kraj, Sprzedawcę, Datę, Utarg oraz 4 nowo utworzone kolumny.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
