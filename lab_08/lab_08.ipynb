{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acc4838d-2373-4213-8540-8d489163fd05",
   "metadata": {},
   "source": [
    "# Lab 8. Apache Airflow - część 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ec895e-aaef-4a03-a7f2-b577d8fbf1b6",
   "metadata": {},
   "source": [
    "Apache Airflow® to platforma open-source do opracowywania, harmonogramowania i monitorowania zadań wsadowych. Dzięki językowy Python, w którym definiowane są grafy zadań możliwe jest połączenie tych definicji praktycznie z każdą technologią. Apache Airflow pozwala na tworzenie zadań o różnym stopniu skomplikowania a same zadania pisane są w postaci kodu w języku Python. Apache Airflow dostarcza również rozbudowany interfejs graficzny, dzięki któremu możemy śledzić wykonanie zadań oraz przepływy zdefiniowane w każdym zadaniu.\n",
    "\n",
    "Apache Airflow nie został stworzony do uruchamiania zadań, których wykonanie trwa w nieskończoność."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80eaffe-1dff-443a-b4c8-a70759bc1fd8",
   "metadata": {},
   "source": [
    "## 1. Instalacja i uruchomienie Apache Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a3b78c-c0d6-4a70-8e85-fa6388e362b0",
   "metadata": {},
   "source": [
    "W opisywanym przypadku instalacja Apache Airflow zostanie wykonana za pośrednictwem menedżera pip, który aktualnie jest jedynym oficjalnie wspieranym rozwiązaniem do instalacji tego modułu języka Python.\n",
    "\n",
    "\n",
    "Polcecenie:\n",
    "```bash\n",
    "pip install apache-airflow\n",
    "```\n",
    "\n",
    "Instalacja i uruchomienie przedstawione w tym labie nie są odpowiednie dla środowiska produkcyjnego, co jest jasno zakomunikowane w dokumentacji.\n",
    "\n",
    "> UWAGA! Wykonanie poniższych komend zalecane jest w nowym oknie terminala, gdzie można śledzić kolejne komunikaty wyświetlane w tym oknie, ale również ze względu na to, że spowoduje to uruchomienie usługi, która będzie działała póki jej nie przerwiemy i nie będziemy mogli w tym oknie podawać kolejnych komend, bez przełączania między zadaniami w tle.\n",
    "\n",
    "Uruchomienie zainstalowanego modułu Apache Airflow można wykonać w trybie `standalone`.\n",
    "\n",
    "Polecenie\n",
    "```console\n",
    "airflow standalone \n",
    "```\n",
    "\n",
    "uruchomi kilka modułów z domyślnymi parametrami.\n",
    "Gdybyśmy chcieli uruchomić je inaczej możemy wywołać kolejne moduły jak podano w dokumentacji:\n",
    "\n",
    "```bash\n",
    "airflow db migrate\n",
    "\n",
    "airflow users create \\\n",
    "    --username admin \\\n",
    "    --firstname Peter \\\n",
    "    --lastname Parker \\\n",
    "    --role Admin \\\n",
    "    --email spiderman@superhero.org\n",
    "\n",
    "airflow webserver --port 8080 # może się przydać, jeżeli w naszym kontenerze działa już jakaś usługa na tym porcie\n",
    "\n",
    "airflow scheduler\n",
    "```\n",
    "\n",
    "\n",
    "Więcej informacji o procesie uruchomienia Apache Airflow można znaleźć tutaj:\n",
    "https://airflow.apache.org/docs/apache-airflow/stable/start.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ec8e53-802e-44b5-a6e7-55bfdf863dfb",
   "metadata": {},
   "source": [
    "Po uruchomieniu serwera obserwuj komunikaty w konsoli, gdyż w pewnym momencie pojawi się informacja o utworzonym koncie razem z hasłem do zalogowania do serwisu www dostępnego pod adresem http://localhost:9090 (takie domyślne mapowanie zostało stworzone w pliku konfiguracyjnym 9090:8080)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9461084-aa1b-4206-b034-0fb0786295be",
   "metadata": {},
   "source": [
    "Po zalogowaniu powinno pojawić się okno ze wszystkimi zdefiniowanymi grafami z załadowanej lokalizacji (która jest określona w pliku konfiguracyjnym, opisanym w dalszej części tych materiałów).\n",
    "\n",
    "![Apache Aiflow](airflow_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf21a5c4-10af-4f0d-a0e2-fa0bacdc8d67",
   "metadata": {},
   "source": [
    "## 2. Definiowanie zadań."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329c8e92-49c6-439d-b75a-6a9290b90bc0",
   "metadata": {},
   "source": [
    "Omówienie przykładowej definicji zadania opiera się o zadanie z dokumentacji Apache Airflow (`airflow/example_dags/tutorial.py`).\n",
    "\n",
    "Włączenie widoczności numerów linii skrótem `SHIFT + L`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ad7abd-6720-4945-a0d5-a8ea1f396c01",
   "metadata": {},
   "source": [
    "_**Listing 1**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67d1bd6-0e63-4185-be54-653b9028e408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# The DAG object; we'll need this to instantiate a DAG\n",
    "from airflow.models.dag import DAG\n",
    "\n",
    "# Operators; we need this to operate!\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "with DAG(\n",
    "    \"tutorial\",\n",
    "    # These args will get passed on to each operator\n",
    "    # You can override them on a per-task basis during operator initialization\n",
    "    default_args={\n",
    "        \"depends_on_past\": False,\n",
    "        \"email\": [\"airflow@example.com\"],\n",
    "        \"email_on_failure\": False,\n",
    "        \"email_on_retry\": False,\n",
    "        \"retries\": 1,\n",
    "        \"retry_delay\": timedelta(minutes=5),\n",
    "        # 'queue': 'bash_queue',\n",
    "        # 'pool': 'backfill',\n",
    "        # 'priority_weight': 10,\n",
    "        # 'end_date': datetime(2016, 1, 1),\n",
    "        # 'wait_for_downstream': False,\n",
    "        # 'sla': timedelta(hours=2),\n",
    "        # 'execution_timeout': timedelta(seconds=300),\n",
    "        # 'on_failure_callback': some_function, # or list of functions\n",
    "        # 'on_success_callback': some_other_function, # or list of functions\n",
    "        # 'on_retry_callback': another_function, # or list of functions\n",
    "        # 'sla_miss_callback': yet_another_function, # or list of functions\n",
    "        # 'on_skipped_callback': another_function, #or list of functions\n",
    "        # 'trigger_rule': 'all_success'\n",
    "    },\n",
    "    description=\"A simple tutorial DAG\",\n",
    "    schedule=timedelta(days=1),\n",
    "    start_date=datetime(2021, 1, 1),\n",
    "    catchup=False,\n",
    "    tags=[\"example\"],\n",
    ") as dag:\n",
    "\n",
    "    # t1, t2 and t3 are examples of tasks created by instantiating operators\n",
    "    t1 = BashOperator(\n",
    "        task_id=\"print_date\",\n",
    "        bash_command=\"date\",\n",
    "    )\n",
    "\n",
    "    t2 = BashOperator(\n",
    "        task_id=\"sleep\",\n",
    "        depends_on_past=False,\n",
    "        bash_command=\"sleep 5\",\n",
    "        retries=3,\n",
    "    )\n",
    "    t1.doc_md = textwrap.dedent(\n",
    "        \"\"\"\\\n",
    "    #### Task Documentation\n",
    "    You can document your task using the attributes `doc_md` (markdown),\n",
    "    `doc` (plain text), `doc_rst`, `doc_json`, `doc_yaml` which gets\n",
    "    rendered in the UI's Task Instance Details page.\n",
    "    ![img](https://imgs.xkcd.com/comics/fixing_problems.png)\n",
    "    **Image Credit:** Randall Munroe, [XKCD](https://xkcd.com/license.html)\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    dag.doc_md = __doc__  # providing that you have a docstring at the beginning of the DAG; OR\n",
    "    dag.doc_md = \"\"\"\n",
    "    This is a documentation placed anywhere\n",
    "    \"\"\"  # otherwise, type it like this\n",
    "    templated_command = textwrap.dedent(\n",
    "        \"\"\"\n",
    "    {% for i in range(5) %}\n",
    "        echo \"{{ ds }}\"\n",
    "        echo \"{{ macros.ds_add(ds, 7)}}\"\n",
    "    {% endfor %}\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    t3 = BashOperator(\n",
    "        task_id=\"templated\",\n",
    "        depends_on_past=False,\n",
    "        bash_command=templated_command,\n",
    "    )\n",
    "\n",
    "    t1 >> [t2, t3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e296d4-ebe6-4a09-8f60-8e298921a59e",
   "metadata": {},
   "source": [
    "We fragmencie:\n",
    "\n",
    "```python\n",
    "with DAG(\n",
    "    \"tutorial\",\n",
    "    # These args will get passed on to each operator\n",
    "    # You can override them on a per-task basis during operator initialization\n",
    "    default_args={\n",
    "        \"depends_on_past\": False,\n",
    "        \"email\": [\"airflow@example.com\"],\n",
    "        \"email_on_failure\": False,\n",
    "        \"email_on_retry\": False,\n",
    "        \"retries\": 1,\n",
    "        \"retry_delay\": timedelta(minutes=5),\n",
    "        # 'queue': 'bash_queue',\n",
    "        # 'pool': 'backfill',\n",
    "        # 'priority_weight': 10,\n",
    "        # 'end_date': datetime(2016, 1, 1),\n",
    "        # 'wait_for_downstream': False,\n",
    "        # 'sla': timedelta(hours=2),\n",
    "        # 'execution_timeout': timedelta(seconds=300),\n",
    "        # 'on_failure_callback': some_function, # or list of functions\n",
    "        # 'on_success_callback': some_other_function, # or list of functions\n",
    "        # 'on_retry_callback': another_function, # or list of functions\n",
    "        # 'sla_miss_callback': yet_another_function, # or list of functions\n",
    "        # 'on_skipped_callback': another_function, #or list of functions\n",
    "        # 'trigger_rule': 'all_success'\n",
    "    },\n",
    "    description=\"A simple tutorial DAG\",\n",
    "    schedule=timedelta(days=1),\n",
    "    start_date=datetime(2021, 1, 1),\n",
    "    catchup=False,\n",
    "    tags=[\"example\"],\n",
    ") as dag:\n",
    "```\n",
    "zdefiniowano identyfikator tego DAGa (`dag_id`) o nazwie `tutorial` (który można zdefiniować również przez kwargs `dag_id=\"tutorial\"`) oraz\n",
    "zdefiniowane zostały parametry, które zostaną przekazane do każdego operatora, które w ramach tego grafu zadań zostaną zdefiniowane. Można je zapisać jako domyślne do późniejszego wykorzystania i ewentualnego nadpisania tylko niektórych z nich.\n",
    "\n",
    "Kolejno zdefiniowany jest opis oraz interwał wykonania zadania (`timedelta`), tutaj 1 dzień, datę pierwszego uruchomienia, `catchup` (zobacz: https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dag-run.html#catchup) oraz tagi.\n",
    "\n",
    "\n",
    "> Dokumentacja obiektów DAG: https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html\n",
    "> \n",
    "> Lista i opis wszystkich parametrów dla obiektu `BaseOperator`, z którego dziedziczą\n",
    "> wszystkie inne operatory znajduje się pod adresem:\n",
    "> https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/models/baseoperator/index.html#airflow.models.baseoperator.BaseOperator\n",
    "\n",
    "**Operatory** do zdefiniowane szablony zadań, które można wykorzystać w grafie zadań lub można rozszerzyć je o własne implementacje. Listę zdefiniowanych operatorów oraz ich dokumentację znajdziemy tu: https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/operators.html#operators\n",
    "\n",
    "\n",
    "Wykorzystanie operatorów polega na zdefiniowaniu zadań (tasków), które te operatory wykorzystują, jak poniższy fragment:\n",
    "\n",
    "```python\n",
    "# t1, t2 and t3 are examples of tasks created by instantiating operators\n",
    "t1 = BashOperator(\n",
    "    task_id=\"print_date\",\n",
    "    bash_command=\"date\",\n",
    ")\n",
    "\n",
    "t2 = BashOperator(\n",
    "    task_id=\"sleep\",\n",
    "    depends_on_past=False,\n",
    "    bash_command=\"sleep 5\",\n",
    "    retries=3,\n",
    ")\n",
    "t3 = BashOperator(\n",
    "    task_id=\"templated\",\n",
    "    depends_on_past=False,\n",
    "    bash_command=templated_command,\n",
    ")\n",
    "```\n",
    "\n",
    "Argument `bash_command` operatora `BashOperator` może również wskazywać na skrypt powłoki, a ścieżka do niego przekazywana jest relatywnie do położenia skryptu samej definicji grafu (czyli tutaj tutorial.py).\n",
    "\n",
    "Widać też w przykładzie zdefiniowaną dokumentację (`dag.doc_md` oraz `t1.doc_md`), która będzie również widoczna w graficznym interfejsie obsługi Apache Airflow, ale już w samym przykładzie widać, że możemy to również zrobić określając tę dokumentację jako `doc` (plain text), `doc_rst`, `doc_json` czy `doc_yaml`.\n",
    "\n",
    "I na sam koniec określone są zależności między zadaniami zdefiniowanymi w ramach tego grafu.\n",
    "\n",
    "```python\n",
    "t1 >> [t2, t3]\n",
    "```\n",
    "\n",
    "Powyższy zapis oznacza, że zadania 2 oraz 3 zależą od zadania 1.\n",
    "\n",
    "Rozwinięcie nazwy DAG oznacza Directed Acyclic Graphs czyli skierowane acykliczne grafy i jeżeli zdefiniujemy w naszym grafach cykle Apache Airflow zgłosi wyjątek przy próbie uruchomienia takiego grafu. Wyjątek pojawi się również w przypadku gdy pojawi się odwołanie do zależności więcej niż jeden raz.\n",
    "\n",
    "Z dokumentacji możemy również zobaczyć inne przykłady definiowania zależności, np. takie jak poniżej.\n",
    "\n",
    "```python\n",
    "\n",
    "# 1. Zadanie 2 zależy od pomyslnego wykonania zadania 1\n",
    "t1.set_downstream(t2)\n",
    "# lub\n",
    "t1 >> t2\n",
    "# lub\n",
    "t2.set_upstream(t1)\n",
    "# lub\n",
    "t2 << t1\n",
    "\n",
    "\n",
    "# 2. łańcuch zależności\n",
    "t1 >> t2 >> t3\n",
    "\n",
    "# 3. można również określić listę zależnych zadań jak poniżej\n",
    "t1.set_downstream([t2, t3])\n",
    "t1 >> [t2, t3]\n",
    "[t2, t3] << t1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3755a9f6-6769-43d9-95a9-10c96f9ae806",
   "metadata": {},
   "source": [
    "## 3. Konfiguracja Apache Airflow i jej zachowanie w naszym środowisku"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6229bb7a-3b60-4e0b-839b-89399d7b6d26",
   "metadata": {},
   "source": [
    "Konfiguracja narzędzia Apache Airflow zostanie domyślnie stworzona w folderze `~/airflow` czyli w naszym przypadku jeżeli korzystamy ze środowiska docker skonfigurowanego w trakcie zajęć będzie to `/home/spark/airflow/`, które póki co nie będzie utrwalane na dysku maszyny hosta gdyż mapowanie w `volumnes` zostało ustawione tylko na folder `/opt/spark/work-dir`. Abyśmy mogli przechowywać konfigurację Aiflow między uruchomieniami kontenera oraz aby meta baza Airflow również była utrwalana należy dodać odpowiednią konfigurację do pliku `docker-compose.yml`. Możemy to zrobić jak poniżej:\n",
    "\n",
    "```yml\n",
    "...\n",
    "    volumes:\n",
    "       - ./apps:/opt/spark/work-dir\n",
    "       - ./airflow:/home/spark/airflow\n",
    "```\n",
    "\n",
    "Pamiętajmy jednak, że te ustawienia zostaną wykorzystane dopiero po zatrzymaniu kontenera i ponownym jego uruchomieniu, więc na pewno możemy stracić to co już w ustawieniach Airflow jest w tym momencie. Możemy to jednak skopiować z poziomu kontenera przed jego zatrzymaniem, np tak:\n",
    "\n",
    "```bash\n",
    "# będąc w folderze /home/spark\n",
    "cp -R ~/airflow /opt/spark/work-dir\n",
    "```\n",
    "\n",
    "Pamiętajmy, że najlepiej wykonać to po zatrzymaniu Apache Aiflow, a przed zatrzymaniem kontenera. Wysłanie komendy zakończenia wykonania bieżącego zadania uruchomionego w terminalu można wykonać za pomocą kombinacji klawiszy `CTRL + c` - trzeba tutaj poczekać dłuższą chwilę, aż usługa się zatrzyma.\n",
    "\n",
    "Po zatrzymaniu Apache Airflow, wykonaniu kopii i zatrzymaniu kontenera, powinniśmy przenieść skopiowany folder aiflow z folderu hosta (powinien znajdować się w folderze `apps`) na ten sam poziom co folder `apps`. Teraz możemy wywołać polecenie `docker compose up` w terminalu ze ścieżki, w której znajduje się zmodyfikowana wersja pliku `docker-compose.yml` i folder powinien się mapować teraz przy kolejnych uruchomieniach kontenera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26835d45-1ad1-43da-b452-f6e504da6fb3",
   "metadata": {},
   "source": [
    "## Zadania"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73de1e67-cf6c-4dc1-905a-8d1ddad73472",
   "metadata": {},
   "source": [
    "**Zadanie 1**  \n",
    "Zainstaluj i uruchom Apache Airflow. Zaloguj się do panelu aplikacji www.\n",
    "\n",
    "**Zadanie 2**  \n",
    "Uruchom zadanie `airflow/example_dags/tutorial.py` i prześledź w aplikacji jego wykonanie, log, wykres Gannt i inne statystyki, które są udostępniane przez interfejs. Na zakładce `Graph` po zaznaczeniu konkretnego zadania z grafu zmienia się ścieżka powrotu na górze okna i przechodząc teraz po zakładkach widzimy opis i inne właściwości zdefiniowane dla zadania, a nie całego grafu. Przeglądnij te informacje dla trzech zdefiniowanych zadań w tym grafie.\n",
    "\n",
    "**Zadanie 3**  \n",
    "Stwórz nowy graf poprzez modyfikację grafu z listingu 1 i zarejestruj go pod nazwą `tutorial_hourly` w pliku o nazwie `tutorial_hourly.py` i zapisz w tym samym folderze co oryginał. W definicji tego grafu zmodyfikuj:\n",
    "* interwał uruchamiania z dziennego na godzinny,\n",
    "* opis,\n",
    "* dokumentację obiektu dag (widoczną później również poprzez interfejs graficzny),\n",
    "* datę pierwszego uruchomienia na bieżącą,\n",
    "* tag (np. lab8).\n",
    "\n",
    "**Zadanie 4**  \n",
    "\n",
    "Posługując się opisem w labie wykonaj operacje, które pozwolą na przechowanie konfiguracji Apache Airflow pomiędzy uruchomieniami kontenera."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
