{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bee397b-0c28-42ae-8511-97abec931f9e",
   "metadata": {},
   "source": [
    "# lab_01 - Czy na pewno już potrzebujesz narzędzi BIG DATA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24447e9-0522-49c1-98b9-f9697276b73d",
   "metadata": {},
   "source": [
    "# 1. Przetwarzanie rozproszone - wady i zalety.\n",
    "\n",
    "Po narzędzia do przetwarzania dużych zbiorów danych sięgamy zazwyczaj wtedy, kiedy ich przetwarzanie lokalnie staje się niemożliwe lub problematyczne. Zazwyczaj wtedy, gdy ilość przetwarzanych danych nie jest możliwa do \"upchnięcia\" w dostępnej pamięci RAM (lub pamięci kart GPU) i szukamy sposobu na skalowanie architektury poziomo.\n",
    "Jednak czy zrobiliśmy wystarczająco dużo, aby zoptymalizować wykorzystanie tej pamięci? \n",
    "Przetwarzanie rozproszone, które jest sercem przetwarzania danych o wolumenie big data, ma swoje zalety, ale również i wady.\n",
    "\n",
    "**Zalety przetwarzania rozproszonego:**\n",
    "* możliwość rozłożenia pracy na większą ilość węzłów, których sumaryczna wydajność (procesor) oraz ilość pamięci może być wielokrotnie większa niż sprzętu dostępnego lokalnie,\n",
    "* możliwość dość łatwego skalowania klastra w razie potrzeby,\n",
    "* awaria pojedynczego węzła nie musi zakończyć się niepowodzeniem całego procesu przetwarzania danych,\n",
    "\n",
    "**Wady przetwarzania rozproszonego:**\n",
    "* wymaga dostępu do klastra (konfiguracja, dostępy),\n",
    "* przy nieodpowiednio dobranej konfiguracji do wielkości zbioru danych przetwarzanie może zająć więcej czasu niż lokalnie:\n",
    "  * w zależności od tego gdzie znajdują się dane - czas ich przesłania do klastra i propagacji na poszczególne węzły może być długi,\n",
    "  * nieodpowiednie dobranie wielkości partycji (w sensie fragmentów zbioru do przetworzenia) do parametrów klastra może spowodować, że węzły przez większośc czasu będą oczekiwały na zakończenie zadań zależnych zamiast faktycznie coś liczyć. Poprawne dobranie parametrów wymaga doświadczenia, ale czasem wielu prób przed wdrożeniem produkcyjnym.\n",
    "* koszt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e6c4c9-fd44-46e8-9fbe-3fc1f1900b2e",
   "metadata": {},
   "source": [
    "# 2. Optymalizacja. Co możemy zrobić?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeb0431-73de-4788-8cd2-ea36a23f65ec",
   "metadata": {},
   "source": [
    "## 2.1 Optymalizacja wykorzystania pamięci.\n",
    "\n",
    "Każdy punkt danych, który jest wczytany do pamięci RAM zajmuje jej część w zależności od typu, jaki został mu przydzielony. W zależności od języka programowania oraz wybranej struktury danych te wielkości mogą się bardzo różnić. Przykłady optymalizacji zaprezentowane zostaną w języku Python i z użyciem najpopularniejszych bibliotekach do przetwarzania danych czyli numpy oraz pandas.\n",
    "\n",
    "Proces doboru bardziej optymalnego typu danych można rozszerzyć o zmianę zakresu (dziedziny) tych danych, który nazywa się kwantyzacją (ang. quantization) i jest obecnie powszechnie stosowany m.in. do zmniejszania rozmiarów modeli LLM.\n",
    "\n",
    "Optymalizacja pamięci możliwa jest również na poziomie pamięci dyskowej, gdzie powstały bardziej zoptymalizowane formaty przechowywania danych niż te najbardziej popularne wśród osób pracujących na co dzień w obszarze data science. Są to między innymi formaty:\n",
    "* Parquet,\n",
    "* ORC,\n",
    "* AVRO.\n",
    "\n",
    "Temat formatów danych zostanie omówiony w późniejszym czasie.\n",
    "\n",
    "## 2.2 Multiprocessing lokalnie.\n",
    "\n",
    "Nie które języki programowania natywnie wykorzystują wszystkie dostępne rdzenie i nie musimy się zbyt często martwić, aby jako programista wysokopoziomowy optymalizować samodzielnie ten kod na kod wielowątkowy lub wieloprocesowy. CPython, który jest najbardziej popularną implementacją interpretera języka Python, posiada dość poważne ograniczenie w postaci blokady GIL (czytaj więcej: [tu](https://realpython.com/python-gil/), [tu](https://wiki.python.org/moin/GlobalInterpreterLock) oraz [tu](https://bulldogjob.pl/readme/python-dazy-do-usuniecia-gil-i-zwiekszenia-wspolbieznosci)), która dotyczy wielu popularnych bibliotek.\n",
    "\n",
    "## 2.3 Python - biblioteka Numba.\n",
    "\n",
    "Jest to biblioteka dedykowana do optymalizacji ciężkich obliczeń numerycznych poprzez możliwość kompilacji odpowiednio napisanego kodu w języku Python do kodu maszynowego. Numba współpracuje z biblioteką Dask oraz systememSpark, które zostaną zaprezentowane w późniejszym czasie. Możliwe jest również wykorzystanie bilioteki CUDA w celu wykonywania obliczeń na kartach graficznych firmy NVIDIA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3520a9a-64ce-428f-973f-641597efe036",
   "metadata": {},
   "source": [
    "# 3. Optymalizowanie danych w bibliotece pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3017d5e-6b97-4256-a8a7-f0e3cf5d8367",
   "metadata": {},
   "source": [
    "## 3.1 Rozgrzewka."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88635bef-0768-482f-8291-97f48aa539c2",
   "metadata": {},
   "source": [
    "Pakiety niezbędne do wykonania kodu z bieżącego labu:\n",
    "* pandas\n",
    "* numpy\n",
    "* jupyter-lab\n",
    "* fastparquet\n",
    "* filesplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352f277c-dda6-4826-9f73-b6b235dbe9ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optymalizację możemy zacząć już na etapie procesu wczytywania danych do pandas DataFrame.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# wczytywanie danych z pliku \"na raz\" - przy małych plikach optymalne rozwiązanie\n",
    "df = pd.read_csv('zamowienia.csv', header=0, sep=';')\n",
    "display(df.head())\n",
    "# poniższa funkcja zwróci nam między innymi typy danych dla każdej kolumny i SZACUNKOWĄ wielkość pamięci, którą zajmuje\n",
    "# nie jest to jednak wielkość dokładna\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7437389-f97c-4ea9-94b8-edf30d3aa392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aby sprawdzić ilość pamięci zajmowaną przez ramkę (lub serię) danych, skorzystamy z funkcji memory_usage\n",
    "df.memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78820552-14a6-4801-a6b7-8c8b5873cb9b",
   "metadata": {},
   "source": [
    "Informacja została podana dla każdej kolumny (również indeksu) wyrażona w bajtach. Zwrócony typ danych? Pandas series. Możemy więc to zsumować."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8742e3c0-1427-4601-adb8-fc9115fd8d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df.memory_usage())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9872af3-052c-4e8c-a692-6016ee6137f0",
   "metadata": {},
   "source": [
    "Ta liczba bajtów odpowiada informacji podanej po wywołaniu `df.info()`, ale domyślna wartość parametru `deep=False` powoduje, że nie są to ponownie informacje dokładne. Sprawdźmy więc ile to jest dokładnie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97e74ae-7403-4eb5-8f9a-a8cb067c9478",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df.memory_usage(deep=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1731f695-deb6-41bc-9420-57f3db47b091",
   "metadata": {},
   "source": [
    "Widać teraz, że faktycznie jest to wielkość kilkukrotnie większa.\n",
    "\n",
    "Na potrzeby naszych eksperymentów wykorzystamy funkcję, która będzie nam tłumaczyła bajty na coś bardziej przyjaznego (za: https://stackoverflow.com/questions/1094841/get-a-human-readable-version-of-a-file-size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead7dabf-0f48-4262-85d5-adb340fb3543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sizeof_fmt(num, suffix=\"B\"):\n",
    "    for unit in (\"\", \"Ki\", \"Mi\", \"Gi\", \"Ti\", \"Pi\", \"Ei\", \"Zi\"):\n",
    "        if abs(num) < 1024.0:\n",
    "            return f\"{num:3.1f}{unit}{suffix}\"\n",
    "        num /= 1024.0\n",
    "    return f\"{num:.1f}Yi{suffix}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa13363-45b0-4032-a3b9-19e9a138e9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizeof_fmt(sum(df.memory_usage(deep=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69723cb8-9873-453f-b3c3-3705364c2ae2",
   "metadata": {},
   "source": [
    "## 3.2 Optymalizacja wczytywania plików w bibliotece pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76324387-a1b8-4c59-a0d7-b017800e53aa",
   "metadata": {},
   "source": [
    "Plik źródłowy jest mały, więc trudno będzie miarodajnie zmierzyć różnice pomiędzy różnymi sposobami jego wczytywania. Sztucznie zwielokrotnimy więc dane we wczytanej ramce danych i zapiszemy do nowego pliku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7767462a-20bf-44d1-a197-0695fa1645a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zwiększamy ramkę 50 000 razy - uwaga z wartością tego parametru w zależności od ilości dostępnej pamięci RAM\n",
    "new_df = pd.concat([df.sample(frac=1) for n in range(50_000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2db8f0f-ccf9-41b1-8eee-191f5e7cc606",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943e0b37-22d5-4cd6-8e9e-cca151a2e464",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizeof_fmt(sum(new_df.memory_usage(deep=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fe7142-2fba-4f46-a171-ea4a109b6352",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('zamowienia_expanded.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e5313e-bb57-4256-841c-30edd659d9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "new_df = pd.read_csv('zamowienia_expanded.csv', header=0)\n",
    "print(f\"Czas wczytywania case 1: {datetime.now() - start} sekund\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e02ecc-7b08-4b8a-93b1-29882eb5d0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aby nie dodawać każdorazowo linii kodu z pomiarem czasu opakujemy tę część w dekorator, który można wielokrotnie reużywać\n",
    "\n",
    "def count_time(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = datetime.now()\n",
    "        func(*args, **kwargs)\n",
    "        print(f\"Czas wczytywania {func.__name__}: {datetime.now() - start} sekund\")\n",
    "        return func(*args, **kwargs)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f22e78-ce54-4aab-b646-31098015e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dekoratorów można używać w postaci adnotacji poprzedzającej definicję funkcji, którą następnie musimy jeszcze wywołać.\n",
    "# Poniższe dwie funkcje wczytują plik csv na dwa sposoby, pierwsza wczytuje plik \"na raz\", a druga dzieląc go na części\n",
    "# składające się z ilości linii przekazanych przez parametr chunksize. Każdy wczytany fragment to oddzielna ramka danych,\n",
    "# którą możemy scalić lub przetwarzać oddzielnie.\n",
    "\n",
    "@count_time\n",
    "def read_file_1():\n",
    "    return pd.read_csv('zamowienia_expanded.csv', header=0)\n",
    "    \n",
    "@count_time\n",
    "def read_file_2():\n",
    "    chunks = pd.read_csv('zamowienia_expanded.csv', header=0, chunksize=4_000_000)\n",
    "    return pd.concat(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492c3326-95fd-4fc6-b888-5656cd9c8dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = read_file_1()\n",
    "df2 = read_file_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e70e9ca-ced9-48d4-b340-82359450f641",
   "metadata": {},
   "source": [
    "> Porównując jedynie czas wykonania, niewielką przewagę będzie posiadała metoda wczytująca plik \"na raz\", jednak jeżeli popatrzymy na utylizację pamięci RAM w trakcie obu procesów to w zależności od systemu operacyjnego jej wykorzystanie może się różnić. W systemie Windows przy wykorzystaniu metody wczytującej plik we fragmentach można dostrzec spadki wykorzystania pamięci RAM w dość równych odstępach czasu. Będzie się to zbiegało z wczytywaniem kolejnych chunków. To powoduje, że maksymalny peak utylizacji pamięci RAM będzie niższy w przypadku wczytywania z podziałem na części. Im większy plik, tym ta różnica będzie wzrastać."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3455bbdc-0130-47fe-96ad-483cc20669f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizeof_fmt(sum(df1.memory_usage(deep=True))), sizeof_fmt(sum(df2.memory_usage(deep=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0a46f2-8acf-4d8e-a4c7-fedae9f9bb34",
   "metadata": {},
   "source": [
    "Warto też zwrócić uwagę na różnicę w rozmiarze pliku csv vs. rozmiar w pamięci RAM po wczytaniu do pandas DataFrame z domyślnymi typami danych."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff46152a-a5d2-4860-940f-f064f0f5b075",
   "metadata": {},
   "source": [
    "**Inne formaty plików**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a32600-198f-42ed-ba14-19590cb62c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format parquet\n",
    "import fastparquet\n",
    "\n",
    "df1.to_parquet('zamowienia_expanded.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b199dd-4fa1-45fb-b5c2-0df8865d4941",
   "metadata": {},
   "outputs": [],
   "source": [
    "@count_time\n",
    "def read_parquet_1():\n",
    "    df = pd.read_parquet('zamowienia_expanded.parquet', engine='fastparquet')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f449d0bb-7721-4aac-9247-21d4d1113279",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = read_parquet_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6943724d-0e92-4745-9f52-3a0d02eadd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizeof_fmt(sum(df3.memory_usage(deep=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad872fb8-1dfe-4f7b-805f-a12d05a8a396",
   "metadata": {},
   "source": [
    "**Multiprocessing**\n",
    "\n",
    "Ten kod należy uruchomić poza Jupyter Notebookiem, gdyż nie jest on obsługiwany dla tego przypadku. Pamiętaj o dodaniu zdefiniowanej wcześniej funkcji `count_time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b6e4e7-1323-439f-bf55-9fd2cc69e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import repeat\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from filesplit.split import Split\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "\n",
    "\n",
    "def apply_args_and_kwargs(func, args, kwargs):\n",
    "    return func(*args, **kwargs)\n",
    "\n",
    "\n",
    "def starmap_with_kwargs(pool, func, args_iter, kwargs_iter):\n",
    "    args_for_starmap = zip(repeat(func), args_iter, kwargs_iter)\n",
    "    return pool.starmap(apply_args_and_kwargs, args_for_starmap)\n",
    "\n",
    "\n",
    "def split_file(filepath, chunksize, destination):\n",
    "    split = Split(filepath, destination)\n",
    "    split.bylinecount(linecount=chunksize, includeheader=True)\n",
    "\n",
    "\n",
    "@count_time\n",
    "def load_files(directory):\n",
    "\n",
    "    files = [[f\"{directory}/{f}\"] for f in os.listdir(directory) if f.endswith(\".csv\")]\n",
    "\n",
    "    kwargs_list = [\n",
    "        {\n",
    "            'on_bad_lines': \"skip\",\n",
    "        }\n",
    "        for n in range(len(files))\n",
    "    ]\n",
    "\n",
    "    pool = Pool(processes=5)\n",
    "    args_iter = files\n",
    "\n",
    "    results = starmap_with_kwargs(pool, pd.read_csv, args_iter, kwargs_list)\n",
    "    results = pd.concat(results)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    split_file('zamowienia_expanded.csv', 8_000_000, 'data')\n",
    "    df4 = load_files('data')\n",
    "    df4.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb79910-3af9-4dd3-bf1a-dc2c6c249b39",
   "metadata": {},
   "source": [
    "## 3.3 Optymalizacja wykorzystania pamięci RAM ramek biblioteki pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c6c4de-dc64-46ab-ac00-fab867774b00",
   "metadata": {},
   "source": [
    "Każda kolumna danych z pliku wczytanego do ramki pandas otrzymuje swój typ, który wynika z zawartości danych w tej kolumnie. Przydzielanie tych typów może być automatyczne (domyślnie), ale można również wskazać pożądany typ lub zmienić go już po wczytaniu danych. Automatyczne przydzielanie typów bywa czasami bardzo nieoptymalne pod kątem wykorzystania pamięci RAM i może w pewnych przypadkach uniemożliwić przetwarzanie zbioru (błędy out of memory), który po optymalizacji tych typów, może na danej maszynie jednak być przetworzony."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db2285e-7bc7-4bed-a63b-2f0325a02e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dla przypomnienia zerknijmy na typy danych ustawione automatycznie\n",
    "df1.info() # lub df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c958fa-4153-4790-8796-30d0e78d698d",
   "metadata": {},
   "source": [
    "Mamy 3 kolumny typu 'object' (typ str) oraz po jednej typu int64 oraz float64. Pamiętajmy tutaj, że biblioteka pandas wykorzystuje struktury danych z biblioteki numpy (która jest wrapperem do stosownego kodu napisanego w języku C) do przechowywania danych. Mamy więc do dyspozycji znacznie więcej typów niż natywnie dostępne standardowo w Pythonie. Więcej tutaj: https://numpy.org/doc/stable/user/basics.types.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f6b326-58b4-4cf7-a8dc-eec8b8412e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# poznanie zakresu danych powinno pomóc w ocenie czy dobrany typ danych numerycznych jest optymalny\n",
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4a9955-46fd-4634-9225-a553fc80eb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zmiana domyślnej precyzji formatu wyświetlania danych\n",
    "pd.options.display.float_format = '{:.5f}'.format\n",
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60609e22-5aa7-4dca-a40c-fedc76de4cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df1.columns:\n",
    "    print(f'{column}: {sizeof_fmt(df1[column].memory_usage(deep=True))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa5d9b6-f0bf-4fe6-b372-022a2331f982",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizeof_fmt(df1['idZamowienia'].astype(np.int16).memory_usage(deep=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ac0466-af9d-45a2-9337-66cf0e26b1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizeof_fmt(df1['Kraj'].astype('category').memory_usage(deep=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9839b62f-0e92-455f-93ff-37fff2e7ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizeof_fmt(df1['Sprzedawca'].astype('category').memory_usage(deep=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc7286b-2f19-4cf1-94db-958d377138a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizeof_fmt(pd.to_datetime(df1['Data zamowienia']).memory_usage(deep=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe33d37-5017-41ba-9a9c-0cf683ae6eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tworzymy pistą ramkę danych, aby przechować w niej dane w nowym, bardziej optymalnym formacie\n",
    "df2 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5e8df7-9e3e-4d01-8da8-82c74f9bdc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zmieniamy format niektórych kolumn\n",
    "df2['Kraj'] = df1['Kraj'].astype('category')\n",
    "df2['Sprzedawca'] = df1['Sprzedawca'].astype('category')\n",
    "df2['Data zamowienia'] = pd.to_datetime(df1['Data zamowienia'])\n",
    "df2['idZamowienia'] = df1['idZamowienia'].astype(np.int16)\n",
    "df2['Utarg'] = df1['Utarg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baee9402-0c0a-47c0-81d3-136853cd9c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizeof_fmt(sum(df2.memory_usage(deep=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8889a2f2-62e9-4ec4-901a-fb8a5ac52e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# możemy również spróbować wykonać downcasting dla kolumn numerycznych wykorzystując wbudowaną funkcję biblioteki panda to_numeric\n",
    "utarg_downcast = pd.to_numeric(df2[\"Utarg\"], downcast='float')\n",
    "sizeof_fmt(utarg_downcast.memory_usage(deep=True)), utarg_downcast.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfa63a4-5321-44d3-bb82-c6bf24e4e39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ostatecznie uzyskamy\n",
    "df2['Utarg'] =  pd.to_numeric(df1[\"Utarg\"], downcast='float')\n",
    "sizeof_fmt(sum(df2.memory_usage(deep=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03db0c54-f8ad-4a50-907c-11efed43e8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i dla każdej kolumny oddzielnie\n",
    "for column in df2.columns:\n",
    "    print(f'{column}: {sizeof_fmt(df2[column].memory_usage(deep=True))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71457228-cafd-4f38-a5ab-a49f014972a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ebde97-bc53-4322-8dde-bd1802d5a6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b487ed57-aae2-4486-bf2e-37d25c87d720",
   "metadata": {},
   "source": [
    "#### Porównanie czasów wykonania dla oryginalnej ramki oraz ramki zoptymalizowanej "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3f6c11-6a4d-42ff-b112-06e950e09e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "display(df1.groupby(['Sprzedawca']).agg({'Utarg': ['mean']}))\n",
    "print(f'Czas: {datetime.now() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77424a71-f3e7-49a6-b57d-3b0b51504c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "display(df2.groupby(['Sprzedawca']).agg({'Utarg': ['mean']}))\n",
    "print(f'Czas: {datetime.now() - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a80878-6752-4c03-86cc-408ea4f96f0c",
   "metadata": {},
   "source": [
    "## Zadania\n",
    "\n",
    "> Zbiór danych do wykonania zadań: https://huggingface.co/datasets/vargr/private_instagram/tree/refs%2Fconvert%2Fparquet/default/train\n",
    ">\n",
    "> **UWAGA!**  \n",
    "> W zależności od ilości pamięci RAM pobierz tyle plików, aby możliwe było wczytanie danych do pamięci RAM.\n",
    "> Spróbuj dobrać tyle danych, aby maksymalnie wykorzystać pamięć operacyjną.\n",
    "> Możesz również spróbować dobrać więcej danych niż zmieści się w pamięci operacyjnej w celu wywołania błędu biblioteki pandas \n",
    "\n",
    "**Zadanie 1**  \n",
    "Wczytaj pliki danych i scal je w jedną ramkę DataFrame.\n",
    "Wykonaj analizę typów danych podobnie jak w przykładach.\n",
    "Zmierz wielkość pamięci RAM ramki z domyślnymi typami danych.\n",
    "\n",
    "**Zadanie 2**  \n",
    "Dobierz bardziej optymalne typy danych i ponownie zmierz wielkość zajmowanej pamięci RAM.\n",
    "Porównaj obie wielkości na wykresie (wybierz pasujący typ wykresu).\n",
    "\n",
    "**Zadanie 3**  \n",
    "Wykonaj 3 wybrane operacje (grupowanie + agregacja, filtrowanie, itp.) na całej ramce i zmierz czas wykonania na danych oryginalnych i zoptymalizowanych.\n",
    "Wyświetl te czasy.\n",
    "\n",
    "**Zadanie 4**  \n",
    "Zapisz ramkę jako plik csv, z nagłówkami kolumn, bez indeksu.\n",
    "Sprawdź jaka jest różnica w wielkości pliku csv i sumy wielkości plików w formacie parquet (w eksploratorze, nie trzeba tego robić z poziomu kodu).\n",
    "\n",
    "**Zadanie 5**  \n",
    "Zmierz czas wczytywania danych z pliku csv dla 3 przypadków:\n",
    "* cały plik na raz,\n",
    "* cały plik ze wskazaniem parametru `chunksize` (możesz poeksperymentować z wielkością tego parametru),\n",
    "* z użyciem multiprocessingu zaprezentowanego w przykładzie (wcześniej podziel plik na kilka mniejszych), wskazując ilość procesów jako `ilość_rdzeni - 2` oraz drugi przypadek `(ilosc_rdzeni - 2) * 2`.\n",
    "\n",
    "\n",
    "**Zadanie 6** (z gwiazdką, nie jest obowiązkowe, ale pouczające)  \n",
    "\n",
    "Wczytaj każdy plik podzielony w zadaniu 5 do oddzielnej ramki danych. \n",
    "Dla każdej ramki policz sumę na kolumnie `likes`, a następnie policz sumę tych sum. Tę część zadania wykonaj sekwencyjnie.\n",
    "Teraz wykorzystując multiprocessing (i przykłady z labu) wykonaj to samo zadanie zrównoleglając je. Zmierz czas obu przypadków i go wyświetl.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
