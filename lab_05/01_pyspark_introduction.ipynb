{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fa05bbf-4cf7-4e10-ba10-db527e9176d6",
   "metadata": {},
   "source": [
    "# Lab 5 - Apache Spark - wprowadzenie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50d92ad-8fc0-468f-9e51-fe5bceed2fb1",
   "metadata": {},
   "source": [
    "Apache Spark jest silnikiem do przetwarzania dancych na dużą skalę, pozwalający na wykonywanie operacji w sposób zrównoleglony i rozproszony. Spark dostarcza API dla języków java, Scala, Python oraz R do przetwarzania grafów obliczeń. Spark składa się z wielu narzędzi takich jak:\n",
    "* Resilient Distributed Datasets (RDD) - niskopoziomowy typ zbioru danych Spark, na którym opierają się struktury danych na wyższych poziomach abstrakcji,\n",
    "* Spark SQL - Spark Dataset, Spark DataFrame,\n",
    "* Pandas API on Spark - API pozwalające na wykorzystanie biblioteki pandas w sposób zrównoleglony na klastrze Spark,\n",
    "* Structured Streaming - zestaw narzędzi do przetwarzania strumieniowego,\n",
    "* MLlib - moduł wspierający wykorzystanie Machine Learning z użyciem typów danych Spark oraz klastrów Spark,\n",
    "* GraphX - przetwarzanie grafów,\n",
    "* SparkR - API w języku R do pracy w środowisku Spark,\n",
    "* PySpark - API Python do pracy w środowisku Spark,\n",
    "* Spark SQL CLI - przetwarzanie danych z użyciem Spark SQL z poziomu wiersza poleceń."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b99b46f-13ca-4762-9858-d3b201e670f8",
   "metadata": {},
   "source": [
    "## 1. Instalacja PySpark i przygotowanie środowiska pracy.\n",
    "\n",
    "W trakcie zajęc z racji dotychczasowego środowiska pracy (Python) będzie wykorzystywane API Pythona, które dostarcza Spark.\n",
    "\n",
    "**Dokumentacja Spark Python API:** https://spark.apache.org/docs/3.5.3/api/python/index.html\n",
    "\n",
    "> Oficjalna dokumentacja alternatywnych sposobów instalacji oraz zależności modułu PySpark: https://spark.apache.org/docs/latest/api/python/getting_started/install.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97b77cf-2391-4f51-b98a-9002bc22d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SPARK_NAME'] = \"/opt/spark\"\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "# os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/opt/spark/work-dir/.venv/bin/python3'\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/spark/work-dir/.venv/bin/python3'\n",
    "\n",
    "# można też spróbować wykorzystać moduł findspark do automatycznego odnalezienia miejsca instalacji sparka\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "# lub\n",
    "# findspark.init(\"/opt/spark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a1abfd-62dd-41ff-99f1-66c055883a93",
   "metadata": {},
   "source": [
    "> Przed uruchomieniem poniższej komórki należy zainstalować moduł `pyspark` do środowiska wirtualnego Pythona"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efbb0fb-ebe5-4450-bb02-8428f5e3d3ee",
   "metadata": {},
   "source": [
    "> Oficjana dokumentacja konfiguracji Sparka, również parametrów wywołania: https://spark.apache.org/docs/3.5.3/configuration.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14aaa31-0a62-4890-9646-74cea29e2e1a",
   "metadata": {},
   "source": [
    "Aby możliwe było wysłanie zadań do wykonania z wykorzystaniem klastra Spark (lokalnego lub rozproszonego) musimy uzyskać najpierw referencję do obiektu typu SparkSession. Tworząc obiekt musimy zdefiniować nazwę aplikacji, którą otrzyma ta sesja oraz możemy zdefiniować dodatkowe parametry. Przykład poniżej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7a59b7-ad6d-43ff-b5dd-2bc48d45bd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession.builder.master(\"spark://spark-master:7077\").appName(\"Create-DataFrame\").getOrCreate()\n",
    "# konfiguracja z określeniem liczby wątków (2) oraz ilości pamięci do wykorzystania poza stertą interpretera Pythona\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[2]\")\\\n",
    "        .appName(\"Create-DataFrame\")\\\n",
    "        .config(\"spark.memory.offHeap.enabled\",\"true\")\\\n",
    "        .config(\"spark.memory.offHeap.size\",\"4g\")\\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f167ac46-653d-489a-9e63-b31fae824f3d",
   "metadata": {},
   "source": [
    "Podobnie jak w przypadku biblioteki Dask możemy śledzić pracę klastra poprzez przeglądarkę. Adres to http://\\<host\\>:4040. Wykorzystując dockera wymagane jest stworzenie odpowiedniego mapowania portu oraz w naszym przypadków również zmiany domyślnego hosta, który zapewne będzie skróconą wersją hasha kontenera. Skoro port jest zmapowany to znaczy, że będzie dostępny na hoście lokalnym. Finalnie więc adres dla węzła master powinien być dostępny pod adresem `http://localhost:4040`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f834a08-53fb-456e-9c8d-ab5dca9b3a2c",
   "metadata": {},
   "source": [
    "## 2. Spark Resilient Distributed Datasets (RDD)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0646cd6f-408e-41db-ab88-29d2a1f4913c",
   "metadata": {},
   "source": [
    "Na wysokim poziomie każda aplikacja Spark składa się z **programu sterownika (ang. driver program)**, który uruchamia główną funkcję użytkownika i wykonuje różne operacje równoległe na klastrze. \n",
    "\n",
    "Główną abstrakcją zapewnianą przez Spark jest **odporny rozproszony zbiór danych (Resilient Distributed Datasets - RDD)**, który jest zbiorem elementów podzielonych na węzły klastra, które mogą być obsługiwane równolegle. RDD mogą być również utrwalane w pamięci klastra, umożliwiając jego ponowne wykorzystanie w operacjach równoległych. Wreszcie, RDD automatycznie odzyskują sprawność po awarii węzła.\n",
    "\n",
    "Wykorzystanie API RDD nie jest rekomendowane w wielu przypadkach, gdyż wymaga dobrej znajomości niskopoziomowego API Sparka i ręcznej optymalizacji operacji. Zazwyczaj lepszym pomysłem będzie wykorzystanie Spark Dataset oraz Spark DataFrame.\n",
    "\n",
    "Kilka przykładów wykorzystania tego API zostanie jednak tutaj przedstawionych.\n",
    "\n",
    "> Poradnik programisty dla RDD znajduje się pod linkiem: https://spark.apache.org/docs/3.5.3/rdd-programming-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98272878-be6a-42ca-95df-bdfaddb12643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista wartości zostaje podzielona na partycje i rozproszona na wszystkie dostępne węzły\n",
    "# https://spark.apache.org/docs/3.5.3/api/python/reference/api/pyspark.SparkContext.parallelize.html\n",
    "rdd = spark.sparkContext.parallelize(list(range(20)))\n",
    "\n",
    "# rdd w formie rozproszonej zostaje scalone w listę zawierającą wszystkie elementy RDD\n",
    "# np. za pomocą funkcji collect()\n",
    "# https://spark.apache.org/docs/3.5.3/api/python/reference/api/pyspark.RDD.collect.html\n",
    "\n",
    "rddCollect = rdd.collect()\n",
    "display(type(rddCollect))\n",
    "print(f\"Liczba partycji: {rdd.getNumPartitions()}\")\n",
    "print(f\"Pierwszy element: {rdd.first()}\")\n",
    "print(rddCollect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fd79bf-7544-4d16-8b86-7b5dd0fabf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obiekt RDD może przechowywać dane z różnych źródeł, które są zgodne z systemem plików Apache Hadoop\n",
    "# np. Amazon S3, Cassandra, HDFS, HBase i inne\n",
    "\n",
    "# możemy dla uniknięcia potrzeby każdorazowego odwoływania się do kontekstu poprzez spark.sparkContext zapisać sobie to w zmiennej pomocniczej\n",
    "sc = spark.sparkContext\n",
    "# tutaj wczytamy do RDD plik tekstowy\n",
    "pan_tadeusz_file = sc.textFile(\"pan-tadeusz.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080ebe6e-b412-4c4a-b550-9b489a8b00e7",
   "metadata": {},
   "source": [
    "Więcej informacji odnośnie obsługi plików w środowisku Spark można znaleźć m.in. tu: https://spark.apache.org/docs/3.5.3/rdd-programming-guide.html#external-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63510143-12ef-4a56-bb50-15d993fb6e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pan_tadeusz_file.getNumPartitions())\n",
    "pan_tadeusz_file.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08afe171-686f-44f9-b9dd-ddd309397aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# możemy zmienić liczbę automatycznie stworzonych partycji i ponownie rozproszyć je po węzłach\n",
    "pan_tadeusz_file = pan_tadeusz_file.repartition(4)\n",
    "pan_tadeusz_file.getNumPartitions()\n",
    "\n",
    "# również metoda coalesce może posłużyć nam do zmiany ilości partycji dla obiektu RDD np. po zastosowaniu filtrowania, które\n",
    "# znacznie zmniejsza wielkość pierwotnego obiektu RDD a co za tym idzie każdej partycji i dalsze obliczenia mogą nie być\n",
    "# wykonywane zbyt efektywnie (zbyt mały rozmiar partycji)\n",
    "# https://spark.apache.org/docs/3.5.3/api/python/reference/api/pyspark.RDD.coalesce.html\n",
    "# główna różnica między repartition a coalesce jest taka, że ta pierwsza wykorzystuje mechanizm tasowania danych a ta druga może, ale nie\n",
    "# musi go wykorzystywać gdyż możemy tym sterować za pomocą parametru wywołania tej metody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fcddb4-0dcb-4820-ab88-80e7987e1079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jedną z funkcji dostępnej w tym API jest możliwość wykonania funkcji na każdej z partycji\n",
    "# minusem może być to, że funkcja foreachPartition zwraca typ None, więc wyniki należy przetworzyć w inny sposób\n",
    "\n",
    "def count_words(iterator):\n",
    "    words = sum([len(x.split()) for x in iterator])\n",
    "    print(words)\n",
    "\n",
    "pan_tadeusz_file.foreachPartition(count_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94610e50-5d24-4f09-aa82-002c89a9031e",
   "metadata": {},
   "source": [
    "Przy wypisywaniu wartości z RDD trzeba również zwrócić uwagę na różnicę w działaniu tych metod w trybie pracy lokalnej (czyli tak jak w tym labie) oraz klastra. Efekty mogą być różne, więcej przeczytasz tu: https://spark.apache.org/docs/3.5.3/rdd-programming-guide.html#printing-elements-of-an-rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5e2f7e-7da8-4cfd-bbff-7d882194392c",
   "metadata": {},
   "source": [
    "**RDD obsługują dwa rodzaje operacji: transformacje**, które tworzą nowy zbiór danych z istniejącego, oraz **akcje**, które zwracają wartość do programu sterownika po uruchomieniu obliczeń na zbiorze danych. Przykładowo, map jest transformacją, która przepuszcza każdy element zbioru danych przez funkcję i zwraca nowy RDD reprezentujący wyniki. Z drugiej strony, reduce jest akcją, która agreguje wszystkie elementy RDD przy użyciu pewnej funkcji i zwraca końcowy wynik do programu sterownika.\n",
    "\n",
    "Wszystkie transformacje są wykonywane w sposób **leniwy** tzn, że obliczenia nie są wykonywane dopóki nie jest potrzebnych wynik. To podobnie jak w przypadku frameworka Dask umożliwia optymalizację obliczeń np. w przypadku gdy nie są potrzebne wyniki pośrednie z każdego węzła po zastosowaniu funkcji poprzez `map`, ale tylo wynik akcji `reduce`, więc nie ma potrzeby przesyłania całych pośrednich RDD do drivera.\n",
    "\n",
    "**Lista wybranych transformacji dostępna jest tu:** https://spark.apache.org/docs/3.5.3/rdd-programming-guide.html#transformations\n",
    "\n",
    "**Lista wybranych akcji tu:** https://spark.apache.org/docs/3.5.3/rdd-programming-guide.html#actions\n",
    "\n",
    "W uzasadnionych przypadkach można również przyspieszyć obliczenia poprzez utrwalenie danych w pamięci lub pamięci podręcznej poprzez metody `persist` lub `cache` na obiekcie RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789f6556-df63-438f-997b-75dcab6466d8",
   "metadata": {},
   "source": [
    "**Kilka przykładów transformacji i akcji**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f65130-0146-4bf1-9ca7-d7d9af40e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funkcje map oraz reduce\n",
    "\n",
    "# możemy również wykonać operację w inny sposób, tym raze mapując funkcję na każdy element obiektu RDD\n",
    "# zwrócony zostanie obiekt RDD, na którym możemy wykonać kolejne operacje\n",
    "pan_tadeusz_file.map(lambda s: len(s.split())).take(10)\n",
    "\n",
    "# np. reduce - i tu nawiązanie do dość znanej techniki przetwarzania, MapReduce\n",
    "# więcej: https://en.wikipedia.org/wiki/MapReduce\n",
    "# oraz: https://wiadrodanych.pl/big-data/jak-dziala-mapreduce/\n",
    "pan_tadeusz_file.map(lambda s: len(s.split())).reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e577a083-a73f-4b4e-aa2e-9ff5abf9be64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lub tak - ten sam efekt\n",
    "from operator import add\n",
    "pan_tadeusz_file.map(lambda s: len(s.split())).reduce(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01115c0d-e2b1-4c2a-8812-6dc30f7b7537",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# różnica między map() a flatMap() dla tego przypadku\n",
    "display(pan_tadeusz_file.map(lambda s: s.split()).take(10))\n",
    "pan_tadeusz_file.flatMap(lambda s: s.split()).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113c9de6-9853-447b-8cbb-73bd9c42a50c",
   "metadata": {},
   "source": [
    "### Zadania"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0440a501-ff7a-4d48-a72a-f4d348990bd8",
   "metadata": {},
   "source": [
    "**Zadanie 1**  \n",
    "\n",
    "Wykorzystując plik z treścią Pana Tadeusza zaprezentowany w przykładach policz ile jest linii, w których zawiera się słowo `Tadeusz`. Wykorzystaj akcje i transformacje RDD.\n",
    "\n",
    "**Zadanie 2**  \n",
    "Wykorzystując metodę `top()` dla obiektu RDD wyświetl 3 najdłuższe linie wczytane z pliku z książką Pan Tadeusz.\n",
    "\n",
    "**Zadanie 3**  \n",
    "Wykorzystując listę stopwords z adresu https://github.com/bieli/stopwords/blob/master/polish.stopwords.txt wykorzystaj akcje i transformacje RDD i wygeneruj listę unikalnych słów z pliku z treścią Pana Tadeusza (sprawdź transformację countByValue()) pomijając powyższe słowa stop oraz wszelkie znaki przestankowe. Wynik zapisz do słownika, a następnie do pliku json o nazwie pan_tadeusz_bag_of_words.json.\n",
    "Które słowo występuje w tym tekście najczęściej? Wyświetl je z wyników wygenerowanych powyżej."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
