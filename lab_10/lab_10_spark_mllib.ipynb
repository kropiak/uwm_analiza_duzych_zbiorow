{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c4698f-f88f-470a-96c5-4f6810197244",
   "metadata": {},
   "source": [
    "# Spark MLlib z wykorzystaniem PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf9d9d4-021b-41fe-8ff8-acc7cbae5824",
   "metadata": {},
   "source": [
    "## 1. Czy jest Spark MLlib?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79d60dc-d61f-4d7f-9312-b060e0cbc11d",
   "metadata": {},
   "source": [
    "MLlib to biblioteka Machine Learning silnika Spark, której architektura kontrukcji potoków ML jest bardzo podobna do tej z pakietu [scikit-learn](https://scikit-learn.org/stable/).\n",
    "\n",
    "Jej możliwości to:\n",
    "\n",
    "* Algorytmy uczenia maszynowego: klasyfikacja, regresja, klastrowanie i filtrowanie oparte na współpracy (collaborative filtering).\n",
    "* Featurization: ekstrakcja cech, transformacja, redukcja wymiarowości i selekcja\n",
    "* Potoki (pipelines): narzędzia do konstruowania, oceny i dostrajania potoków ML\n",
    "* Trwałość: zapisywanie i ładowanie algorytmów, modeli i potoków\n",
    "* Narzędzia: algebra liniowa, statystyka, obsługa danych itp.\n",
    "\n",
    "Biblioteki obliczeń numerycznych Spark MLlib mogą wykorzystywać zewnętrzne biblioteki w celu przyspieszania konkretnych obliczeń w zależności od tego czy są dostępne w systemie uruchomieniowym. Może to być np. [Intel MKL](https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl.html), [Open BLAS](http://www.openmathlib.org/OpenBLAS/) czy numpy w przypadku PySpark (numpy w wersji minimum 1.4).\n",
    "\n",
    "\n",
    "Listę dostępnych komponentów z wyżej wymienionych kategorii znjadziemy w dokumentacji API Spark MLlib:\n",
    "\n",
    "> Lista komponentów API MLlib: https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26317d4-06d6-4991-ad3b-0a2e08010b84",
   "metadata": {},
   "source": [
    "### 1.1. Komponenty potoku ML w Spark MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3e6ee7-d3d5-4c62-836f-e2d0419067fc",
   "metadata": {},
   "source": [
    "#### 1.1.1 Transformer\n",
    "\n",
    "> Dokumentacja API: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.Transformer.html\n",
    "\n",
    "Jest to algorytm, który przekształca obiekt DataFrame w inny obiekt DataFrame. Może to być algorytm, który mapuje funkcję na kolumnie (wykonuje operację na każdym jej wierszu), a następnie zwraca nowy obiekt DataFrame z tą dodatkową kolumną. Może to być również zastosowanie modelu ML, który do przekazanej ramki Spark doda kolumnę z predykcjami dla obserwacji z tej ramki.\n",
    "\n",
    "MLlib dostarcza abstrakcyjnej klasy, która zapewnia metodę `transform()`, która jest głównym jej komponentem i to poprzez tę metodę przekazujemy dane do wykonania transformacji.\n",
    "\n",
    "#### 1.1.2 Estimator\n",
    "\n",
    "> Dokumentacja API: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.Estimator.html\n",
    "\n",
    "Estymator jest również Transformerem (może również zwracać obiekt typu Transformer dla ramki danych Spark), ale tak mogło się stać musi być wcześniej zainicjalizowany na danych (wywołanie metody `fit()`. Można to porównać do wywołania metody statycznej (Transformer) oraz metody obiektu (Estimator). Tutaj może to być wywołanie np. LogisticRegression, która jest transformerem, gdyż dodaje predykcje do przekazanej ramki danych, ale wymaga wcześniejszego \"wytrenowania\" (metoda `fit()`) aby móc dokonać transformacji, więc jest również estymatorem.\n",
    "\n",
    "\n",
    "#### 1.1.3 Pozostałe elementy potoków:\n",
    "\n",
    "* **Models** - są to obiekty wytrenowanych modeli zwracane przez Estymatory,\n",
    "* **Params** - obiekty, które wykorzystywane są do definiowania parametrów w potoku. Patrz listing 4, w którym został zdefiniowany nowy parametr,\n",
    "* **Predictor** - dedykowana klasa estymatora do zadań regresji i klasyfikacji."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3a5f74-5863-4edc-bde2-2c1ec36ce862",
   "metadata": {},
   "source": [
    "## 2. Przykłady"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaed3c30-1117-41ca-b55a-75495e02ba62",
   "metadata": {},
   "source": [
    "**_Listing 1 - uruchomienie środowiska Spark_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "595af1b1-6d58-4c13-ba08-b4cf7a51ad68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T08:10:42.845578Z",
     "iopub.status.busy": "2024-12-18T08:10:42.845018Z",
     "iopub.status.idle": "2024-12-18T08:10:48.934359Z",
     "shell.execute_reply": "2024-12-18T08:10:48.933163Z",
     "shell.execute_reply.started": "2024-12-18T08:10:42.845555Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/18 08:10:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://a4d204ef8b6f:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark MLlib</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=Spark MLlib>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "findspark.init()\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[2]\")\\\n",
    "        .appName(\"Spark MLlib\")\\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ce1826-e618-486a-b477-cda5d9050859",
   "metadata": {},
   "source": [
    "### 2.1 Transformery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee780dfa-15db-4407-bc63-d69954727b7f",
   "metadata": {},
   "source": [
    "**_Listing 2 - Binarizer_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "620e7f03-9eab-454b-8960-3ba40a1e76ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T08:24:26.206925Z",
     "iopub.status.busy": "2024-12-18T08:24:26.206295Z",
     "iopub.status.idle": "2024-12-18T08:24:26.769589Z",
     "shell.execute_reply": "2024-12-18T08:24:26.768596Z",
     "shell.execute_reply.started": "2024-12-18T08:24:26.206891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|values|\n",
      "+------+\n",
      "|   0.5|\n",
      "|   0.1|\n",
      "|   0.6|\n",
      "|   0.9|\n",
      "|   0.0|\n",
      "|   0.7|\n",
      "|   1.2|\n",
      "+------+\n",
      "\n",
      "inputCol: input column name. (current: values)\n",
      "inputCols: input column names. (undefined)\n",
      "outputCol: output column name. (default: Binarizer_a5fe8a6f20b0__output, current: features)\n",
      "outputCols: output column names. (undefined)\n",
      "threshold: Param for threshold used to binarize continuous features. The features greater than the threshold will be binarized to 1.0. The features equal to or less than the threshold will be binarized to 0.0 (default: 0.0, current: 0.5)\n",
      "thresholds: Param for array of threshold used to binarize continuous features. This is for multiple columns input. If transforming multiple columns and thresholds is not set, but threshold is set, then threshold will be applied across all columns. (undefined)\n",
      "+------+--------+\n",
      "|values|features|\n",
      "+------+--------+\n",
      "|   0.5|     0.0|\n",
      "|   0.1|     0.0|\n",
      "|   0.6|     1.0|\n",
      "|   0.9|     1.0|\n",
      "|   0.0|     0.0|\n",
      "|   0.7|     1.0|\n",
      "|   1.2|     1.0|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "from pyspark.ml import Transformer\n",
    "\n",
    "df = spark.createDataFrame([(0.5,), (0.1,), (0.6,), (0.9,), (0.0,), (0.7,), (1.2,)], [\"values\"])\n",
    "binarizer = Binarizer(threshold=0.5, inputCol=\"values\", outputCol=\"features\")\n",
    "\n",
    "df.show()\n",
    "print(binarizer.explainParams())\n",
    "isinstance(binarizer, Transformer)\n",
    "\n",
    "# i ostatcznie wykonanie transformacji\n",
    "df_transformed = binarizer.transform(df)\n",
    "df_transformed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4a5afc-e3c1-4bdd-8ab3-baf484b19cda",
   "metadata": {},
   "source": [
    "**_Listing 3 - Tokenizer oraz RegexTokenizer_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9295b403-f4ad-4b33-a1ac-a664b50e0400",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T09:04:45.087996Z",
     "iopub.status.busy": "2024-12-18T09:04:45.087694Z",
     "iopub.status.idle": "2024-12-18T09:04:45.358624Z",
     "shell.execute_reply": "2024-12-18T09:04:45.357709Z",
     "shell.execute_reply.started": "2024-12-18T09:04:45.087976Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|zdania                                      |\n",
      "+--------------------------------------------+\n",
      "|Ala ma kota.                                |\n",
      "|Polacy nie gęsi i swój język mają.          |\n",
      "|Co ma być to będzie...                      |\n",
      "|Pan Tadeusz, czyli ostatni zajazd na Litwie.|\n",
      "+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "\n",
    "df1 = spark.createDataFrame([\n",
    "    ('Ala ma kota.',), \n",
    "    ('Polacy nie gęsi i swój język mają.',), \n",
    "    ('Co ma być to będzie...',), \n",
    "    ('Pan Tadeusz, czyli ostatni zajazd na Litwie.',)], \n",
    "    ['zdania'])\n",
    "\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "334fd150-e1dc-4aaa-9848-d7266cf45679",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T09:05:57.980890Z",
     "iopub.status.busy": "2024-12-18T09:05:57.980615Z",
     "iopub.status.idle": "2024-12-18T09:05:58.272170Z",
     "shell.execute_reply": "2024-12-18T09:05:58.271174Z",
     "shell.execute_reply.started": "2024-12-18T09:05:57.980870Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+----------------------------------------------------+\n",
      "|zdania                                      |tokeny                                              |\n",
      "+--------------------------------------------+----------------------------------------------------+\n",
      "|Ala ma kota.                                |[ala, ma, kota.]                                    |\n",
      "|Polacy nie gęsi i swój język mają.          |[polacy, nie, gęsi, i, swój, język, mają.]          |\n",
      "|Co ma być to będzie...                      |[co, ma, być, to, będzie...]                        |\n",
      "|Pan Tadeusz, czyli ostatni zajazd na Litwie.|[pan, tadeusz,, czyli, ostatni, zajazd, na, litwie.]|\n",
      "+--------------------------------------------+----------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer dzieli łancuch znaków na tokeny na podstawie białych znaków (ang. whitespace)\n",
    "tokenizer = Tokenizer(inputCol='zdania', outputCol='tokeny')\n",
    "df1_transformed = tokenizer.transform(df1)\n",
    "df1_transformed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b398ee-99c0-467f-9f5c-4f70780b9f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pobieramy przykładowy plik\n",
    "!wget https://raw.githubusercontent.com/logpai/loghub/refs/heads/master/Apache/Apache_2k.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bb9bfe3d-bfcb-46fc-af9a-b67adb3c6e05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T09:09:13.053048Z",
     "iopub.status.busy": "2024-12-18T09:09:13.052763Z",
     "iopub.status.idle": "2024-12-18T09:09:13.176748Z",
     "shell.execute_reply": "2024-12-18T09:09:13.175251Z",
     "shell.execute_reply.started": "2024-12-18T09:09:13.053028Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sun Dec 04 04:47:44 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties\n",
      "[Sun Dec 04 04:47:44 2005] [error] mod_jk child workerEnv in error state 6\n",
      "[Sun Dec 04 04:51:08 2005] [notice] jk2_init() Found child 6725 in scoreboard slot 10\n",
      "[Sun Dec 04 04:51:09 2005] [notice] jk2_init() Found child 6726 in scoreboard slot 8\n",
      "[Sun Dec 04 04:51:09 2005] [notice] jk2_init() Found child 6728 in scoreboard slot 6\n"
     ]
    }
   ],
   "source": [
    "!head -5 Apache_2k.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9678722e-15fe-4ed1-a141-6f69d6961923",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T09:29:11.691992Z",
     "iopub.status.busy": "2024-12-18T09:29:11.691705Z",
     "iopub.status.idle": "2024-12-18T09:29:11.815290Z",
     "shell.execute_reply": "2024-12-18T09:29:11.814434Z",
     "shell.execute_reply.started": "2024-12-18T09:29:11.691971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------+\n",
      "|tokens                                                                                     |\n",
      "+-------------------------------------------------------------------------------------------+\n",
      "|[sun dec 04 04:47:44 2005, notice, workerenv.init() ok /etc/httpd/conf/workers2.properties]|\n",
      "|[sun dec 04 04:47:44 2005, error, mod_jk child workerenv in error state 6]                 |\n",
      "|[sun dec 04 04:51:08 2005, notice, jk2_init() found child 6725 in scoreboard slot 10]      |\n",
      "|[sun dec 04 04:51:09 2005, notice, jk2_init() found child 6726 in scoreboard slot 8]       |\n",
      "|[sun dec 04 04:51:09 2005, notice, jk2_init() found child 6728 in scoreboard slot 6]       |\n",
      "+-------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.text('Apache_2k.log')\n",
    "reg_tokenizer = RegexTokenizer(pattern=']\\s\\[|]\\s|^\\[' ,inputCol='value', outputCol='tokens')\n",
    "df_transformed = reg_tokenizer.transform(df2)\n",
    "df_transformed.select(df_transformed.tokens).show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083001a0-60de-4487-a855-46316cde4e9f",
   "metadata": {},
   "source": [
    "**_Listing 4 - Własny Transformer_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bc4aecfa-3cae-427c-a457-3f5a294d0249",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T10:28:49.056165Z",
     "iopub.status.busy": "2024-12-18T10:28:49.055285Z",
     "iopub.status.idle": "2024-12-18T10:28:49.067005Z",
     "shell.execute_reply": "2024-12-18T10:28:49.065698Z",
     "shell.execute_reply.started": "2024-12-18T10:28:49.056142Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "\n",
    "class CustomBinarizer(\n",
    "    Transformer,\n",
    "    HasInputCol,               # Sets up an inputCol parameter\n",
    "    HasOutputCol,              # Sets up an outputCol parameter\n",
    "    DefaultParamsReadable,     # Makes parameters readable from file\n",
    "    DefaultParamsWritable      # Makes parameters writable from file\n",
    "                     ):\n",
    "\n",
    "    true_val = Param(\n",
    "        Params._dummy(),\n",
    "        \"true_val\",\n",
    "        \"List of values to be mapped as logical 1.\",\n",
    "        typeConverter=TypeConverters.toList, \n",
    "    )\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self,  inputCol=None, outputCol=None, true_val: list =None):\n",
    "        super().__init__()\n",
    "        self._setDefault(true_val=None)\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, true_val=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "  \n",
    "    def setTrueVal(self, new_true_val):\n",
    "        return self.setParams(true_val=new_true_val)\n",
    "  \n",
    "    # Required if you use Spark >= 3.0\n",
    "    def setInputCol(self, new_inputCol):\n",
    "        return self.setParams(inputCol=new_inputCol)\n",
    "  \n",
    "    # Required if you use Spark >= 3.0\n",
    "    def setOutputCol(self, new_outputCol):\n",
    "        return self.setParams(outputCol=new_outputCol)\n",
    "  \n",
    "    def getTrueVal(self):\n",
    "        return self.getOrDefault(self.true_val)\n",
    "\n",
    "    def _transform(self, df):\n",
    "        \"\"\"\n",
    "        Przetwarza kolumnę ustawiając wartość na 1 jeżeli zawiera ona jedną z wartości true_val,\n",
    "        w przeciwnym wypadku przypisuje wartość 0\n",
    "        \"\"\"\n",
    "        if not self.isSet(\"inputCol\"):\n",
    "            raise ValueError(\n",
    "                \"No input column set for the \"\n",
    "                \"CustomBinarizer transformer.\"\n",
    "            )\n",
    "        if not self.isSet(\"true_val\"):\n",
    "            raise ValueError(\n",
    "                \"You must provide list of values to map as logical True.\"\n",
    "            )\n",
    "\n",
    "        def binarize(val):\n",
    "            \"\"\"Funkcja pomocnicza\"\"\"\n",
    "            if str(val) in self.getTrueVal():\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        binarize_udf = udf(binarize, StringType())\n",
    "        return df.withColumn(self.getOutputCol(), binarize_udf(df[self.getInputCol()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e748d6e8-f300-425f-b326-6e5e23b88411",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T09:50:35.277590Z",
     "iopub.status.busy": "2024-12-18T09:50:35.277075Z",
     "iopub.status.idle": "2024-12-18T09:50:35.536698Z",
     "shell.execute_reply": "2024-12-18T09:50:35.535516Z",
     "shell.execute_reply.started": "2024-12-18T09:50:35.277566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|log    |\n",
      "+-------+\n",
      "|error  |\n",
      "|warning|\n",
      "|notice |\n",
      "|info   |\n",
      "|error  |\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = spark.createDataFrame([('error',), ('warning',), ('notice',), ('info',), ('error',)], ['log'])\n",
    "df3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bca62b61-ec83-4089-b96c-39d36bcaa539",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T10:28:52.208019Z",
     "iopub.status.busy": "2024-12-18T10:28:52.207205Z",
     "iopub.status.idle": "2024-12-18T10:28:53.592483Z",
     "shell.execute_reply": "2024-12-18T10:28:53.591200Z",
     "shell.execute_reply.started": "2024-12-18T10:28:52.207997Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 64:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|    log|important|\n",
      "+-------+---------+\n",
      "|  error|        1|\n",
      "|warning|        1|\n",
      "| notice|        0|\n",
      "|   info|        0|\n",
      "|  error|        1|\n",
      "+-------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cust_binarizer = CustomBinarizer(true_val=['error','warning'], inputCol='log', outputCol='important')\n",
    "df3_transformed = cust_binarizer.transform(df3)\n",
    "df3_transformed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5177de-4706-4456-93bc-627b519fde09",
   "metadata": {},
   "source": [
    "### 2.2 Estymatory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672783c9-91d1-42e9-bdf4-390c7921ee3d",
   "metadata": {},
   "source": [
    "**_Listing 5 - StringIndexer oraz OneHotEncoder_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a6bab0f3-60fa-4a58-9f85-adb07ef7465d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T11:25:48.993388Z",
     "iopub.status.busy": "2024-12-18T11:25:48.992718Z",
     "iopub.status.idle": "2024-12-18T11:25:49.838291Z",
     "shell.execute_reply": "2024-12-18T11:25:49.837218Z",
     "shell.execute_reply.started": "2024-12-18T11:25:48.993367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|    log|log_idx|\n",
      "+-------+-------+\n",
      "|  error|    0.0|\n",
      "|warning|    3.0|\n",
      "| notice|    2.0|\n",
      "|   info|    1.0|\n",
      "|  error|    0.0|\n",
      "+-------+-------+\n",
      "\n",
      "+-------+-------+-------------+\n",
      "|    log|log_idx|   onehot_log|\n",
      "+-------+-------+-------------+\n",
      "|  error|    0.0|(3,[0],[1.0])|\n",
      "|warning|    3.0|    (3,[],[])|\n",
      "| notice|    2.0|(3,[2],[1.0])|\n",
      "|   info|    1.0|(3,[1],[1.0])|\n",
      "|  error|    0.0|(3,[0],[1.0])|\n",
      "+-------+-------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SparseVector(3, {0: 1.0})"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "\n",
    "df3 = spark.createDataFrame([('error',), ('warning',), ('notice',), ('info',), ('error',)], ['log'])\n",
    "\n",
    "# StringIndexer to estymator zamienia wartości kategoryczne na numeryczne poprzez podanie indeksu elementu z\n",
    "# utworzonej listy wartości ze wskazanej kolumny\n",
    "stringIndexer = StringIndexer(inputCol=\"log\", outputCol=\"log_idx\")\n",
    "\n",
    "# metoda fit zwraca z kolej obiekt typu transformer\n",
    "model = stringIndexer.fit(df3)\n",
    "\n",
    "\n",
    "transformed_df3 = model.transform(df3)\n",
    "transformed_df3.show()\n",
    "\n",
    "#  teraz możemy użyć kodowania one-hot\n",
    "ohe = OneHotEncoder()\n",
    "# parametry można również ustawiać przez dedykowane metody zamiast przekazywać do konstruktora\n",
    "# w formie kwargs\n",
    "ohe.setInputCols([\"log_idx\"])\n",
    "ohe.setOutputCols([\"onehot_log\"])\n",
    "\n",
    "model = ohe.fit(transformed_df3)\n",
    "transformed_df3 = model.transform(transformed_df3)\n",
    "transformed_df3.show()\n",
    "\n",
    "# typ kolumny to będzie wektor rzadki (SparseVector)\n",
    "# przykładowy wektor: (3,[0],[1.0])\n",
    "# oznacza, że wektor ma długość 3, kolejna wartość to indeksy, na których występują wartości w tym wektorze, a ostatni to faktyczne wartości w tym wektorze\n",
    "transformed_df3.dtypes\n",
    "transformed_df3.select(transformed_df3.onehot_log).head()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8decc2b7-130d-4fdd-a742-da42a996167b",
   "metadata": {},
   "source": [
    "## 3. Przykład całego eksperymentu ML z użyciem Spark MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6507acf1-4bf6-4554-a63a-6d5b903d3749",
   "metadata": {},
   "source": [
    "Poniżej przedstawiono kolejne kroki przykładowego eksperymentu ML z wykorzystaniem biblioteki MLlib. Przedstawiono przykład regresji dla zbioru danych 50_startups (pobrany poniżej). Kolejne kroki obejmują zakodowanie wartości kategorycznych do postaci wektora onehot, a następnie rozbicie go na oddzielne cechy binarne, podział danych na zbiór train i test oraz uruchomienie modelu i jego ewaluacja. Nie zostały tutaj opisane wszystkie szczegóły wzorów metryk, matematycznych szczegółów implementacji algorytmów. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d672ce8c-0dea-46d9-b8ff-58a9336c1391",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T11:39:07.935640Z",
     "iopub.status.busy": "2024-12-18T11:39:07.935270Z",
     "iopub.status.idle": "2024-12-18T11:39:08.410879Z",
     "shell.execute_reply": "2024-12-18T11:39:08.409703Z",
     "shell.execute_reply.started": "2024-12-18T11:39:07.935616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-18 11:39:07--  https://raw.githubusercontent.com/shorya1996/ML_Sklearn/refs/heads/master/Multiple%20Linear%20Regression/50_Startups.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2386 (2.3K) [text/plain]\n",
      "Saving to: ‘50_Startups.csv’\n",
      "\n",
      "50_Startups.csv     100%[===================>]   2.33K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2024-12-18 11:39:08 (2.01 MB/s) - ‘50_Startups.csv’ saved [2386/2386]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pobieramy dane\n",
    "!wget https://raw.githubusercontent.com/shorya1996/ML_Sklearn/refs/heads/master/Multiple%20Linear%20Regression/50_Startups.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6e5b88aa-3f6f-403a-9dff-2ce72c1cbd42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T11:39:12.942019Z",
     "iopub.status.busy": "2024-12-18T11:39:12.940569Z",
     "iopub.status.idle": "2024-12-18T11:39:13.066700Z",
     "shell.execute_reply": "2024-12-18T11:39:13.065653Z",
     "shell.execute_reply.started": "2024-12-18T11:39:12.941991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50_Startups.csv\n"
     ]
    }
   ],
   "source": [
    "!ls | grep *.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0bf747ee-ed3b-4f08-962a-6d6385448126",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T11:39:15.806366Z",
     "iopub.status.busy": "2024-12-18T11:39:15.805697Z",
     "iopub.status.idle": "2024-12-18T11:39:16.108442Z",
     "shell.execute_reply": "2024-12-18T11:39:16.107597Z",
     "shell.execute_reply.started": "2024-12-18T11:39:15.806341Z"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('50_Startups.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "dba059c8-e7e3-4aa2-86a5-c3a34c3a3b3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T11:39:54.414331Z",
     "iopub.status.busy": "2024-12-18T11:39:54.414021Z",
     "iopub.status.idle": "2024-12-18T11:39:54.526671Z",
     "shell.execute_reply": "2024-12-18T11:39:54.525737Z",
     "shell.execute_reply.started": "2024-12-18T11:39:54.414310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+---------------+----------+---------+\n",
      "|R&D Spend|Administration|Marketing Spend|     State|   Profit|\n",
      "+---------+--------------+---------------+----------+---------+\n",
      "| 165349.2|      136897.8|       471784.1|  New York|192261.83|\n",
      "| 162597.7|     151377.59|      443898.53|California|191792.06|\n",
      "|153441.51|     101145.55|      407934.54|   Florida|191050.39|\n",
      "|144372.41|     118671.85|      383199.62|  New York|182901.99|\n",
      "|142107.34|      91391.77|      366168.42|   Florida|166187.94|\n",
      "| 131876.9|      99814.71|      362861.36|  New York|156991.12|\n",
      "|134615.46|     147198.87|      127716.82|California|156122.51|\n",
      "|130298.13|     145530.06|      323876.68|   Florida| 155752.6|\n",
      "|120542.52|     148718.95|      311613.29|  New York|152211.77|\n",
      "|123334.88|     108679.17|      304981.62|California|149759.96|\n",
      "|101913.08|     110594.11|      229160.95|   Florida|146121.95|\n",
      "|100671.96|      91790.61|      249744.55|California| 144259.4|\n",
      "| 93863.75|     127320.38|      249839.44|   Florida|141585.52|\n",
      "| 91992.39|     135495.07|      252664.93|California|134307.35|\n",
      "|119943.24|     156547.42|      256512.92|   Florida|132602.65|\n",
      "|114523.61|     122616.84|      261776.23|  New York|129917.04|\n",
      "| 78013.11|     121597.55|      264346.06|California|126992.93|\n",
      "| 94657.16|     145077.58|      282574.31|  New York|125370.37|\n",
      "| 91749.16|     114175.79|      294919.57|   Florida| 124266.9|\n",
      "|  86419.7|     153514.11|            0.0|  New York|122776.86|\n",
      "+---------+--------------+---------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "bd3ee005-43c1-4a50-b250-9967e2174e40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T11:40:23.966789Z",
     "iopub.status.busy": "2024-12-18T11:40:23.966490Z",
     "iopub.status.idle": "2024-12-18T11:40:23.974533Z",
     "shell.execute_reply": "2024-12-18T11:40:23.973113Z",
     "shell.execute_reply.started": "2024-12-18T11:40:23.966767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- R&D Spend: double (nullable = true)\n",
      " |-- Administration: double (nullable = true)\n",
      " |-- Marketing Spend: double (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Profit: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "681b43a8-cf7d-497c-84fe-7d107c0b31de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T11:40:35.711344Z",
     "iopub.status.busy": "2024-12-18T11:40:35.711050Z",
     "iopub.status.idle": "2024-12-18T11:40:35.877674Z",
     "shell.execute_reply": "2024-12-18T11:40:35.876750Z",
     "shell.execute_reply.started": "2024-12-18T11:40:35.711323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- R&D Spend: double (nullable = true)\n",
      " |-- Administration: double (nullable = true)\n",
      " |-- Marketing Spend: double (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Profit: double (nullable = true)\n",
      " |-- State_numeric: double (nullable = false)\n",
      " |-- State_onehot: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "# zamiana wartości kategorycznych na numeryczne\n",
    "indexer = StringIndexer(inputCol='State', outputCol='State_numeric')\n",
    "indexer_fitted = indexer.fit(df)\n",
    "df_indexed = indexer_fitted.transform(df)\n",
    "\n",
    "# one-hot encoding\n",
    "encoder = OneHotEncoder(inputCols=['State_numeric'], outputCols=['State_onehot'])\n",
    "df_onehot = encoder.fit(df_indexed).transform(df_indexed)\n",
    "\n",
    "df_onehot.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "498cfeb2-df41-4ecd-a694-adb8950dc15f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T11:42:25.966910Z",
     "iopub.status.busy": "2024-12-18T11:42:25.966599Z",
     "iopub.status.idle": "2024-12-18T11:42:26.045909Z",
     "shell.execute_reply": "2024-12-18T11:42:26.044995Z",
     "shell.execute_reply.started": "2024-12-18T11:42:25.966887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+---------------+----------+---------+-------------+-------------+----------+\n",
      "|R&D Spend|Administration|Marketing Spend|     State|   Profit|State_numeric| State_onehot|col_onehot|\n",
      "+---------+--------------+---------------+----------+---------+-------------+-------------+----------+\n",
      "| 165349.2|      136897.8|       471784.1|  New York|192261.83|          1.0|(2,[1],[1.0])|[0.0, 1.0]|\n",
      "| 162597.7|     151377.59|      443898.53|California|191792.06|          0.0|(2,[0],[1.0])|[1.0, 0.0]|\n",
      "|153441.51|     101145.55|      407934.54|   Florida|191050.39|          2.0|    (2,[],[])|[0.0, 0.0]|\n",
      "|144372.41|     118671.85|      383199.62|  New York|182901.99|          1.0|(2,[1],[1.0])|[0.0, 1.0]|\n",
      "|142107.34|      91391.77|      366168.42|   Florida|166187.94|          2.0|    (2,[],[])|[0.0, 0.0]|\n",
      "| 131876.9|      99814.71|      362861.36|  New York|156991.12|          1.0|(2,[1],[1.0])|[0.0, 1.0]|\n",
      "|134615.46|     147198.87|      127716.82|California|156122.51|          0.0|(2,[0],[1.0])|[1.0, 0.0]|\n",
      "|130298.13|     145530.06|      323876.68|   Florida| 155752.6|          2.0|    (2,[],[])|[0.0, 0.0]|\n",
      "|120542.52|     148718.95|      311613.29|  New York|152211.77|          1.0|(2,[1],[1.0])|[0.0, 1.0]|\n",
      "|123334.88|     108679.17|      304981.62|California|149759.96|          0.0|(2,[0],[1.0])|[1.0, 0.0]|\n",
      "|101913.08|     110594.11|      229160.95|   Florida|146121.95|          2.0|    (2,[],[])|[0.0, 0.0]|\n",
      "|100671.96|      91790.61|      249744.55|California| 144259.4|          0.0|(2,[0],[1.0])|[1.0, 0.0]|\n",
      "| 93863.75|     127320.38|      249839.44|   Florida|141585.52|          2.0|    (2,[],[])|[0.0, 0.0]|\n",
      "| 91992.39|     135495.07|      252664.93|California|134307.35|          0.0|(2,[0],[1.0])|[1.0, 0.0]|\n",
      "|119943.24|     156547.42|      256512.92|   Florida|132602.65|          2.0|    (2,[],[])|[0.0, 0.0]|\n",
      "|114523.61|     122616.84|      261776.23|  New York|129917.04|          1.0|(2,[1],[1.0])|[0.0, 1.0]|\n",
      "| 78013.11|     121597.55|      264346.06|California|126992.93|          0.0|(2,[0],[1.0])|[1.0, 0.0]|\n",
      "| 94657.16|     145077.58|      282574.31|  New York|125370.37|          1.0|(2,[1],[1.0])|[0.0, 1.0]|\n",
      "| 91749.16|     114175.79|      294919.57|   Florida| 124266.9|          2.0|    (2,[],[])|[0.0, 0.0]|\n",
      "|  86419.7|     153514.11|            0.0|  New York|122776.86|          1.0|(2,[1],[1.0])|[0.0, 1.0]|\n",
      "+---------+--------------+---------------+----------+---------+-------------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# ta funkcja zamienia wektor rzadki w tablicę\n",
    "df_col_onehot = df_onehot.select('*', vector_to_array('state_onehot').alias('col_onehot'))\n",
    "df_col_onehot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "21b91bec-8b0b-42ce-ab56-1a73db1e6911",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T11:42:49.151869Z",
     "iopub.status.busy": "2024-12-18T11:42:49.151490Z",
     "iopub.status.idle": "2024-12-18T11:42:49.344889Z",
     "shell.execute_reply": "2024-12-18T11:42:49.343652Z",
     "shell.execute_reply.started": "2024-12-18T11:42:49.151849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+---------------+----------+---------+-------------+-------------+----------+----------+--------+\n",
      "|R&D Spend|Administration|Marketing Spend|     State|   Profit|State_numeric| State_onehot|col_onehot|California|New York|\n",
      "+---------+--------------+---------------+----------+---------+-------------+-------------+----------+----------+--------+\n",
      "| 165349.2|      136897.8|       471784.1|  New York|192261.83|          1.0|(2,[1],[1.0])|[0.0, 1.0]|       0.0|     1.0|\n",
      "| 162597.7|     151377.59|      443898.53|California|191792.06|          0.0|(2,[0],[1.0])|[1.0, 0.0]|       1.0|     0.0|\n",
      "|153441.51|     101145.55|      407934.54|   Florida|191050.39|          2.0|    (2,[],[])|[0.0, 0.0]|       0.0|     0.0|\n",
      "|144372.41|     118671.85|      383199.62|  New York|182901.99|          1.0|(2,[1],[1.0])|[0.0, 1.0]|       0.0|     1.0|\n",
      "|142107.34|      91391.77|      366168.42|   Florida|166187.94|          2.0|    (2,[],[])|[0.0, 0.0]|       0.0|     0.0|\n",
      "| 131876.9|      99814.71|      362861.36|  New York|156991.12|          1.0|(2,[1],[1.0])|[0.0, 1.0]|       0.0|     1.0|\n",
      "|134615.46|     147198.87|      127716.82|California|156122.51|          0.0|(2,[0],[1.0])|[1.0, 0.0]|       1.0|     0.0|\n",
      "|130298.13|     145530.06|      323876.68|   Florida| 155752.6|          2.0|    (2,[],[])|[0.0, 0.0]|       0.0|     0.0|\n",
      "|120542.52|     148718.95|      311613.29|  New York|152211.77|          1.0|(2,[1],[1.0])|[0.0, 1.0]|       0.0|     1.0|\n",
      "|123334.88|     108679.17|      304981.62|California|149759.96|          0.0|(2,[0],[1.0])|[1.0, 0.0]|       1.0|     0.0|\n",
      "|101913.08|     110594.11|      229160.95|   Florida|146121.95|          2.0|    (2,[],[])|[0.0, 0.0]|       0.0|     0.0|\n",
      "|100671.96|      91790.61|      249744.55|California| 144259.4|          0.0|(2,[0],[1.0])|[1.0, 0.0]|       1.0|     0.0|\n",
      "| 93863.75|     127320.38|      249839.44|   Florida|141585.52|          2.0|    (2,[],[])|[0.0, 0.0]|       0.0|     0.0|\n",
      "| 91992.39|     135495.07|      252664.93|California|134307.35|          0.0|(2,[0],[1.0])|[1.0, 0.0]|       1.0|     0.0|\n",
      "|119943.24|     156547.42|      256512.92|   Florida|132602.65|          2.0|    (2,[],[])|[0.0, 0.0]|       0.0|     0.0|\n",
      "|114523.61|     122616.84|      261776.23|  New York|129917.04|          1.0|(2,[1],[1.0])|[0.0, 1.0]|       0.0|     1.0|\n",
      "| 78013.11|     121597.55|      264346.06|California|126992.93|          0.0|(2,[0],[1.0])|[1.0, 0.0]|       1.0|     0.0|\n",
      "| 94657.16|     145077.58|      282574.31|  New York|125370.37|          1.0|(2,[1],[1.0])|[0.0, 1.0]|       0.0|     1.0|\n",
      "| 91749.16|     114175.79|      294919.57|   Florida| 124266.9|          2.0|    (2,[],[])|[0.0, 0.0]|       0.0|     0.0|\n",
      "|  86419.7|     153514.11|            0.0|  New York|122776.86|          1.0|(2,[1],[1.0])|[0.0, 1.0]|       0.0|     1.0|\n",
      "+---------+--------------+---------------+----------+---------+-------------+-------------+----------+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rozbijamy wartości z kolumny onehot na tzw. dummy data czyli nowe kolumny dla każdej cechy z wartością = 1\n",
    "# jeżeli dana cecha w tym wektorze występuje, 0 w przeciwnym wypadku\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "num_categories = len(df_col_onehot.first()['col_onehot']) \n",
    "cols_expanded = [(f.col('col_onehot')[i].alias(f'{indexer_fitted.labels[i]}')) for i in range(num_categories)]\n",
    "df_cols_onehot = df_col_onehot.select('*', *cols_expanded)\n",
    "df_cols_onehot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "34b1c451-6dd5-48db-9b05-a1e3071b86f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T11:43:36.494744Z",
     "iopub.status.busy": "2024-12-18T11:43:36.494024Z",
     "iopub.status.idle": "2024-12-18T11:43:36.513909Z",
     "shell.execute_reply": "2024-12-18T11:43:36.513011Z",
     "shell.execute_reply.started": "2024-12-18T11:43:36.494720Z"
    }
   },
   "outputs": [],
   "source": [
    "# dobieramy tylko wybrane kolumny do ostatecznej ramki danych\n",
    "df_final = df_cols_onehot.select(\"R&D Spend\", \"Administration\", \"Marketing Spend\", \"California\", \"New York\", \"profit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "22bc561b-5c0b-4fc8-9e8e-b1f2ed892c0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T11:43:45.421357Z",
     "iopub.status.busy": "2024-12-18T11:43:45.421032Z",
     "iopub.status.idle": "2024-12-18T11:43:45.512538Z",
     "shell.execute_reply": "2024-12-18T11:43:45.511632Z",
     "shell.execute_reply.started": "2024-12-18T11:43:45.421336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+---------------+----------+--------+---------+\n",
      "|R&D Spend|Administration|Marketing Spend|California|New York|   profit|\n",
      "+---------+--------------+---------------+----------+--------+---------+\n",
      "| 165349.2|      136897.8|       471784.1|       0.0|     1.0|192261.83|\n",
      "| 162597.7|     151377.59|      443898.53|       1.0|     0.0|191792.06|\n",
      "|153441.51|     101145.55|      407934.54|       0.0|     0.0|191050.39|\n",
      "|144372.41|     118671.85|      383199.62|       0.0|     1.0|182901.99|\n",
      "|142107.34|      91391.77|      366168.42|       0.0|     0.0|166187.94|\n",
      "| 131876.9|      99814.71|      362861.36|       0.0|     1.0|156991.12|\n",
      "|134615.46|     147198.87|      127716.82|       1.0|     0.0|156122.51|\n",
      "|130298.13|     145530.06|      323876.68|       0.0|     0.0| 155752.6|\n",
      "|120542.52|     148718.95|      311613.29|       0.0|     1.0|152211.77|\n",
      "|123334.88|     108679.17|      304981.62|       1.0|     0.0|149759.96|\n",
      "+---------+--------------+---------------+----------+--------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ee57a61a-88bc-431f-8f44-fa5cbd2a57f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T11:44:15.551156Z",
     "iopub.status.busy": "2024-12-18T11:44:15.550568Z",
     "iopub.status.idle": "2024-12-18T11:44:15.691153Z",
     "shell.execute_reply": "2024-12-18T11:44:15.690175Z",
     "shell.execute_reply.started": "2024-12-18T11:44:15.551132Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|            features|   profit|\n",
      "+--------------------+---------+\n",
      "|[165349.2,136897....|192261.83|\n",
      "|[162597.7,151377....|191792.06|\n",
      "|[153441.51,101145...|191050.39|\n",
      "|[144372.41,118671...|182901.99|\n",
      "|[142107.34,91391....|166187.94|\n",
      "|[131876.9,99814.7...|156991.12|\n",
      "|[134615.46,147198...|156122.51|\n",
      "|[130298.13,145530...| 155752.6|\n",
      "|[120542.52,148718...|152211.77|\n",
      "|[123334.88,108679...|149759.96|\n",
      "|[101913.08,110594...|146121.95|\n",
      "|[100671.96,91790....| 144259.4|\n",
      "|[93863.75,127320....|141585.52|\n",
      "|[91992.39,135495....|134307.35|\n",
      "|[119943.24,156547...|132602.65|\n",
      "|[114523.61,122616...|129917.04|\n",
      "|[78013.11,121597....|126992.93|\n",
      "|[94657.16,145077....|125370.37|\n",
      "|[91749.16,114175....| 124266.9|\n",
      "|[86419.7,153514.1...|122776.86|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# to transformer, który składa zadane cechy (kolumny) w jeden wektor cech\n",
    "assembler = VectorAssembler(inputCols=df_final.columns[:-1],outputCol='features')\n",
    "\n",
    "data_set = assembler.transform(df_final)\n",
    "data_set = data_set.select(['features','profit'])\n",
    "data_set.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6255e9c4-07c4-41fc-af3a-28c51b433d43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T11:48:03.089122Z",
     "iopub.status.busy": "2024-12-18T11:48:03.088320Z",
     "iopub.status.idle": "2024-12-18T11:48:03.495314Z",
     "shell.execute_reply": "2024-12-18T11:48:03.494392Z",
     "shell.execute_reply.started": "2024-12-18T11:48:03.089096Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 9432.417289656889\n",
      "R2: 0.9523442645657374\n",
      "MSE: 88970495.92621821\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# podział danych train test\n",
    "train_data,test_data = data_set.randomSplit([0.8,0.2])\n",
    "\n",
    "# inicjalizacja modelu regresji\n",
    "lr = LinearRegression(featuresCol=\"features\",labelCol='profit', regParam=0.1)\n",
    "lrModel = lr.fit(train_data)\n",
    "test_stats = lrModel.evaluate(test_data)\n",
    "\n",
    "# wypisanie wyników\n",
    "print(f\"RMSE: {test_stats.rootMeanSquaredError}\")\n",
    "print(f\"R2: {test_stats.r2}\")\n",
    "print(f\"MSE: {test_stats.meanSquaredError}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99efc6f6-e5c4-4361-8c4c-98f2810c0064",
   "metadata": {},
   "source": [
    "### Zadania"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3788671-6b56-42c6-a0d1-8501bf6d69ef",
   "metadata": {},
   "source": [
    "**Zadanie 1**\n",
    "\n",
    "Bazując na przykładach opisanych w tym labie przygotuj podobny eksperyment regresji z wykorzystaniem poniższego zbioru danych:\n",
    "* https://www.kaggle.com/datasets/amrahhasanov23/otodom-pl-flat-prices-in-poland\n",
    "\n",
    "W eksperymencie uwzględnij:\n",
    "* konieczność (lub jej brak) kodowania danych kategorycznych na numeryczne,\n",
    "* wybór tylko podzbioru cech,\n",
    "* przeprowadź kilka eksperymentów zapisując wyniki z różnym doborem cech i/lub ich kodowaniem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
