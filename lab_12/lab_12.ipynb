{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f48899-42e6-4375-ab17-aa6787fc6b33",
   "metadata": {},
   "source": [
    "# Lab 12. Spark Structured Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7168d4d2-7802-41ba-8199-5bcdbd256e24",
   "metadata": {},
   "source": [
    "Structured Streaming to skalowalny i odporny na błędy silnik przetwarzania strumieniowego oparty na silniku Spark SQL. Jest to kolejna generacja rozwiązania przetwarzania strumieniowego z wykorzystaniem środowiska Spark, które zostało przedstawione wraz z wersją 2.0 środowiska Spark i jest następcą Spark Streaming, które to nie otrzymuje już żadnych nowych update-ów (możesz poczytać o tej starszej wersji [**tu**](https://spark.apache.org/docs/3.5.1/streaming-programming-guide.html)).\n",
    "\n",
    "Structured Streaming pozwala na pracę z interfejsami API Dataset oraz DataFrame, które zostały już przedstawione wcześniej, w taki sam sposób jak przy przetwarzaniu wsadowym. Ponownie mamy do dyspozycji interfejsy dla języka Scala, Java, Python oraz R, dzięki którym można tworzyć agregacje strumieniowe, okna zdarzeń w czasie, połączenia stream-to-batch i inne. Obliczenia są wykonywane na tym samym zoptymalizowanym silniku Spark SQL. Structured Streaming zapewnia szybkie, skalowalne, odporne na błędy, kompleksowe przetwarzanie strumieniowe dokładnie jeden raz (ang. exactly-once), bez konieczności zastanawiania się przez użytkownika nad sposobem przesyłania strumieniowego.\n",
    "\n",
    "Wewnętrznie, domyślnie, zapytania Structured Streaming są przetwarzane przy użyciu silnika przetwarzania mikropartii, który przetwarza strumienie danych jako serię małych zadań wsadowych, osiągając w ten sposób opóźnienia end-to-end tak niskie, jak 100 milisekund i gwarancje tolerancji błędów exactly-once. Jednak od wersji Spark 2.3 wprowadzono nowy tryb przetwarzania o niskim opóźnieniu o nazwie Continuous Processing, który może osiągnąć opóźnienia end-to-end tak niskie jak 1 milisekunda z gwarancjami at-least-once. Bez zmiany operacji Dataset/DataFrame w zapytaniach, można wybrać tryb w oparciu o wymagania aplikacji."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21385615-7598-4516-a653-78caf09f775a",
   "metadata": {},
   "source": [
    "# 1. Przykład użycia Structured Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7f8bfa-02b6-4c6d-83a6-81d99c78afb9",
   "metadata": {},
   "source": [
    "## 1.1 Konfiguracja wstępna.\n",
    "\n",
    "W przykładach wykorzystywane jest narzędzie `netcat`, które nie jest aktualnie zainstalowane w naszym obrazie dockerowym Ubuntu.\n",
    "\n",
    "**Krok 1.**\n",
    "\n",
    "Dodajemy do pliku `Dockerfile` następujące linie przed linią `USER spark`:\n",
    "```bash\n",
    "\n",
    "...\n",
    "\n",
    "RUN apt-get update; \\\n",
    "\tapt-get install -y netcat;\n",
    "\n",
    "# poniższa linia już w pliku się znajduje\n",
    "USER spark\n",
    "```\n",
    "\n",
    "**Krok 2**\n",
    "\n",
    "Budujemy obraz ponownie co spowoduje dodanie kolejnej warstwy do istniejącego obrazu, ale zapiszemy go pod nową nazwą, tak na wszelki wypadek, gdyby coś poszło nie tak.\n",
    "\n",
    "Uruchamiamy terminal w systemie hosta i ustawiamy ścieżkę na folder, który zawiera zmodyfikowany w kroku 1 plik Dockerfile. Następnie uruchamiamy polecenie:\n",
    "```bash\n",
    "docker build -t nowa_nazwa_obrazu:tag .\n",
    "```\n",
    "\n",
    "**Krok 3**\n",
    "\n",
    "Teraz należy zmodyfikować plik `docker-compose.yml` i wskazać nowę nazwę obrazu, a którego obraz ma być tworzony, np.\n",
    "\n",
    "```yml\n",
    "# stara wersja\n",
    "services:\n",
    "  spark-master-3.5.3:\n",
    "    image: spark-3.5.3:v3\n",
    "    ...\n",
    "\n",
    "# nowa wersja\n",
    "services:\n",
    "  spark-master-3.5.3:\n",
    "    image: spark-3.5.3:v4 (tu nazwa nowego obrazu)\n",
    "    ...\n",
    "```\n",
    "\n",
    "**Krok 4**\n",
    "\n",
    "Uruchamiamy kontener poleceniem `docker-compose up` i sprawdzamy w konsoli kontenera czy polecenie `nc` jest już dostępne."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c71e9a-9d01-4ba2-8a3d-251efecb8ea8",
   "metadata": {},
   "source": [
    "## 1.2 Kod i sposób uruchomienia prostego przykładu przetwarzania strumieniowego"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075603c6-eca5-4db5-8f3a-4b11f5aed69f",
   "metadata": {},
   "source": [
    "**UWAGA !**\n",
    "\n",
    "Jeżeli nie chcesz blokować notebooka (ten kod spowoduje uruchomienie przetwarzania w tle, ale będzie ono działało póki nie zostanie przerwane) powinno się uruchomić przykład w oddzielnym oknie terminala wewnątrz kontenera zgodnie z przykładem podanym po ostatnim fragmencie skryptu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db1b62c-aec4-4f31-9efd-fbc8f331cf29",
   "metadata": {},
   "source": [
    "Poniżej zaprezentowany został przykład działania Structured Streaming zaczerpnięty z oficjalnej dokumentacji znajdującej się w poradniku: https://spark.apache.org/docs/3.5.1/structured-streaming-programming-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1c348e-a37a-4b3e-8c8d-5689ae882265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicjalizacja sesji Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StructuredStreamingExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# domyślnie log level to INFO, które dość mocno spamuje okno konsoli\n",
    "# możemy więc zmienić na nasze potrzeby do poziomu WARN\n",
    "spark.sparkContext.setLogLevel('WARN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928d3e95-6fc3-4bf1-af10-1ed39d656417",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55fe9e2-c0cc-4fbf-86bf-f17d87f6a5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tworzymy DataFrame reprezentujący strumień linii z połączenia do localhost:9999\n",
    "# w dokumentacji znajdziemy też informację, że format socket jest wykorzystywany tylko\n",
    "# do testowania\n",
    "\n",
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# dzielenie linii na słowa\n",
    "words = lines.select(\n",
    "   explode(\n",
    "       split(lines.value, \" \")\n",
    "   ).alias(\"word\")\n",
    ")\n",
    "\n",
    "# uruchomienie mechanizmu dzielenia tekstu na słowa\n",
    "wordCounts = words.groupBy(\"word\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7432ab-0a17-477f-ab5d-bc25bb42cba3",
   "metadata": {},
   "source": [
    "Teraz pozostaje włączenie mechanizmu pobierania danych i zliczania słów. Włączamy wyświetlanie całkowitej liczby zliczeń (ustawiane przez `outputMode(\"complete\")`) na konsolę za każdym razem jak ta wartość zostanie zaktualizowana i uruchamiamy obliczenia strumieniowe poprzez metodę `start()`\n",
    "\n",
    "Metoda `awaitTermination()` powoduje wykonanie tej akcji tylko wtedy, gdy obliczenia są już zakończone, a nie w ich trakcie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565b0e03-42be-4232-b0eb-88dcc9faaeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uruchomienie zapytania, które będzie wyświetlało liczbę słów na konsolę\n",
    "query = wordCounts \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156c837c-dd93-4835-8089-803e470db35a",
   "metadata": {},
   "source": [
    "**UWAGA !**\n",
    "\n",
    "Aby przetwarzanie strumieniowe zadziałało poprawnie musimy najpierw uruchomić poniższe polecenie, które uruchomi narzędzie `netcat`, które będzie nasłuchiwać na porcie 9999 póki nie zakończymy tego procesu. Wykonujemy to w oddzielnym oknie terminala naszego kontenera.\n",
    "\n",
    "```bash\n",
    "nc -lk 9999\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea6fd5b-2149-4f0d-a923-831e3fe388d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# możemy też zebrać całość w jeden skrypt i uruchomić z poziomu konsoli, co nie będzie blokowało możliwości\n",
    "# wykonywania innych komórek notatnika, ale rekomendowane jest wywołanie komendy w terminalu\n",
    "# wewnątrz kontenera ze względu na znaczną objętość outputu tej komórki oraz naturę tej \"aplikacji\"\n",
    "# UPDATE: możemy jednak zmienić poziom logów na WARN co poprawia trochę sytuację w notebooku\n",
    "# patrz plik źródłowy: structured_streaming_example.py\n",
    "! ../../bin/spark-submit ./structured_streaming_example.py localhost 9999\n",
    "\n",
    "# prawdopodobna postać polecenia w terminalu kontenera\n",
    "/opt/spark/bin/spark-submit ./lab_12/structured_streaming_example.py localhost 9999"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0911be20-9e1c-43fb-9e09-db12ab344da5",
   "metadata": {},
   "source": [
    "## 1.3 Jak to działa?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7632be0c-b089-422a-a4ce-b74ad75930d5",
   "metadata": {},
   "source": [
    "Poniżej przedstawiona zostanie koncepcja działania przetwarzania strumieniowego z użyciem Spark Structored Streaming oparta na oficjalnej dokumentacji."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af5ac9d-d865-44df-9a9a-3919eadd0134",
   "metadata": {},
   "source": [
    "Utworzony w powyższym zadaniu obiekt DataFrame oraz kolejne jego części, które są przetwarzane strumieniowo jako mikro paczki (ang. micro-batch) i stopniowo dołączane do ramki w kolejności napływania danych w strumieniu. Wizualizacja została przedstawiona na poniższej grafice.\n",
    "\n",
    "![Streamin processing](./images/structured-streaming-stream-as-a-table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c0e1ab-89e1-4716-9b99-eb4284a90c08",
   "metadata": {},
   "source": [
    "W naszym przypadku na wyjściu otrzymujemy stan 'completed', ale możliwe jest określenie innych strategii zwracania danych z ramki. Poniżej opis za dokumentacją:\n",
    "\n",
    "* `Complete Mode` - cała zaktualizowana tabela wyników zostanie zapisana w pamięci zewnętrznej. Decyzja o sposobie obsługi zapisu całej tabeli należy do konektora pamięci masowej.\n",
    "\n",
    "* `Append Mode` - tylko nowe wiersze dodane do tabeli wyników od ostatniego wyzwalacza zostaną zapisane w zewnętrznej pamięci masowej. Ma to zastosowanie tylko w przypadku zapytań, w których nie oczekuje się zmiany istniejących wierszy w tabeli wyników.\n",
    "\n",
    "* `Update Mode` - tylko wiersze, które zostały zaktualizowane w tabeli wyników od ostatniego wyzwalacza zostaną zapisane w pamięci zewnętrznej (dostępne od wersji Spark 2.1.1). Należy pamiętać, że ten tryb różni się od trybu `Complete`, ponieważ wyświetla tylko wiersze, które uległy zmianie od ostatniego wyzwalacza. Jeśli zapytanie nie zawiera agregacji, będzie to równoważne z trybem `Append`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b265a6-13d2-4aff-a98a-3eb72334f01b",
   "metadata": {},
   "source": [
    "Nasz przykład wykorzystuje strumień oparty o gniazdo (ang. socket), który rekomendowany jest tylko do testów. Poniżej wymienione zostały inne możliwe wbudowane źródła danych strumieniowych dla obiektów Dataset/DataFrame i Structured Streaming:\n",
    "\n",
    "* `źródło plikowe (file source)` - odczytuje pliki ze wskazanego katalogu jako dane strumieniowe. Pliki są przetwarzane w kolejności zgodnej z ich datą modyfikacji. Można tę kolejność odwrócić w zależności od potrzeb (parametr latestFirst). Wspierane formaty plików to: pliki tekstowe, CSV, JSON, ORC, Parquet.\n",
    "\n",
    "* `źródło Apache Kafka` - Odczytywanie danych ze strumienia Apache Kafka. Więcej informacji w poradniku integracji Spark i Kafka dostępnym [tu](https://spark.apache.org/docs/3.5.1/structured-streaming-kafka-integration.html).\n",
    "\n",
    "* `źródło gniazda (socket source)`, tylko do testowania - odczytuje tekst w formacie UTF8 ze źródła w postaci gniazda, a serwer nasłuchujący znajduje się na driverze klastra Spark. Nie zapewnia mechanizmu gwarancji and-to-end fault-tolerance.\n",
    "\n",
    "* `żródło rate`, tylko testowanie - generuje dane z określoną liczbą wierszy na sekundę, każdy wiersz wyjściowy zawiera znacznik czasu i wartość. Znacznik czasu jest typem Timestamp zawierającym czas wysłania wiadomości, a value jest typem Long zawierającym liczbę wiadomości, zaczynając od 0 jako pierwszy wiersz. To źródło jest przeznaczone do testowania i analizy wydajności.\n",
    "\n",
    "* `źródło Rate Per Micro-Batch`, tylko testowanie - generuje dane z określoną liczbą wierszy na sekundę, każdy wiersz wyjściowy zawiera znacznik czasu i wartość. Znacznik czasu jest typem Timestamp zawierającym czas wysłania wiadomości, a value jest typem Long zawierającym liczbę wiadomości, zaczynając od 0 jako pierwszy wiersz. W przeciwieństwie do źródła danych rate, to źródło danych zapewnia spójny zestaw wierszy wejściowych na mikropartię niezależnie od wykonania zapytania (konfiguracja wyzwalacza, opóźnienie zapytania itp.), powiedzmy, partia 0 wygeneruje 0\\~999, a partia 1 wygeneruje 1000\\~1999 itd. To samo dotyczy wygenerowanego czasu. To źródło jest przeznaczone do testowania i analizy wydajności."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e07f789-5e62-4083-baaa-c2d230de0479",
   "metadata": {},
   "source": [
    "## 1.4 Inne przykłady ramek strumieniowych."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21e425a-b978-49b8-a97d-712efd5f6883",
   "metadata": {},
   "source": [
    "Ponownie przykład ze strumieniem z gniazda. Jeżeli w tle nadal działa narzędzie netcat i do strumienia zostały wysłane jakieś dane, możemy je odczytać lub obserwować tutaj aktualizację ramki jeżeli do strumienia trafią nowe dane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e44cf4-4102-4e62-86fb-062c533419fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do uruchomienia tych przykładów potrzebne jest wcześniejsze stworzenie sesji Spark\n",
    "socketDF = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431fb3f5-c2f4-4389-b3e9-037f51e3b625",
   "metadata": {},
   "outputs": [],
   "source": [
    "socketDF.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3a90f7-4659-4a31-b9bd-1600f218af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "socketDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba08add-1965-4dc9-93b4-8168278c8e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the streaming DataFrame to a table\n",
    "socketDF.writeStream \\\n",
    "    .option(\"checkpointLocation\", \"./data/socstream/\") \\\n",
    "    .toTable(\"socketTable\")\n",
    "\n",
    "# Check the table result\n",
    "spark.read.table(\"socketTable\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035c9f11-f492-4e51-9e47-ac0dd8c5bc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# po wysłaniu kilku wartości w terminalu z uruchomionym netcatem możemy ponownie odczytać\n",
    "# wartości z tabeli utworzonej ze źródła strumieniowego\n",
    "spark.read.table(\"socketTable\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d30fc6-9e3a-4962-9331-77280ae61470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# po tej operacji są tworzone również pliki tymczasowe ukryte i zatrzymanie oraz wznowienie tego\n",
    "# strumienia nie jest możliwe, jeżeli nie posprzątamy w tym folderze\n",
    "# poniższe polecenie pomoże nam to zrobić\n",
    "!rm -rf ./data/socstream/*\n",
    "!rm -rf ./spark-warehouse/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7145aadb-e88c-46be-b74e-a2b6fd594a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jeżeli uruchomiliśmy wcześniej kontekst Sparka, możemy go zakończyć\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
