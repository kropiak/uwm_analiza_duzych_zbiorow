{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a542fab-d83d-4bc4-91f3-088466b16848",
   "metadata": {},
   "source": [
    "# lab_02 - Wprowadzenie do biblioteki Dask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7779f71b-3774-457c-a973-ac94370ef481",
   "metadata": {},
   "source": [
    "## 1. Krótki opis biblioteki Dask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d67a029-47f9-41e3-bebc-be352cd9d999",
   "metadata": {},
   "source": [
    "Dask jest zbiorem rozwiązań, który pozwala na zrównoleglenie obliczeń w języku Python oraz przetwarzanie zbiorów większych niż dostępna pamięć RAM. Ekosystem Dask składa się z wielu elementów. \n",
    "\n",
    "**Kolekcje Dask (Dask collections):**\n",
    "* Kolekcje wysokiego poziomu\n",
    "  * Dask Dataframe\n",
    "  * Dask Array\n",
    "  * Dask Bag\n",
    "* Niskopoziomowe kolekcje\n",
    "  * Dask Dalayed & Futures\n",
    "\n",
    "**Klaster Dask**\n",
    "\n",
    "**Inne elementy ekosystemu Dask:**\n",
    "\n",
    "* Dask-ML (parallel scikit-learn-style API)\n",
    "* Dask-image\n",
    "* Dask-cuDF\n",
    "* Dask-sql\n",
    "* Dask-snowflake\n",
    "* Dask-mongo\n",
    "* Dask-bigquery\n",
    "\n",
    "\n",
    "![dask overwiev](dask-overview.svg)\n",
    "\n",
    "\n",
    "_źródło: dask.org_\n",
    "\n",
    "**Instalacja Dask**\n",
    "\n",
    "Oficjalna dokumentacja: https://docs.dask.org/en/stable/install.html\n",
    "\n",
    "Instalacja podstawowej biblioteki Dask jest bardzo prosta:\n",
    "\n",
    "```bash\n",
    "python -m pip install dask\n",
    "```\n",
    "\n",
    "Dask posiada jednak duży zbiór opcjonalnych modułów, które mogą się przydać w zależności od potrzeb i zakresu serwisów oraz źródeł danych, z których chcemy skorzystać. Można więc zainstalować również wszystko bez zwracania uwagi na szczegóły:\n",
    "\n",
    "```bash\n",
    "python -m pip install dask[complete]\n",
    "```\n",
    "\n",
    "Szczegóły instalowanych zależności i ich zastosowanie znajduje się w oficjalnej dokumentacji."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50db0795-41ea-4179-9b27-bb01b4a02186",
   "metadata": {},
   "source": [
    "## 2. Dask DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ea4f61-5490-47b6-8279-119f047ecc94",
   "metadata": {},
   "source": [
    "> **Oficjalna dokumentacja:** https://docs.dask.org/en/latest/dataframe.html\n",
    "\n",
    "Ramka Dask jest bardzo zbliżona do ramki pandas w kontekście obsługi z poziomu programisty. Główne różnice są ukryte w sposobie jej przechowywania i wykonywania obliczeń. Obliczenia odbywają się w sposób rozproszony i zrównoleglony.\n",
    "Dask DataFrame składa się z wielu ramek pandas, odpowiednio podzielonych, aby można było zarówno dane jak i obliczenia wykonać na wielu węzłach (ang. worker) jednocześnie.\n",
    "\n",
    "\n",
    "![dask_dataframe](dask-dataframe.svg)\n",
    "_źródło: dask.org_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad65230-3e8e-4adc-9754-95b6f53e6826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obserwuj komunikaty, aby zainstalować ewentualnie brakujące komponenty\n",
    "# w komórce wynikowej notebooka pojawi się link do dashboardu, w którym można obserwować aktualne\n",
    "# zadania i obciążenie klastra jak i zadania wykonane\n",
    "# UWAGA! Po doinstalowaniu biblioteki bokeh może być konieczne zrestartowanie jądra Pythona i całego notebooka\n",
    "# aby dashboard poprawnie działał. Każde kolejne uruchomienie poniższego kodu bez restartu jądra, utworzy nową\n",
    "# instancję klienta\n",
    "\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=4)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c3a0df-0ef4-4cbf-867c-d4f9da7e9783",
   "metadata": {},
   "source": [
    "Powyższy kod uruchamia instancję lokalnego klastra, który określna 4 workery, między które będzie rozdzielana praca do wykonania. Konfiguracja tego klastra jest tutaj zredukowana do minimum, gdyż bardziej szczegółowo zostanie to omówione na kolejnych zajęciach. Warto tu wiedzieć, że możemy określić czy zadania będą uruchamiane w ramach nowych procesów, czy wątków. Możemy również wskazać ile wątków na worker przypadnie jeżeli na wątki się zdecydujemy. Możemy również określić limit pamięci na worker, co jest dobrym pomysłem gdyż pozostawienie tego parametru z wartością domyślną rozdzieli pamięć po równo na każdy z workerów. Warto pozostawić systemowi hosta trochę zasobów (można sprawdzić wcześniej ile zasobów zużywa system \"na jałowym biegu\").\n",
    "\n",
    "Szczegóły API dla lokalnego klastra znajdziemy tu: https://distributed.dask.org/en/latest/api.html#distributed.LocalCluster\n",
    "\n",
    "W celu pogłębienia wiedzy o niskopoziomowych niuansach działania Pythona, a szczególnie w kontekście współbieżności (lub jej braku) zachęcam do oglądnięcia wystąpienia Marcina Kawki pod tytułem \"Wątki i procesy, czyli o zrównoleglaniu programów w Pythonie\" na Pytech Summit 2022. Film dostępny pod adresem: https://www.youtube.com/watch?v=kRy_UwUhBpo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6359173f-6e50-451c-aa1f-fde31b11a1e5",
   "metadata": {},
   "source": [
    "Więcej szczegółów oraz film z wprowadzeniem do dashboardu Dask znajdziesz pod adresem: https://docs.dask.org/en/stable/dashboard.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac93a90f-34d8-4f10-ab34-ff7b950cd759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7e7bf5-8755-4a09-a5cd-cc1b29718b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "\n",
    "# dane, które są przykładowe i nie są dołączone do notebooka\n",
    "# to dane, które ze zbioru zamowienia.csv zostały sztucznie zwielokrotnione, podzielone i zapisane w kilku\n",
    "# plikach .csv\n",
    "ddf = dd.read_csv(os.path.join(\"..\", \"lab_01\", \"data\", \"*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9f075f-66d6-4de6-9c3f-3738f357b945",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507046d6-1712-48e7-a7bf-56c507f81c2f",
   "metadata": {},
   "source": [
    "Dask nie wykonuje operacji w sposób, do którego możemy być przyzwyczajeni. Tutaj mamy do czynienia z mechnizmem leniwym (ang. lazy), a tym przypadku _lazy loading_, gdzie dask sprawdził ile plików jest do wczytania, podzielił pracę na 20 części oraz na podstawie kilku pierwszych linii z pierwszego pliku ustawił nagłówki kolumn i założył typy danych (które w momencie wystąpienia niespójności w kolumnach mogą ulec zmianie)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704ea3f7-1f0f-4632-b383-27c94c8742b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee94f39-4c9c-4784-a866-31b998666728",
   "metadata": {},
   "source": [
    "Aby faktycznie wywołać obliczenia musimy wywołać metodę `.compute()` lub jedną z metod, która ją wywołuje niejawnie np. `len`, `head`, `tail`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fe8583-1284-4891-a894-f3348bae42ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# przed uruchomieniem bieżącej komórki ustaw kartę przeglądarki z dashboardem tak, aby byłą również widoczna\n",
    "# będzie można śledzić pracę klastra\n",
    "\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606cb98c-1cdd-479c-9850-a09f07ef23d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# podobnie jak na zajęciach poprzednich możemy sprawdzić ilość pamięci niezbędnej do przechowania danych ramki\n",
    "# pamiętajmy o mechanizmie leniwego wywołania, który przygotuje graf obliczeń, ale ich faktycznie jeszcze nie wykona\n",
    "ddf.memory_usage(deep=True).visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d954a3-23f2-4812-b314-7d82b5cb20a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ponownie można śledzić zadania w dashboardzie\n",
    "ddf.memory_usage(deep=True).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f954ce3-c359-415f-ae2d-8e56a9cecc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "utarg_sum = ddf.groupby(['Kraj']).Utarg.sum()\n",
    "utarg_sum.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77eb165-d297-4bb1-8e42-8799ce6f8930",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "utarg_sum.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe33b8e-c2c5-4cf8-8f62-0226551409c5",
   "metadata": {},
   "source": [
    "Obliczenia odbywają się w sposób rozproszony i w zależności od wielkości zbioru danych oraz ilości workerów, może zająć więcej czasu niż wykonanie obliczeń lokalnie, jeżeli wybrana porcja danych zmieściłaby się pamięci operacyjnej. Możliwe jest wykonanie operacji zapisania dask dataframe w pamięci w celu przyspieszenia obliczeń na mniejszych fragmentach zbioru. Do zapisania danych w pamięci RAM wykorzystujemy metodę `.persist()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c2c48d-f72e-4308-8fe4-7355422cb31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "polska = ddf[ddf['Kraj'] == 'Polska']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e23100-1c2d-4eb7-9f8c-92b3d8452a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# obliczenia w sposób load and select (Dask usuwa z pamięci dane będące obliczeniami pośrednimi z grafu,\n",
    "# stąd nie może ich ponownie wykorzystać jeżeli obliczenia są takie same)\n",
    "polska['Utarg'].sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a37a3ae-fe5b-4a15-a425-b174eaabebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zapisanie w pamięci RAM\n",
    "polska = polska.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4513c285-472f-4d82-88b9-53086e7daa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# obliczenia w sposób rozproszony\n",
    "polska['Utarg'].sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7820bd2-8ca4-470c-9c18-5f5586279aed",
   "metadata": {},
   "source": [
    "## 3. Dask Array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786bcf1e-dcec-45fa-a167-6e7179b8de49",
   "metadata": {},
   "source": [
    "Tablice Dask są skonstruowane wedle takiej samej idei jak ramki danych. Są to struktury, które wewnątrz składają się z wielu tablic numpy, które są wynikiem podzielenia oryginalj tablicy na mniejsze części, aby zrównoleglić i rozproszyć obliczenia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f07ba89-f903-4fdd-99fb-645976ae017d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![dask array](dask-array.svg)\n",
    "\n",
    "_źródło: dask.org_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c866e7-fd34-46dd-b85c-760bcf1b7346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d613cf7a-78ed-4a20-9cfa-3acfafb515f4",
   "metadata": {},
   "source": [
    "Tablica numpy oraz obliczenie średniej z tej tablicy wartości próbkowanych z rozkładu normalnego o danych parametrach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d437fead-c0fd-4525-a0fa-5d465c772ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "arr = np.random.normal(5, 0.2, size=(20_000, 20_000))\n",
    "arr_mean = arr.mean(axis=0)\n",
    "arr_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c377ff-ae83-4705-801c-e2ab27b34c1c",
   "metadata": {},
   "source": [
    "Teraz ta sama operacja z wykorzystaniem tablicy Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310708a4-91a4-4a3c-93a3-bcae1f19d914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tutaj określimy wielkość chunka ręcznie\n",
    "darr = da.random.normal(5, 0.2, size=(20_000, 20_000), chunks=(2000, 2000))\n",
    "darr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bcdefd-8d7e-4269-b3e9-fe30b07679ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "darr_mean = darr.mean(axis=0)\n",
    "darr_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0eeb6e-be1b-45bf-90e2-ca97f187ea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uwaga z wywolaniem dla dużej liczby chunków tablicy dask!\n",
    "darr_mean.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0a3bfe-6200-40a8-b7fb-df4403e32a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "darr_mean.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24705af0-4051-45f3-b8ad-ef0b6ebd33ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stworzenie tej samej tablicy i przekazanie daskowi zadania automatycznego określenia\n",
    "# wielkości chunka\n",
    "darr = da.random.normal(5, 0.2, size=(20_000, 20_000))\n",
    "darr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0da922e-2e2d-49cf-9731-33ebbfade119",
   "metadata": {},
   "source": [
    "Dask przydzielił ilość i wielkość chunków, która bardziej odpowiada architekturze komputerów (system dwójkowy) niż systemowi, w którym człowiek czuje się lepiej (system dziesiętny).\n",
    "Więcej o tym mechanizmie można doczytać tu: https://docs.dask.org/en/stable/array-chunks.html#automatic-chunking, a kilka sprawdzonych porad co do ich wielkości można również znaleźć tutaj: https://tutorial.dask.org/02_array.html#Rules-of-thumb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8507b79d-e590-4809-8704-931a2a80e05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "darr.mean(axis=0).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fd8e80-3be5-4c22-888f-8eb3e9c189be",
   "metadata": {},
   "source": [
    "## **Zadania**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35903602-319f-46f4-83f6-603dbe355406",
   "metadata": {},
   "source": [
    "**Zadanie 1**  \n",
    "Wczytaj plik `zamowienia.csv` do ramki pandas, a następnie w kilku miejscach (ale nie w pierwszych 10 wierszach) wstaw wartość NaN, aby zasymulować wartości brakujące. Zapisz ramkę do pliku `zamowienia_missing.csv`. Wczytaj teraz plik do ramki Dask i sprawdź jakie typy danych zostały przydzielone. Czy zgadzają się z typami z oryginalnego pliku? Wykonaj dowolne obliczenia na całej ramce Dask, aby wymusić wywołanie `.compute()`. Czy pojawił się błąd dotyczący niespójności typów danych? Spróbuj uruchomić kilka razy funkcję wczytywania danych do ramki Dask dataframe z różnymi wartościami parametru `samples`. Dokumentacja `dask.dataframe.read_csv()`: https://docs.dask.org/en/stable/generated/dask.dataframe.read_csv.html\n",
    "\n",
    "**Zadanie 2**  \n",
    "Ze strony https://docs.dask.org/en/stable/dashboard.html skonfiguruj plugin Dask dashboard dla Jupyter Lab i przetestuj jego działanie.\n",
    "\n",
    "**Zadanie 3**  \n",
    "Skonfiguruj lokalny klaster (`Client`) tak, aby nie zaalokował wszystkich zasobów (np. zostaw 8 GB RAM dla systemu hosta + 2 rdzenie). Pobierz dane udostępnione na poprzednich zajęciach (https://huggingface.co/datasets/vargr/private_instagram/tree/main/data) i załaduj do ramki Dask tyle części ile zdołasz w formie bez optymalizacji. Zmierz czas tej operacji. \n",
    "\n",
    "**Zadanie 4**  \n",
    "Wykonaj kilka operacji na klastrze lokalnym z danymi z zadania 3:\n",
    "* wyświetl top 10 użytkowników z najwyższą liczbą like'ów,\n",
    "* pobierz dane tylko za pierwsze półrocze 2019 roku.\n",
    "Każdorazowo zmierz i wyświetl czas operacji i obserwuj dashboard.\n",
    "\n",
    "**Zadanie 5**  \n",
    "Wczytaj te same dane do ramki Dask co w zadaniu 3, ale podaj typy danych, które zostały wybrane w procesie optymalizacji wykonanej w zadaniach z lab 01. Porównaj czas ładowania z zadaniem 3. Wykonaj również te same operacje co w zadaniu 4 i porównaj czas. Śledź wykonanie zadań patrząć na graf wywołań.\n",
    "\n",
    "**Zadanie 6**  \n",
    "Podziel tablicę `darr` z przykładów na inne liczby chunków (eksperymentuj) i wykonaj te same obliczenie (średnia). Dla każdej liczby chunków wypisz czas obliczeń (wykonaj to samo obliczenie minimum 10 razy, aby nieco uwiarygodnić wyniki i uśrednij) i porównaj wyniki. Napisz wniosek o wynikach swoich eksperymentów i automatycznego podziału na chunki. Czy udało Ci się osiągnąć lepszą wydajność niż przy domyślnych ustawieniach?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
